{
  "id": "confluence_display_KAFKA_Consumer+Group+Example",
  "title": "Consumer Group Example - Apache Kafka - Apache Software Foundation",
  "content": "Using the High Level Consumer\nWhy use the High Level Consumer\nSometimes the logic to read messages from Kafka doesn't care about handling the message offsets, it just wants the data. So the High Level Consumer is provided to abstract most of the details of consuming events from Kafka.\nFirst thing to know is that the High Level Consumer stores the last offset read from a specific partition in ZooKeeper. This offset is stored based on the name provided to Kafka when the process starts. This name is referred to as the Consumer Group.\nThe Consumer Group name is global across a Kafka cluster, so you should be careful that any 'old' logic Consumers be shutdown before starting new code. When a new process is started with the same Consumer Group name, Kafka will add that processes' threads to the set of threads available to consume the Topic and trigger a 're-balance'. During this re-balance Kafka will assign available partitions to available threads, possibly moving a partition to another process. If you have a mixture of old and new business logic, it is possible that some messages go to the old logic.\nDesigning a High Level Consumer\nThe first thing to know about using a High Level Consumer is that it can (and should!) be a multi-threaded application. The threading model revolves around the number of partitions in your topic and there are some very specific rules:\nif you provide more threads than there are partitions on the topic, some threads will never see a message\nif you have more partitions than you have threads, some threads will receive data from multiple partitions\nif you have multiple partitions per thread there is NO guarantee about the order you receive messages, other than that within the partition the offsets will be sequential. For example, you may receive 5 messages from partition 10 and 6 from partition 11, then 5 more from partition 10 followed by 5 more from partition 10 even if partition 11 has data available.\nadding more processes/threads will cause Kafka to re-balance, possibly changing the assignment of a Partition to a Thread.\nNext, your logic should expect to get an iterator from Kafka that may block if there are no new messages available.\nHere is an example of a very simple consumer that expects to be threaded.\npackage com.test.groups;\nimport kafka.consumer.ConsumerIterator;\nimport kafka.consumer.KafkaStream;\npublic class ConsumerTest implements Runnable {\nprivate KafkaStream m_stream;\nprivate int m_threadNumber;\npublic ConsumerTest(KafkaStream a_stream, int a_threadNumber) {\nm_threadNumber = a_threadNumber;\nm_stream = a_stream;\n}\npublic void run() {\nConsumerIterator<byte[], byte[]> it = m_stream.iterator();\nwhile (it.hasNext())\nSystem.out.println(\"Thread \" + m_threadNumber + \": \" + new String(it.next().message()));\nSystem.out.println(\"Shutting down Thread: \" + m_threadNumber);\n}\n}\nThe interesting part here is the\nwhile (it.hasNext())\nsection. Basically this code reads from Kafka until you stop it.\nConfiguring the test application\nUnlike the SimpleConsumer the High level consumer takes care of a lot of the bookkeeping and error handling for you. However you do need to tell Kafka where to store some information. The following method defines the basics for creating a High Level Consumer:\nprivate static ConsumerConfig createConsumerConfig(String a_zookeeper, String a_groupId) {\nProperties props = new Properties();\nprops.put(\"zookeeper.connect\", a_zookeeper);\nprops.put(\"group.id\", a_groupId);\nprops.put(\"zookeeper.session.timeout.ms\", \"400\");\nprops.put(\"zookeeper.sync.time.ms\", \"200\");\nprops.put(\"auto.commit.interval.ms\", \"1000\");\nreturn new ConsumerConfig(props);\n}\nThe ‘\nzookeeper.connect\n’ string identifies where to find once instance of Zookeeper in your cluster. Kafka uses ZooKeeper to store offsets of messages consumed for a specific topic and partition by this Consumer Group\nThe ‘\ngroup.id\n’ string defines the Consumer Group this process is consuming on behalf of.\nThe ‘\nzookeeper.session.timeout.ms\n’ is how many milliseconds Kafka will wait for ZooKeeper to respond to a request (read or write) before giving up and continuing to consume messages.\nThe ‘\nzookeeper.sync.time.ms\n’ is the number of milliseconds a ZooKeeper ‘follower’ can be behind the master before an error occurs.\nThe ‘\nauto.commit.interval.ms\n’ setting is how often updates to the consumed offsets are written to ZooKeeper. Note that since the commit frequency is time based instead of # of messages consumed, if an error occurs between updates to ZooKeeper on restart you will get replayed messages.\nMore information about these settings can be found\nhere\nCreating the thread pool\nThis example uses the Java\njava.util.concurrent\npackage for thread management since it makes creating a thread pool very simple.\npublic void run(int a_numThreads) {\nMap<String, Integer> topicCountMap = new HashMap<String, Integer>();\ntopicCountMap.put(topic, new Integer(a_numThreads));\nMap<String, List<KafkaStream<byte[], byte[]>>> consumerMap = consumer.createMessageStreams(topicCountMap);\nList<KafkaStream<byte[], byte[]>> streams = consumerMap.get(topic);\n// now launch all the threads\n//\nexecutor = Executors.newFixedThreadPool(a_numThreads);\n// now create an object to consume the messages\n//\nint threadNumber = 0;\nfor (final KafkaStream stream : streams) {\nexecutor.execute(new ConsumerTest(stream, threadNumber));\nthreadNumber++;\n}\n}\nFirst we create a Map that tells Kafka how many threads we are providing for which topics. The consumer.createMessageStreams is how we pass this information to Kafka. The return is a map of KafkaStream to listen on for each topic. (Note here we only asked Kafka for a single Topic but we could have asked for multiple by adding another element to the Map.)\nFinally we create the thread pool and pass a new ConsumerTest object to each thread as our business logic.\nClean Shutdown and Error Handling\nKafka does not update Zookeeper with the message offset last read after every read, instead it waits a short period of time. Due to this delay it is possible that your logic has consumed a message and that fact hasn't been synced to zookeeper. So if your client exits/crashes you may find messages being replayed next time to start.\nAlso note that sometimes the loss of a Broker or other event that causes the Leader for a Partition to change can also cause duplicate messages to be replayed.\nTo help avoid this, make sure you provide a clean way for your client to exit instead of assuming it can be 'kill -9'd.\nAs an example, the main here sleeps for 10 seconds, which allows the background consumer threads to consume data from their streams 10 seconds. Since auto commit is on, they will commit offsets every second. Then, shutdown is called, which calls shutdown on the consumer, then on the ExecutorService, and finally tries to wait for the ExecutorService to finish all outsanding work. This gives the consumer threads time to finish processing the few outstanding messages that may remain in their streams. Shutting down the consumer causes the iterators for each stream to return false for hasNext() once all messages already received from the server are processed, so the other threads should exit gracefully. Additionally, with auto commit enabled, the call to consumer.shutdown() will commit the final offsets.\ntry {\nThread.sleep(10000);\n} catch (InterruptedException ie) {\n}\nexample.shutdown();\nIn practice, a more common pattern is to use sleep indefinitely and use a shutdown hook to trigger clean shutdown.\nRunning the example\nThe example code expects the following command line parameters:\nZooKeeper connection string with port number\nConsumer Group name to use for this process\nTopic to consume messages from\n# of threads to launch to consume the messages\nFor example:\nserver01.myco.com1:2181 group3 myTopic  4\nWill connect to port 2181 on\nserver01.myco.com\nfor ZooKeeper and requests all partitions from Topic\nmyTopic\nand consume them via\n4\nthreads. The Consumer Group for this example is\ngroup3\n.\nFull Source Code\npackage com.test.groups;\nimport kafka.consumer.ConsumerConfig;\nimport kafka.consumer.KafkaStream;\nimport kafka.javaapi.consumer.ConsumerConnector;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Properties;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\npublic class ConsumerGroupExample {\nprivate final ConsumerConnector consumer;\nprivate final String topic;\nprivate  ExecutorService executor;\npublic ConsumerGroupExample(String a_zookeeper, String a_groupId, String a_topic) {\nconsumer = kafka.consumer.Consumer.createJavaConsumerConnector(\ncreateConsumerConfig(a_zookeeper, a_groupId));\nthis.topic = a_topic;\n}\npublic void shutdown() {\nif (consumer != null) consumer.shutdown();\nif (executor != null) executor.shutdown();\ntry {\nif (!executor.awaitTermination(5000, TimeUnit.MILLISECONDS)) {\nSystem.out.println(\"Timed out waiting for consumer threads to shut down, exiting uncleanly\");\n}\n} catch (InterruptedException e) {\nSystem.out.println(\"Interrupted during shutdown, exiting uncleanly\");\n}\n}\npublic void run(int a_numThreads) {\nMap<String, Integer> topicCountMap = new HashMap<String, Integer>();\ntopicCountMap.put(topic, new Integer(a_numThreads));\nMap<String, List<KafkaStream<byte[], byte[]>>> consumerMap = consumer.createMessageStreams(topicCountMap);\nList<KafkaStream<byte[], byte[]>> streams = consumerMap.get(topic);\n// now launch all the threads\n//\nexecutor = Executors.newFixedThreadPool(a_numThreads);\n// now create an object to consume the messages\n//\nint threadNumber = 0;\nfor (final KafkaStream stream : streams) {\nexecutor.submit(new ConsumerTest(stream, threadNumber));\nthreadNumber++;\n}\n}\nprivate static ConsumerConfig createConsumerConfig(String a_zookeeper, String a_groupId) {\nProperties props = new Properties();\nprops.put(\"zookeeper.connect\", a_zookeeper);\nprops.put(\"group.id\", a_groupId);\nprops.put(\"zookeeper.session.timeout.ms\", \"400\");\nprops.put(\"zookeeper.sync.time.ms\", \"200\");\nprops.put(\"auto.commit.interval.ms\", \"1000\");\nreturn new ConsumerConfig(props);\n}\npublic static void main(String[] args) {\nString zooKeeper = args[0];\nString groupId = args[1];\nString topic = args[2];\nint threads = Integer.parseInt(args[3]);\nConsumerGroupExample example = new ConsumerGroupExample(zooKeeper, groupId, topic);\nexample.run(threads);\ntry {\nThread.sleep(10000);\n} catch (InterruptedException ie) {\n}\nexample.shutdown();\n}\n}",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
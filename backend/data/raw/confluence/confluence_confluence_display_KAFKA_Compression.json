{
  "id": "confluence_display_KAFKA_Compression",
  "title": "Compression - Apache Kafka - Apache Software Foundation",
  "content": "This feature introduces the end-to-end block compression feature in Kafka. If enabled, data will be compressed by the producer, written in compressed format on the server and decompressed by the consumer. Compression will improve the consumer throughput for some decompression cost. This is especially useful when mirroring data across data centers.\nDesign\nA set of messages can be compressed and represented as one compressed message. In that sense, a message is recursive by definition. A ByteBufferMessageSet could consist of both uncompressed as well as compressed data. Due to that, we need some way of identifying compressed messages from uncompressed ones. To identify a compressed message, we introduce a compression-attributes byte in the message header. This addition of a byte in the message header indicates change in the network byte format as well as the storage byte format. This new format is versioned as magic byte value of 1.\nThe message header format for magic byte=1, now looks like -\n1 byte magic\n1 byte compression-attributes\n4 byte CRC32 of the payload\nThe lowest 2 bits in the attributes byte will select the compression codec used for the compressing data. A compression codec value of 0 indicates an uncompressed message.\nOffset management on the consumer\nThe data received by a consumer for a topic might contain both compressed as well as uncompressed messages. The consumer iterator transparently decompresses compressed data and only returns an uncompressed message. The offset maintenance in the consumer gets a little tricky. In the zookeeper consumer, the consumed offset is updated each time a message is returned. This consumed offset should be a valid fetch offset for correct failure recovery. Since data is stored in compressed format on the broker, valid fetch offsets are the compressed message boundaries. Hence, for compressed data, the consumed offset will be advanced one compressed message at a time. This has the side effect of possible duplicates in the event of a consumer failure. For uncompressed data, consumed offset will be advanced one message at a time.\nBackwards compatibility\nA version 0.7 broker and consumer will be able to understand messages of magic byte values 0 and 1. So the brokers and consumers are backwards compatible.\nConfiguration changes\nThere are 2 new config parameters on the producer side -\nConfig parameter\nDescription\nDefault\ncompression.codec\nControls the compression codec to be used by the producer. (0: No compression, 1: GZIP compression, 2: Snappy compression, 3: LZ4 compression)\n0\ncompressed.topics\ncomma separated list of topics for which compression should be enabled. This doesn't mean anything when compression.codec = 0\nempty\ncompression.topics=empty\ncompression.topics=\"topicA,topicB\"\ncompression.codec=0\nAll topics are uncompressed since compression is disabled\nAll topics are uncompressed since compression is disabled\ncompression.codec=1\nAll topics are compressed\nOnly the topics topicA and topicB are compressed\nCompression codecs supported\nCurrently, only GZIP, Snappy and LZ4 compression codecs are supported.",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/Compression",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
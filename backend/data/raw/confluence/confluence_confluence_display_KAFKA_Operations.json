{
  "id": "confluence_display_KAFKA_Operations",
  "title": "Operations - Apache Kafka - Apache Software Foundation",
  "content": "Here is some information on actually running Kafka as a production system. This is meant as a page for people to record their operational and monitoring practices to help people gather knowledge about successfully running Kafka in production. Feel free to add a section for your configuration if you have anything you want to share. There is nothing magically about most of these configurations, you may be able to improve on them, but they may serve as a helpful starting place.\nLinkedIn\nHardware\nWe are using dual quad-core Intel Xeon machines with 24GB of memory. In general this should not matter too much, we only see pretty low CPU usage at peak even with GZIP compression enabled and a number of clients that don't batch requests. The memory is probably more than is needed for caching the active segments of the log.\nThe disk throughput\nis\nimportant. We have 8x7200 rpm SATA drives in a RAID 10 array. In general this is the performance bottleneck, and more disks is more better. Depending on how you configure flush behavior you may or may not benefit from more expensive disks (if you flush often then higher RPM SAS drives may be better).\nOS Settings\nWe use Linux. Ext4 is the filesystem and we run using software RAID 10. We haven't benchmarked filesystems so other filesystems may be superior.\nWe have added two tuning changes: (1) we upped the number of file descriptors since we have lots of topics and lots of connections, and (2) we upped the max socket buffer size to enable high-performance data transfer between data centers (described\nhere\n).\nJava\n$ java -version\njava version \"1.6.0_21\"\nJava(TM) SE Runtime Environment (build 1.6.0_21-b06)\nJava HotSpot(TM) 64-Bit Server VM (build 17.0-b16, mixed mode)\nHere are our command line options:\njava -server -Xms3072m -Xmx3072m -XX:NewSize=256m -XX:MaxNewSize=256m -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70\n-XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -Xloggc:logs/gc.log -Djava.awt.headless=true\n-Dcom.sun.management.jmxremote -classpath <long list of jars>\nIn 0.8,the GC setting is changed slightly to:\n-Xms3g -Xmx3g -XX:NewSize=256m -XX:MaxNewSize=256m -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:CMSInitiatingOccupancyFraction=30\n-XX:+UseCMSInitiatingOccupancyOnly -XX:+CMSConcurrentMTEnabled -XX:+CMSScavengeBeforeRemark -XX:+PrintGCDetails -XX:+PrintGCDateStamps\n-XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -Xloggc:logs/gc.log\nKafka\nWe are running Kafka 0.7 right now but may move to trunk as we fix bugs.\nThe most important\nserver\nconfigurations for performance are those that control the disk flush rate. The more often data is flushed to disk, the more \"seek-bound\" Kafka will be and the lower the throughput. However we don't hand out data until is is sync'd to disk, so delaying flush also adds some latency. The flush behavior can be set by either giving a timeout (flush at most every 30 seconds, say) or a number of messages (flush every 1000 messages). This can be overridden at the topic level, if desired. The setting always applies to each partition.\nThe most important\nclient\nconfigurations for performance are (1) compression, (2) sync vs async production, (3) batch size for async production, (4) fetch size.\nHere is our server\nconfiguration\n:\nkafka.log.default.flush.interval.ms=10000\nkafka.log.file.size=1073741824\nkafka.log.default.flush.scheduler.interval.ms=2000\nkafka.log.flush.interval=3000\nkafka.socket.send.buffer=2097152\nkafka.socket.receive.buffer=2097152\nkafka.monitoring.period.secs=30\nkafka.num.threads=8\nkafka.log.cleanup.interval.mins=30\nkafka.log.retention.hours=168\nkafka.zookeeper.sessiontimeoutms=6000\nkafka.zookeeper.connection.timeout=2000\nkafka.num.partitions=1\nClient configuration varies a fair amount.\nMonitoring\nOur monitoring is done though a centralized monitoring system custom to LinkedIn, but it keys off the JMX stats exposed from Kafka. To see what is available the easiest thing is just to start a Kafka broker and/or client and fire up JConsole and take a look.\nServer Stats\nbean name: kafka:type=kafka.SocketServerStats\ndef getProduceRequestsPerSecond: Double\ndef getFetchRequestsPerSecond: Double\ndef getAvgProduceRequestMs: Double\ndef getMaxProduceRequestMs: Double\ndef getAvgFetchRequestMs: Double\ndef getMaxFetchRequestMs: Double\ndef getBytesReadPerSecond: Double\ndef getBytesWrittenPerSecond: Double\ndef getNumFetchRequests: Long\ndef getNumProduceRequests: Long\ndef getTotalBytesRead: Long\ndef getTotalBytesWritten: Long\ndef getTotalFetchRequestMs: Long\ndef getTotalProduceRequestMs: Long\nbean name: kafka:type=kafka.BrokerAllTopicStat kafka:type=kafka.BrokerAllTopicStat.[topic]\ndef getMessagesIn: Long\ndef getBytesIn: Long\ndef getBytesOut: Long\ndef getFailedProduceRequest: Long\ndef getFailedFetchRequest: Long\nbean name: kafka:type=kafka.LogFlushStats\ndef getFlushesPerSecond: Double\ndef getAvgFlushMs: Double\ndef getTotalFlushMs: Long\ndef getMaxFlushMs: Double\ndef getNumFlushes: Long\nProducer stats\nbean name: kafka:type=kafka.KafkaProducerStats\ndef getProduceRequestsPerSecond: Double\ndef getAvgProduceRequestMs: Double\ndef getMaxProduceRequestMs: Double\ndef getNumProduceRequests: Long\nbean name: kafka.producer.Producer:type=AsyncProducerStats\ndef getAsyncProducerEvents: Int\ndef getAsyncProducerDroppedEvents: Int\nConsumer stats\nbean name: kafka:type=kafka.ConsumerStats\ndef getPartOwnerStats: String\ndef getConsumerGroup: String\ndef getOffsetLag(topic: String, brokerId: Int, partitionId: Int): Long\ndef getConsumedOffset(topic: String, brokerId: Int, partitionId: Int): Long\ndef getLatestOffset(topic: String, brokerId: Int, partitionId: Int): Long\nbean name: kafka:type=kafka.ConsumerAllTopicStat kafka:type=kafka.ConsumerTopicStat.[topic]\ndef getMessagesPerTopic: Long\ndef getBytesPerTopic: Long\nbean name: kafka:type=kafka.SimpleConsumerStats\ndef getFetchRequestsPerSecond: Double\ndef getAvgFetchRequestMs: Double\ndef getMaxFetchRequestMs: Double\ndef getNumFetchRequests: Long\ndef getConsumerThroughput: Double\nAudit\nThe final alerting we do is on the correctness of the data delivery. We audit that every message that is sent is consumed by all consumers and measure the lag for this to occur. For important topics we alert if a certain completeness is not achieved in a certain time period. The details of this are discussed in\nKAFKA-260\n.\nZookeeper\nZookeeper is essential for the correct operation of Kafka. There are a number of things that must be done to keep Zookeeper running happily as we have learned the hard way, hopefully Dave and Neha will add this since I don't know what we did.\nStable version\nAt LinkedIn, we are running Zookeeper 3.3.*. Version 3.3.3 has known serious issues regarding ephemeral node deletion and session expirations. After running into those issues in production, we upgraded to 3.3.4 and have been running that smoothly for 1/2 year now.\nOperationalizing Zookeeper\nOperationally, we do the following for a healthy Zookeeper installation -\nRedundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don't go nuts) hardware, try to keep redundant power and network paths, etc\nI/O segregation: if you do a lot of write type traffic you'll almost definitely want the transaction logs on a different disk group than application logs and snapshots (the write to the Zookeeper service has a synchronous write to disk, which can be slow).\nApplication segregation: Unless you really understand the application patterns of other apps that you want to install on the same box, it can be a good idea to run Zookeeper in isolation (though this can be a balancing act with the capabilities of the hardware).\nUse care with virtualization: It can work, depending on your cluster layout and read/write patterns and SLAs, but the tiny overheads introduced by the virtualization layer can add up and throw off Zookeeper, as it can be very time sensitive\nZookeeper configuration and monitoring: It's java, make sure you give it 'enough' heap space (We usually run them with 3-5G, but that's mostly due to the data set size we have here). Unfortunately we don't have a good formula for it. As far as monitoring, both JMZ and the 4 letter commands are very useful, they do overlap in some cases (and in those cases we prefer the 4 letter commands, they seem more predictable, or at the very least, they work better with the LI monitoring infrastructure)\nDon't overbuild the cluster: large clusters, especially in a write heavy usage pattern, means a lot of intracluster communication (quorums on the writes and subsequent cluster member updates), but don't underbuild it (and risk swamping the cluster).\nTry to run on a 3-5 node cluster: Zookeeper writes use quorums and inherently that means having an odd number of machines in a cluster. Remember that a 5 node cluster will cause writes to slow down compared to a 3 node cluster, but will allow more fault tolerance.\nOverall, we try to keep the Zookeeper system as small as will handle the load (plus standard growth capacity planning) and as simple as possible. We try not to do anything fancy with the configuration or application layout as compared to the official release as well as keep it as self contained as possible. For these reasons, we tend to skip the OS packaged versions, since it has a tendency to try to put things in the OS standard hierarchy, which can be 'messy', for want of a better way to word it.",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/Operations",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
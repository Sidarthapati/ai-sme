{
  "id": "confluence_display_KAFKA_KIP-838+Simulate+batching+and+compression",
  "title": "KIP-838 Simulate batching and compression - Apache Kafka - Apache Software Foundation",
  "content": "Status\nCurrent state\n:\nWIP\nDiscussion thread\n:\nDiscussing thread:\nhere\nJIRA\n:\nMotivation\nThe idea is to keep in the way of saving money getting accurate information about batching and compression.\nIn our company we have hundreds of clusters and hundreds of marketplaces (producers and consumers) so basically several clients we don't know.\nCurrently thanks to this\nKIP-824\nnow we have a way to inspect the produced requests without affecting the cluster performance. For now we have a rudimentary way which is reading the STDOUT and parsing it in order to get the batching information and the compression type.\nThis is good enough to get information about potential benefit of batching and compression sending the STDOUT to another script, but still we are missing an important\ncheck, SIMULATE the compression payload.\nI would like to create a script which read a sample of the segment log, and output it into a json if the topic/s will benefit of the batching and compression printing the compression ratio.\nThen we can reach the clients with more accurate information, for example the reduction traffic and disk saving. In this way they could see in numbers the cost saving even before applying it.\nAlso with this script we can monitor in real time the topics and automate a way to reach the clients.\nPublic Interfaces\nSimilar to kafka-dump-log.sh the new script would be called (of course open to discuss) kafka-simpulate-batching-and-compression.sh\nThe script will accept paramters :\n- Max bytes to read from the segment log\n- Window time in ms to use as a potential group of batching\n-  Min amount of records in the ms time window mentioned above to consider batching\n- Group them by producer ID (if present)\n- Compression algorithm\n- Skip not active topics and internal ones\nExecuting command\n$ bin/kafka-simpulate-batching-and-compression.sh --topic topic_test --max-bytes 5000000 --batching-window-time 300 --min-records-for-batching 30 --batching-only-by-producer-id --compression-algorithm lz4  --topic-considered-active-last-produce-request-hours 1 --output json\nOutput:\nExecuting command\n{\n{\n\"topic\": \"topic_test\" {\n\"already_batching\": \"false\",\n\"already_compressing\": \"false\",\n\"candidate_for_bathcing\": \"true\",\n\"candidate_for_compression\": \"true\",\n\"compression_ratio_percetage\": 400,\n\"average_recrods_per_batch\": 25\n}\n}\n}\nProposed Changes\nFor reading the segment logs we  can reuse the classes for  reading the segments called FileRecords, similar to what kafka-dump-log.sh  does.\nThe idea is to read the partition 0 of a topic (it always exists)  make sure the topic is \"alive\" so it must be checked the mtime of the current active segment.\nThen the samples are taken respecting the amount of bytes and from the end of the active segment.\nRejected alternatives\nWIP\nCompatibility, Deprecation, and Migration Plan\nThis is a new script so neither creates any compatibility issue nor migration plan is needed.\nWIP",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/KIP-838+Simulate+batching+and+compression",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
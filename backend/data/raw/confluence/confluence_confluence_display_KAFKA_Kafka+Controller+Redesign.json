{
  "id": "confluence_display_KAFKA_Kafka+Controller+Redesign",
  "title": "Kafka Controller Redesign - Apache Kafka - Apache Software Foundation",
  "content": "Motivation\nSummary of existing controller\nCurrent Kafka controller is a multi-threaded controller that emulates a state machine. It works in the following way.\nMaintained state:\nReplicas of partitions on each machine.\nLeaders of partitions.\nState change input source:\nListeners Registered to Zookeeper.\nAddPartitionsListener\nBrokerChangeListener\nDeleteTopicListener\nPartitionReassignedListener(Admin)\nPreferredReplicaElectionListener(Admin)\nReassignedPartitionsIsrChangeListener\nTopicChangeListener\nChannels to brokers (controlled shutdown)\nInternal scheduled tasks (preferred leader election)\nState change execution:\nListener threads, KafkaApi thread and internal scheduler thread makes state change concurrently.\nState change propagation model:\nP2P blocking channel from controller to each broker.\nDedicated message queue for each controller-to-broker connection.\nNo synchronization on message sent to different brokers.\nNo callback for sending messages except topic deletion.\nFail Over/Back model:\nZookeeper based leader election\nZookeeper as persistent state store for fault tolerance.\nProblems of existing controller\nState change are executed by different listeners concurrently. Hence complicated synchronization is needed which is error prone and difficult to debug.\nState propagation is not synchronized. Brokers might be in different state for undetermined time. This leads to unnecessary extra data loss.\nDuring controlled shutdown process, two connections are used for controller to broker communication. This makes state change propagation and controlled shutdown approval out of order.\nSome of the state changes are complicated because the ReplicaStateMachine and PartitionStateMachine are separate but the state changes themselves are not. So the controller has to maintain some sort of ordering between the state changes executed by the individual state machines. In the new design, we will look into folding them into a single one.\nMany state changes are executed for one partition after another. It would be much more efficient to do it in one request.\nSome long running operations need to support cancellation. For example, we want to support cancellation of partition reassignment for those partitions whose reassignment hasn't started yet. The new controller should be able to abort/cancel the long running process.\nNew controller design\nOutline\nWe will keep maintained state and fail over/back model unchanged.\nWe are going to change the state change propagation model, state change execution and output of the state change input source. More specifically:\nAbstract the output of each state change input source to an\nevent\n.\nHave\na single execution thread\nto\nserially\nprocess events one at a time.\nThe zk listeners are responsible for only context updating but not event execution.\nUse o.a.k.clients.NetworClient + callback for state change propagation.\nWe would also like to\nModify KafkaServer to use new NetworkClient and prioritize the controller-to-broker traffic.\nChange the reads/writes to Zookeeper to maybe use multi-operation or async operation.\nRelated tickets\nKAFKA-2139, KAFKA-2029, KAFKA-1305 (and definitely some other tickets... Appreciate it if people can add it here.)\nArchitecture\nThe Controller Context contains two kinds of information:\ncluster reachability\nand\nTopic State\n(Partition, Replica, Leaders, etc)\nTwo types of ZK listeners:\nResponsible of updating\ncluster reachability\nby listening to broker path in zookeeper.\nResponsible for create\nevents\nand add them to Event Queue.\nA controlled shutdown event will be generated when receive controlled shutdown from broker.\nThe controlled shutdown process will be changed to make state change and controlled shutdown approval occur in order. (This might involve broker side change as well)\nScheduled tasks (e.g. preferred leader election) will\nOn controller starting up or resignation, a ControllerStartUp/ControllerResignation event will be generated.\nEvent Executor Thread:\nChange\nTopic State\nin Controller Context\nPropagate the new state to each broker using o.a.k.clients.NetworkClient in non-blocking way.\nBroker will only trigger Zookeeper data change when:\nBroker is down (or long GC)\nNew topic automatically created\nEvent Types and Handling Process\nEvent Types\nThere will be eight types of events in the controller, which are defined as below. Each listener will generate one type of event, controlled shutdown is the eighth event type.\nobject ControllerEventType extends Enumeration {\ntype ControllerEventType = Value\nval AddPartition, TopicChange, DeleteTopic, BrokerChange, PreferredReplicaElection, PartitionReassigned,\nReassignedPartitionIsrChange, ControlledShutdown = Value\n}\nKafkaControllerEvent\nA generic controller event class will be defined:\nabstract class KafkaControllerEvent(eventType: ControllerEventType) {\n// A set that tracks the responses from brokers\nval unAckedNode = new mutable.HashSet[Int]\nval eventDone = new CountDownLatch(1)\ndef makeStatesChange(currentState: PartitionStateMachine): Map[Int, ClientRequest]\ndef controllerRequestCallback(response: ClientResponse) {\nhandleBrokerResponse(response)\nunAckedNode.remove(response.request().request().destination())\nif (unAckedNode.isEmpty)\neventDone.countDown()\n}\ndef handleBrokerResponse(response: ClientResponse)\n}\nAddPartitionEvent\nclass AddPartitionEvent extends KafkaControllerEvent(ControllerEventType.AddPartition) {\noverride def makeStatesChange(): Map[Int, ClientRequest] = {\n// make state change and generate requests to each broker\n}\noverride def handleBrokerResponse(response: ClientResponse) {\n// If necessary, do something when response is received\n}\n}\nTopicChangeEvent\nclass TopicChangeEvent extends KafkaControllerEvent(ControllerEventType.TopicChange) {\noverride def makeStatesChange(): Map[Int, ClientRequest] = {\n// make state change and generate requests to each broker\n}\noverride def handleBrokerResponse(response: ClientResponse) {\n// If necessary, do something when response is received\n}\n}\nDeleteTopicEvent\nclass DeleteTopicEvent extends KafkaControllerEvent(ControllerEventType.DeleteTopic) {\noverride def makeStatesChange(): Map[Int, ClientRequest] = {\n// make state change and generate requests to each broker\n}\noverride def handleBrokerResponse(response: ClientResponse) {\n// If necessary, do something when response is received\n}\n}\nBrokerChangeEvent\nclass BrokerChangeEvent extends KafkaControllerEvent(ControllerEventType.BrokerChange) {\noverride def makeStatesChange(): Map[Int, ClientRequest] = {\n// make state change and generate requests to each broker\n}\noverride def handleBrokerResponse(response: ClientResponse) {\n// If necessary, do something when response is received\n}\n}\nPreferredReplicaElectionEvent\nclass PreferredReplicaElectionEvent extends KafkaControllerEvent(ControllerEventType.PreferredReplicaElection) {\noverride def makeStatesChange(): Map[Int, ClientRequest] = {\n// make state change and generate requests to each broker\n}\noverride def handleBrokerResponse(response: ClientResponse) {\n// If necessary, do something when response is received\n}\n}\nPartitionReassignedEvent\nclass PartitionReassignedEvent extends KafkaControllerEvent(ControllerEventType.PartitionReassigned) {\noverride def makeStatesChange(): Map[Int, ClientRequest] = {\n// make state change and generate requests to each broker\n}\noverride def handleBrokerResponse(response: ClientResponse) {\n// If necessary, do something when response is received\n}\n}\nReassignedPartitionIsrChangeEvent\nclass ReassignedPartitionIsrChangeEvent extends KafkaControllerEvent(ControllerEventType.ReassignedPartitionIsrChange) {\noverride def makeStatesChange(): Map[Int, ClientRequest] = {\n// make state change and generate requests to each broker\n}\noverride def handleBrokerResponse(response: ClientResponse) {\n// If necessary, do something when response is received\n}\n}\nControlledShutdown\nclass ControlledShutdownEvent extends KafkaControllerEvent(ControllerEventType.ControlledShutdown) {\noverride def makeStatesChange(): Map[Int, ClientRequest] = {\n// make state change and generate requests to each broker\n}\noverride def handleBrokerResponse(response: ClientResponse) {\n// If necessary, do something when response is received\n}\n}\nEvent Handling Process\nThe general event handling process would be something like this:\nwhile(!shutdown){\nEvent event = eventQueue.pollFirst()\n// Make state change\ntry {\nval brokerRequests = event.makeStateChange(partitionStateMachine)\nbrokerRequests.map { case (broker, request) =>\nnetworkClient.send(request)\nevent.unAckedNode.add(broker)\n}\nwhile (!event.unAckedNode.isEmpty) {\ntry {\nnetworkClient.poll(timeout)\n} catch {\ncase KafkaApiException =>\n// Do something\ncase Exception =>\n// Error handling\n}\ncheckNodeLivenessAndIgnoreDeadNode()\n}\n} catch {\ncase StateChangeException =>\n// handle illegal state change\n}\n}\nLong Running Process Handling\nCurrently replica reassigment is a really long running process. In new controller, we want to support abort/cancellation of the long running process when some event that affects the partition reassignment occurs after the long running process starts. In new controller, we will let later events\noverrides\nthe state change of previous events based on the principle of\navoiding under replicated partition (URP)\n.\nFor example, partition reassignment consists of several events:\n1 PartitionReassignedEvent + N * ReassignedPartitionIsrChangeEvent.\nTime\nBroker 0\nBroker 1\nBroker 2\nEvent\n0 - Initial State\n{t1p1, replica:{0,1}, leader:0, isr:{0,1}}\n{t1p1, replica:{0,1},\nleader:0, isr:{0,1\n}}\n1 - Reassign t1p1 to broker 1 and broker 2\nPartitionReassignedEvent\n{t1p1,\nreplica:{0,1,2},\nleader:1, isr:{0,1\n}}\n{t1p1, replica:{0,1,2}, leader:1, isr:{0,1}}\n{t1p1, replica:{0,1,2}, leader:1, isr:{0,1}}\n2.A - Broker 2 down (Scenario 1)\nBrokerChangeEvent\n{t1p1,\nreplica:{0,1,2},\nleader:0, isr:{0\n,1}}\n{t1p1, replica:{0,1,2}, leader:0, isr:{0,1}}\n2.B - Broker 1 down (Scenario 2)\n{t1p1,\nreplica:{0,1,2},\nleader:0, isr:{0\n,1}}\n3 - Another partition reassignment\nPartitionReassignedEvent\nIn scenario 2.A, we may choose to remove broker 2 from replica list or we can just stop listening to ISR change and let a later partition reassignment take care of this.\nIn scenario 2.B, we are better off to keep broker 2 in the replica list, and not remove broker 0 from isr after broker 2 enters isr so we do not expose to under replicated partition.\nIn step 3, when another partition reassignment comes, if the reassigned topic partition in step 1 and step 3 are the same, the second partition assignment will overrides the first one. It should:\nclean up the listeners of the previous partition reassignment on the\noverridden\npartitions,\nstart the normal partition reassignment process.\nDiscussion Required\nAs stated in KAFKA-2029, current state change propagation has an issue that in an unbalanced cluster. State change on a heavily loaded broker will be much slower than a lightly loaded broker. This is because of the following two reasons:\nController traffic is not prioritized on broker side. So controller messages needs to wait until some clients requests are handled which takes much longer on a heavily loaded broker.\nHeavily loaded broker needs to take state change for more partitions while same amount state changes are distributed among several brokers as followers.\nBatching state change into a single request and prioritize the controller traffic on broker will solve the problem. But I wonder is there any specific reason we did not batch the state change in current controller?\nIdeally, we should try to let the brokers in consistent state if possible. That indicates that we should put synchronization barriers between two events. Which means we do not execute the next event until:\nCallback has been fired on each broker, or\nA broker is down and we will just skip waiting for its callback and move on.\nDoes it make sense to do so? Implementation wise, it means that if a broker is down, we will stop sending message to all brokers until we know it's down. (And this leads to question 3)\nWe actually have two ways to determine if a broker is alive or not: from Zookeeper or from NetworkClient. Currently we take zookeeper as source of truth, but when we send messages, NetworkClient connectivity is the one actually matters. Because Zookeeper timeout could take a while, so what should we do if Zookeeper says a broker is alive but NetworkClient shows it is disconnected. Should we block the current event processing? If there is a long GC, is it possible that NetworkClient says broker is connected but Zookeeper says it's dead? Back to question 2, it is about when we consider \"a broker is down\".",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Controller+Redesign",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
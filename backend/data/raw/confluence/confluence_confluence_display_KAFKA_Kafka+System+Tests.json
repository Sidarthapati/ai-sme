{
  "id": "confluence_display_KAFKA_Kafka+System+Tests",
  "title": "Kafka System Tests - Apache Kafka - Apache Software Foundation",
  "content": "Introduction\nSystem Test is a Python based regression test framework to run system testing for Kafka. This document is intended to serve as a quick start guide. If you are a Kafka developer and would like to run a sanity test before checking in your change, you may just need to read the following sections:\nRunning System Test\nSpecify what test cases to run\nTest Report Interpretation\nQuick Start\nRunning System Test\n1. Check out kafka codebase:\na. ~ $ git clone https://git-wip-us.apache.org/repos/asf/kafka.git\nb. ~ $ cd <kafka>\n2. Under <kafka>, build kafka\na. <kafka> $ ./gradlew jar\n3. Set JAVA_HOME environment variable (optional, but recommended):\na. export JAVA_HOME=/usr/java/jdk1.7.0_67\n4. Make sure that you can ssh to localhost without a password.\n5. Under <kafka>/system_test, execute the following command to start System Test :\n$ python -u -B system_test_runner.py 2>&1 | tee system_test_output.log\nSpecify what test cases to run\nSystem Test reads <kafka>/system_test/testcase_to_run.json for a list of test cases to run.\n1. The following is the configuration of the file out of the box :\n{\n\"ReplicaBasicTest\"   : [\n\"testcase_0001\"\n]\n}\n2. To run testcase_0002 as well in ReplicaBasicTest, modify the file to look like the following :\n{\n\"ReplicaBasicTest\"   : [\n\"testcase_0001\",\n\"testcase_0002\"\n]\n}\n3. To run testcase_5001 as well in MirrorMakerTest, modify the file to look like the following :\n{\n\"ReplicaBasicTest\"   : [\n\"testcase_0001\",\n\"testcase_0002\"\n],\n\"MirrorMakerTest\"   : [\n\"testcase_5001\"\n]\n}\n4. To run all test cases (100+), you may copy testcase_to_run_all.json into testcase_to_run.json as shown below :\n$ cp <kafka>/system_test/testcase_to_run_all.json <kafka>/system_test/testcase_to_run.json\nTest Report Interpretation\n# ================================================\n#\n#         Test results interpretations\n#\n# ================================================\n1. PASSED case - A PASSED test case should have a test result similar to the following :\n_test_case_name  :  testcase_0201\n_test_class_name  :  ReplicaBasicTest\narg : bounce_broker  :  true\narg : broker_type  :  controller\narg : message_producing_free_time_sec  :  15\narg : num_iteration  :  3\narg : num_messages_to_produce_per_producer_call  :  50\narg : num_partition  :  3\narg : replica_factor  :  3\narg : signal_type  :  SIGTERM\narg : sleep_seconds_between_producer_calls  :  1\nvalidation_status  :\nNo. of messages from consumer on [test_1] at simple_consumer_test_1-0_r1.log  :  711\nNo. of messages from consumer on [test_1] at simple_consumer_test_1-0_r2.log  :  711\nNo. of messages from consumer on [test_1] at simple_consumer_test_1-0_r3.log  :  711\nNo. of messages from consumer on [test_1] at simple_consumer_test_1-1_r1.log  :  700\nNo. of messages from consumer on [test_1] at simple_consumer_test_1-1_r2.log  :  700\nNo. of messages from consumer on [test_1] at simple_consumer_test_1-1_r3.log  :  700\nNo. of messages from consumer on [test_1] at simple_consumer_test_1-2_r1.log  :  604\nNo. of messages from consumer on [test_1] at simple_consumer_test_1-2_r2.log  :  604\nNo. of messages from consumer on [test_1] at simple_consumer_test_1-2_r3.log  :  604\nUnique messages from consumer on [test_1]  :  2000\nUnique messages from producer on [test_1]  :  2000\nValidate for data matched on topic [test_1]  :  PASSED                               <----------\nValidate for data matched on topic [test_1] across replicas  :  PASSED               <---------- All validations\nValidate for merged log segment checksum in cluster [source]  :  PASSED              <----------   PASSED\nValidate index log in cluster [source]  :  PASSED                                    <----------\n2. FAILED case - A FAILED test case is shown below with data loss in topic test_1 :\n_test_case_name  :  testcase_5005\n_test_class_name  :  MirrorMakerTest\narg : bounce_leader  :  false\narg : bounce_mirror_maker  :  true\narg : bounced_entity_downtime_sec  :  30\narg : message_producing_free_time_sec  :  15\narg : num_iteration  :  1\narg : num_messages_to_produce_per_producer_call  :  50\narg : num_partition  :  2\narg : replica_factor  :  3\narg : sleep_seconds_between_producer_calls  :  1\nvalidation_status  :\nUnique messages from consumer on [test_1]  :  1392                                   <------\nUnique messages from consumer on [test_2]  :  1400                                          |\nUnique messages from producer on [test_1]  :  1400                                          |\nUnique messages from producer on [test_2]  :  1400                                          |\nValidate for data matched on topic [test_1]  :  FAILED                               <--------- FAILED because of data matched issue on topic \"test_1\"\nValidate for data matched on topic [test_2]  :  PASSED\nValidate for merged log segment checksum in cluster [source]  :  PASSED\nValidate for merged log segment checksum in cluster [target]  :  PASSED\n3. Skipped case will have result similar to the following (No validation status details) :\n_test_case_name  :  testcase_0201\n_test_class_name  :  ReplicaBasicTest\narg : bounce_broker  :  true\narg : broker_type  :  controller\narg : message_producing_free_time_sec  :  15\narg : num_iteration  :  3\narg : num_messages_to_produce_per_producer_call  :  50\narg : num_partition  :  3\narg : replica_factor  :  3\narg : signal_type  :  SIGTERM\narg : sleep_seconds_between_producer_calls  :  1\nvalidation_status  :\nTest Case Description\ntestcase description\nMisc\nDirectory Structure Overview\n<kafka>\n|- /bin\n|- /config\n|- /contrib\n|- /core\n|- /lib\n|.\n|.\n|.\n|- /system_test\n|- system_test_runner.py              # Main script to start System Test\n|\n|- /utils                             # This is a directory that contains all helper classes / util functions for system test\n|    |- kafka_system_test_utils.py    # utilities specific to Kafka system testing   (e.g. Kafka test cases data loss validation)\n|    |- replication_utils.py          # utilities specific to replication testing    (e.g. Leader election log message pattern)\n|    |- setup_utils.py                # generic helper for system test setup         (e.g. System Test environment setup)\n|    |- system_test_utils.py          # utilities for generic testing purposes       (e.g. reading JSON data file)\n|    |- testcase_env.py               # testcase environment setup                   (e.g. data structure initialization such as brokers-pid mapping)\n|    |- metrics.py                    # utilities for metrics collection             (to be completed)\n|    |- pyh.py                        # from http://code.google.com/p/pyh            (open source)\n|\n|- cluster_config.json                # this file contains the following properties:\n|                                     #   1. what entities (ZK, Broker, Mirror Maker, Migration Tool, Producer, Consumer) should be running\n|                                     #   2. which cluster (source or target)\n|                                     #   3. where they should be running (physical nodes)\n|\n|- /replication_testsuite        --------------------------------------------------------------------------------------------------\n|    |- replica_basic_test.py                                                                 |   this block is one testsuite      |\n|    |- /config                                                                               |   for \"replication_testsuite\"      |\n|         |                                                                                   |------------------------------------|\n|         |- server.properties                      # as a TEMPLATE for server.properties                                          |\n|         |- zookeeper.properties                   # as a TEMPLATE for zookeeper.properties                                       |\n|         |                                                                                                                        |\n|         |- (migration_consumer.properties)        # only in migration_tool_testsuite                                             |\n|         |- (migration_producer.properties)        # only in migration_tool_testsuite                                             |\n|         |- (mirror_maker_consumer.properties)     # only in mirror_maker_testsuite                                               |\n|         |- (mirror_maker_producer.properties)     # only in mirror_maker_testsuite                                               |\n|         |                                                                                                                        |\n|         |- /testcase_0001                                                                                                        |\n|         |    |- (config)                          # generated when this testcase is executed in system test runtime              |\n|         |    |    |- (server.properties)          #   by overriding the TEMPLATE of server.properties, zookeeper.properties      |\n|         |    |    |- (zookeeper.properties)       #   with new values from testcase_0001_properties.json                         |\n|         |    |                                                                                                                   |\n|         |    |- (logs)                            # generated when this testcase is executed in system test runtime              |\n|         |    |                                                                                                                   |\n|         |    |- testcase_0001_properties.json     # this file contains new values to override the default settings of            |\n|         .                                         #   various entities (e.g. ZK, broker, producer, mirror maker, ...)            |\n|         .                                         #   such as \"log.segment.bytes\", \"num.partitions\", \"broker.id\", ...            |\n|         .                                                                                                                        |\n|         |- /testcase_NNNN                                                                                                        |\n|              |- (config)                                                                                                         |\n|              |- (logs)                                                                                                           |\n|              |- testcase_NNNN_properties.json                                                                                    |\n|                                                                                                                                  |\n|   -------------------------------------------------------------------------------------------------------------------------------\n|\n|- /mirror_maker_testsuite\n|    |- mirror_maker_test.py\n|    |- /config\n.\n.\n.\nHow does it work\n<kafka>\n|- /bin\n|- /config\n|- /contrib\n|- /core\n|- /lib\n|.\n|.\n|.\n|- /system_test\n|\n|- system_test_runner.py\n|\n|  # system_test_runner.py is the main script to start system test as follows :\n|  #\n|  #   1. for each test suite directory (XXXX_testsuite) under systemTestEnv.SYSTEM_TEST_BASE_DIR (<kafka>/system_test) :\n|  #   2.     get a list of module scripts (*.py) in test suite directory\n|  #   3.     for each file in the list of module scripts :\n|  #   4.         get class name from the Python module script\n|  #   5.         retrieve corresponding suite & module names from class name\n|  #   6.         dynamically load the module and start the test class\n|  #   7.         save each test case result in systemTestEnv.systemTestResultsList\n|  #   8. for each result in systemTestEnv.systemTestResultsList :\n|  #   9.     print result\n|\n|- /utils      # This is a directory that contains all helper classes / util functions for system test\n|\n|    |- kafka_system_test_utils.py    # utilities specific to Kafka system testing   (e.g. Kafka test cases data loss validation)\n|    |- replication_utils.py          # utilities specific to replication testing    (e.g. Leader election log message pattern)\n|    |- setup_utils.py                # generic helper for system test setup         (e.g. System Test environment setup)\n|    |- system_test_utils.py          # utilities for generic testing purposes       (e.g. reading JSON data file)\n|    |- testcase_env.py               # testcase environment setup                   (e.g. data structure initialization such as brokers-pid mapping)\n|    |- metrics.py                    # utilities for metrics collection\n|    |- pyh.py                        # from http://code.google.com/p/pyh            (open source)\n|\n|- cluster_config.json                # this file contains the following properties:\n|                                     #   1. what entities (Producer, Consumer) should be running\n|                                     #   2. which cluster (source or target)\n|                                     #   3. where they should be running (physical nodes)\n|\n|  # cluster_config.json is used to specify the logical machines configuration :\n|  #\n|  #   entity_id    : In each testcase, there may be zookeeper(s), broker(s), producer(s) and\n|  #                  consumer(s) involved. \"entity_id\" is used to uniquely identify each\n|  #                  component inside the system test.\n|  #   hostname     : It is used to specify the name of the machine in the distributed environment.\n|  #                  \"localhost\" is used by default.\n|  #   role         : The supported values are \"zookeeper\", \"broker\", \"mirror_maker\", \"migration_tool\",\n|  #                  \"producer\", \"consumer\".\n|  #   cluster_name : The supported values are \"source\", \"target\"\n|  #   kafka_home   : Specify the Kafka installation directory of each machine in a distributed environment.\n|  #                  \"default\" is used by default and ../ is assumed to be \"kafka_home\".\n|  #   java_home    : Specify the JAVA_HOME of each machine in a distributed environment.\n|  #                  1. \"default\" is used by default. If JAVA_HOME is specified in the environment,\n|  # \t\t\t\t\tthis value will be used. Otherwise, System Test executes a shell command \"which java\"\n|  #                     to find java bin dir and set JAVA_HOME accordingly. If no java binary can be found,\n|  #                     it throws Exception and exit.\n|  #                  2. If a path is specified other than \"default\", System Test will verify java binary.\n|  #                     Otherwise, it throws Exception and exit.\n|  #   jmx_port     : Specify a JMX_PORT for each component. It must be unique inside the cluster_config.json.\n|  #\n|  # {\n|  #   \"cluster_config\": [\n|  #   {\n|  #     \"entity_id\"    : \"0\",                  <----------------------------------------------------------------------------------|\n|  #     \"hostname\"     : \"localhost\",             \"entity_id\" in cluster_config must match the corresponding \"entity id\" in       |\n|  #     \"role\"         : \"zookeeper\",             testcase_NNNN.properties as shown below                                         |\n|  #     \"cluster_name\" : \"source\",                                                                                                |\n|  #     \"kafka_home\"   : \"default\",                                                                                               |\n|  #     \"java_home\"    : \"default\",                                                                                               |\n|  #     \"jmx_port\"     : \"9990\"                                                                                                   |\n|  #   },                                                                                                                          |\n|  #   .                                                                                                                           |\n|  #   .                                                                                                                           .\n|  #   .                                                                                                                           .\n|  # }                                                                                                                             .\n|\n|- /XXXX_testsuite\n|\n|  # XXXX_testsuite is a directory which contains Python scripts and test case directories for a specific group of functional testings.\n|  # For example, mirror_maker_testsuite contains mirror_maker_test.py in which it will start mirror maker instances which is not\n|  # required for the test cases in migration_tool_testsuite.\n|  #\n|  # The following are the existing test suites:\n|  #   migration_tool_testsuite\n|  #   mirror_maker_testsuite\n|  #   replication_testsuite\n|\n|- <testsuite>.py\n|\n|  #   <testsuite>.py : This Python script may be implemented with the test logic for a group of test scenarios.\n|  #                    For example, in the \"mirror_maker_testsuite\", mirror_maker_test.py is the testsuite script\n|  #                    which is implemented with the following :\n|  #                    1. start zookeeper(s) in source cluster\n|  #                    2. start broker(s) in source cluster\n|  #                    3. start zookeeper(s) in target cluster\n|  #                    4. start broker(s) in target cluster\n|  #                    5. start mirror maker(s)\n|  #                    6. start producer\n|  #                    7. start consumer\n|  #                    8. stop all entities\n|  #                    9. validate no data loss\n|  #\n|  #                    The above test logic is implemented in a generalized fashion and read from\n|  #                    testcase_NNNN_properties.json for the following test case arguments :\n|  #                    1. replication factor (broker)\n|  #                    2. no. of partitions  (broker)\n|  #                    3. log segment bytes  (broker)\n|  #                    4. topics             (producer / consumer)\n|  #                    .\n|  #                    .\n|  #                    .\n|  #\n|  #                    Therefore, each testsuite can be thought of a functional / feature test group\n|  #                    by varying a combination of settings for easier maintenance.\n|\n|- /config\n|\n|  #   config         : This config directory contains the TEMPLATE properties files for this testsuite.\n|  #                    For \"replication_testsuite\", only server.properties and zookeeper.properties are\n|  #                    required. However, \"mirror_maker_testsuite\" has mirror_consumer.properties and\n|  #                    mirror_producer.properties as well.\n|  #\n|  #                    System Test reads the properties from each files in this directory as a TEMPLATE\n|  #                    and override the values with those from testcase_NNNN_properties.json accordingly.\n|\n|- server.properties\n|- zookeeper.properties\n|- (migration_consumer.properties)        # only in migration_tool_testsuite\n|- (migration_producer.properties)        # only in migration_tool_testsuite\n|- (mirror_maker_consumer.properties)     # only in mirror_maker_testsuite\n|- (mirror_maker_producer.properties)     # only in mirror_maker_testsuite\n|\n|- /testcase_NNNN\n|\n|  # testcase_NNNN is a directory which contains a json file to specify the arguments\n|  # required for the <testsuite>.py script to execute.\n|  #\n|  # The main arguments are :\n|  # 1. testcase_args : These are the arguments specific for that test case such as\n|  #    \"replica_factor\", \"num_partition\", \"bounce_broker\", ... etc.\n|  # 2. entities      : These are the arguments required to start running a certain\n|  #                    entity such as a broker : port, replication factor, log file\n|  #                    directory, ... etc.\n|\n|- (config)    # generated when this testcase is executed in system test runtime\n|\n|- (logs)      # generated when this testcase is executed in system test runtime\n|\n|- testcase_NNNN_properties.json     # this file contains new values to override the default settings of\n|                                    #   various entities (e.g. ZK, Broker, Producer, Mirror Maker, ...)\n|                                    #   such as \"log.segment.bytes\", \"num.partitions\", \"broker.id\", ...\n|   # {\n|   #   \"description\": {\"01\":\"Replication Basic : Base Test\",\n|   #                   \"02\":\"Produce and consume messages to a single topic - single partition.\",\n|   #                   \"03\":\"This test sends messages to 3 replicas\",\n|   #                   .\n|   #                   .\n|   #                   .\n|   #   },\n|   #   \"testcase_args\": {\n|   #     \"broker_type\"    : \"leader\",\n|   #     \"bounce_broker\"  : \"false\",\n|   #     \"replica_factor\" : \"3\",\n|   #     \"num_partition\"  : \"1\",\n|   #     \"num_iteration\"  : \"1\",\n|   #     .                                                                                                          .\n|   #     .                                                                                                          .\n|   #     .                                                                                                          .\n|   #   },                                                                                                           |\n|   #   \"entities\": [                                                                                                |\n|   #    {                                                                                                           |\n|   #       \"entity_id\"       : \"0\",                 <---------------------------------------------------------------|\n|   #       \"clientPort\"      : \"2188\",                        1. All attributes defined in this entity must be the\n|   #       \"dataDir\"         : \"/tmp/zookeeper_0\",               corresponding attributes for the \"role\" specified\n|   #       \"log_filename\"    : \"zookeeper_2188.log\",             by \"entity_id\" defined above in cluster_config\n|   #       \"config_filename\" : \"zookeeper_2188.properties\"    2. In this case, the attributes are all zookeeper\n|   #    },                                                       related as the cluster_config has already defined\n|   #    .                                                        entity_id 0 as role \"zookeeper\" above\n|   #    .\n|   #    .\n|   # }\nHow does it validate test case failure\n1. System Test requires ProducerPerformance to print out debugging messages (with sequential message id) as shown below for data loss validation:\n[2013-07-08 15:34:41,169] DEBUG Topic:test_1:ThreadID:0:MessageID:0000000000:xxxxxxxx\n. . .\nWhen ConsoleConsumer consumes the messages, it also prints out the message to the log. The method kafka_system_test_utils.get_message_id parses the Producer / Consumer logs and get lists of message id for comparison.\n2. System Test validates the test case failures in different testsuites accordingly as shown below.\n=========================\nMigration Tool test cases\n=========================\nThe following two methods are called :\nkafka_system_test_utils.validate_data_matched(self.systemTestEnv, self.testcaseEnv, replicationUtils)\nkafka_system_test_utils.validate_broker_log_segment_checksum(self.systemTestEnv, self.testcaseEnv)\n=========================\nMirror Maker test cases\n=========================\nThe following three methods are called :\nkafka_system_test_utils.validate_data_matched(self.systemTestEnv, self.testcaseEnv, replicationUtils)\nkafka_system_test_utils.validate_broker_log_segment_checksum(self.systemTestEnv, self.testcaseEnv, \"source\")\nkafka_system_test_utils.validate_broker_log_segment_checksum(self.systemTestEnv, self.testcaseEnv, \"target\")\n=========================\nReplication test cases\n=========================\nif logRetentionTest.lower() == \"true\":\nkafka_system_test_utils.validate_data_matched(self.systemTestEnv, self.testcaseEnv, replicationUtils)\nelif consumerMultiTopicsMode.lower() == \"true\":\nkafka_system_test_utils.validate_data_matched_in_multi_topics_from_single_consumer_producer(\nself.systemTestEnv, self.testcaseEnv, replicationUtils)\nelse:\nkafka_system_test_utils.validate_simple_consumer_data_matched_across_replicas(self.systemTestEnv, self.testcaseEnv)\nkafka_system_test_utils.validate_broker_log_segment_checksum(self.systemTestEnv, self.testcaseEnv)\nkafka_system_test_utils.validate_data_matched(self.systemTestEnv, self.testcaseEnv, replicationUtils)\nkafka_system_test_utils.validate_index_log(self.systemTestEnv, self.testcaseEnv)\n3. The following are the details of the validation methods:\n=========================================================\nvalidate_data_matched\n=========================================================\n1. for each topic in the test case :\n2.     get a list of all message id in producer log\n3.     get a list of all message id in consumer log\n4.     subtract the list in #3 from #2 and return the diff\n5.     if the diff in #4 is 0 and both lengths of #2 & #3 are greater than 0 :\n6.         PASSED\n7.     else if \"acks == 1\" :\n8.         if the % of data loss in #4 <= ackOneDataLossThresholdPercent :\n9.             PASSED\n10.         else :\n11.             FAILED\n12.     else :\n13.         FAILED\n=========================================================\nvalidate_broker_log_segment_checksum\n=========================================================\n1. for each broker :\n2.     get log segment path (as shown below)\n# localLogSegmentPath :\n# .../system_test/mirror_maker_testsuite/testcase_5002/logs/broker-4/kafka_server_4_logs\n#   |- test_1-0\n#        |- 00000000000000000000.index\n#        |- 00000000000000000000.log\n#        |- 00000000000000000020.index\n#        |- 00000000000000000020.log\n#        |- . . .\n#   |- test_1-1\n#        |- 00000000000000000000.index\n#        |- 00000000000000000000.log\n#        |- 00000000000000000020.index\n#        |- 00000000000000000020.log\n#        |- . . .\n3.     for each topicPartition in localLogSegmentPath :\n4.         append each log segment file md5 hash\n5.     update the hexdigest from #3 & #4 into a dictionary such as brokerLogSegment : hexdigest (as shown below)\n# brokerLogCksumDict will look like this:\n# {\n#   'kafka_server_1_logs:tests_1-0': 'd41d8cd98f00b204e9800998ecf8427e',\n#   'kafka_server_1_logs:tests_1-1': 'd41d8cd98f00b204e9800998ecf8427e',\n#   'kafka_server_1_logs:tests_2-0': 'd41d8cd98f00b204e9800998ecf8427e',\n#   'kafka_server_1_logs:tests_2-1': 'd41d8cd98f00b204e9800998ecf8427e',\n#   'kafka_server_2_logs:tests_1-0': 'd41d8cd98f00b204e9800998ecf8427e',\n#   'kafka_server_2_logs:tests_1-1': 'd41d8cd98f00b204e9800998ecf8427e',\n#   'kafka_server_2_logs:tests_2-0': 'd41d8cd98f00b204e9800998ecf8427e',\n#   'kafka_server_2_logs:tests_2-1': 'd41d8cd98f00b204e9800998ecf8427e'\n# }\n6. re-arrange the checksum of each topic-partition from different replicas as shown below\n# {\n#   'test_1-0' : ['d41d8cd98f00b204e9800998ecf8427e','d41d8cd98f00b204e9800998ecf8427e'],\n#   'test_1-1' : ['d41d8cd98f00b204e9800998ecf8427e','d41d8cd98f00b204e9800998ecf8427e'],\n#   'test_2-0' : ['d41d8cd98f00b204e9800998ecf8427e','d41d8cd98f00b204e9800998ecf8427e'],\n#   'test_2-1' : ['d41d8cd98f00b204e9800998ecf8427e','d41d8cd98f00b204e9800998ecf8427e']\n# }\n7. for each all checksum (value) inside each topic-partitin (key) :\n#   'test_1-0' : ['d41d8cd98f00b204e9800998ecf8427e','d41d8cd98f00b204e9800998ecf8427e'],\n8.     if any checksum in the list is not equal to the first checksum :\n9.         increment failure_count\n10. if failure_count > 0 :\n11.     FAILED\n=========================================================\nvalidate_simple_consumer_data_matched_across_replicas\n=========================================================\n1. for each simple consumer :\n2.     for each topic :\n3.         get message id from consumer log and populate a list of topic-partition-messageid (as shown below)\n# replicaIdxMsgIdList :\n# - This is a list of dictionaries of topic-partition (key)\n#   mapping to list of MessageID in that topic-partition (val)\n# - The list index is mapped to (replicaId - 1)\n# [\n#  // list index = 0 => replicaId = idx(0) + 1 = 1\n#  {\n#      \"topic1-0\" : [ \"0000000001\", \"0000000002\", \"0000000003\"],\n#      \"topic1-1\" : [ \"0000000004\", \"0000000005\", \"0000000006\"]\n#  },\n#  // list index = 1 => replicaId = idx(1) + 1 = 2\n#  {\n#      \"topic1-0\" : [ \"0000000001\", \"0000000002\", \"0000000003\"],\n#      \"topic1-1\" : [ \"0000000004\", \"0000000005\", \"0000000006\"]\n#  }\n# ]\n4. take the first dictionary in replicaIdxMsgIdList (obtained from #2 & #3)\n5. for each topic-partition in the dictionary from #4 :\n6.     compare all replicas' MessageID in corresponding topic-partition\n7.     if there is any mismatch :\n8.         increment failure_count\nLogging of Kafka Components (Broker, ZK, Producer and Consumer)\nThe various logs from Broker, ZK, Producer & Consumer can be found in the logs directory of each individual test case after that test case is completed (regardless of the status of the test results)\n<kafka>\n|- /bin\n|- /config\n|- /contrib\n|- /core\n|- /lib\n|.\n|.\n|.\n|- /system_test\n|- /XXXX_testsuite\n|- /testcase_NNNN\n|- testcase_NNNN_properties.json\n|\n|  ## the directories below are generated in system test runtime ##\n|\n|- /config         # contains config / properties files after overriding TEMPLATE properties files\n|- /dashboards     # for future enhancement\n|- /logs\n|- /broker-1   # naming convention : <role name>-<entity id> (in this case, broker with entity id 1)\n|- /kafka_server_1_logs\n|- /test_1-0                           # topic : \"test_1\" of partition 0\n|- 00000000000000000000.index     # log index\n|- 00000000000000000000.log       # log segment file\n|- /test_1-1                           # topic : \"test_1\" of partition 1\n|- 00000000000000000000.index\n|- 00000000000000000000.log\n|- kafka_server_9091.log                    # kafka log on port 9091\n|- /broker-2\n. . .\n|- /broker-3\n. . .\n|- /console_consumer-5\n. . .\n|- /producer_performance-4\n. . .\n|- /zookeeper-0\n. . .\nTroubleshooting a failing case\nThe following describes the steps to troubleshoot a failing case running in a local machine.\nRefer to\nRunning System Test\non how to quick starting System Test\n1. Under <kafka>/system_test, execute the following command to start System Test :\n$ python -u -B system_test_runner.py 2>&1 | tee system_test_output.log\n2. When the test is completed, the following may be showing in the output:\n_test_case_name  :  testcase_5003\n_test_class_name  :  MirrorMakerTest\narg : bounce_leader  :  false\narg : bounce_mirror_maker  :  true\narg : bounced_entity_downtime_sec  :  30\narg : message_producing_free_time_sec  :  15\narg : num_iteration  :  1\narg : num_messages_to_produce_per_producer_call  :  50\narg : num_partition  :  1\narg : replica_factor  :  3\narg : sleep_seconds_between_producer_calls  :  1\nvalidation_status  :\nUnique messages from consumer on [test_1]  :  2440                      <- consumer has consumed 60 messsages less than producer produced\nUnique messages from producer on [test_1]  :  2500                      <- producer has produced 2500 messages\nValidate for data matched on topic [test_1]  :  FAILED                  <- data matched is \"FAILED\"\nValidate for merged log segment checksum in cluster [source]  :  PASSED\nValidate for merged log segment checksum in cluster [target]  :  PASSED\n3. The status of item \"Validate for data matched on topic [test_1]\" is FAILED. Take a look at system_test_output.log and search for \"validating data\":\ni. Producer produces 2500 messages              ----------------------------------------------------------------------------------------\n|\nii. Consumer consumes 2440 messages              ----------------------------------------------------------------------------------------|---\n|   |\niii. Data loss threshold 5% for Ack=1 case        ----------------------------------------------------------------------------------------|---|---\n|   |   |\niv. But this is not a Ack=1 test case            ----------------------------------------------------------------------------------------|---|---|---\n(Therefore, this case FAILED)                                                                                                        |   |   |   |\n|   |   |   |\n2013-07-23 09:25:53,769 - INFO - ======================================================                                               |   |   |   |\n2013-07-23 09:25:53,769 - INFO - validating data matched                                                                              |   |   |   |\n2013-07-23 09:25:53,769 - INFO - ======================================================                                               |   |   |   |\n2013-07-23 09:25:53,769 - DEBUG - #### Inside validate_data_matched (kafka_system_test_utils)                                         |   |   |   |\n2013-07-23 09:25:53,770 - DEBUG - working on topic : test_1 (kafka_system_test_utils)                                                 |   |   |   |\n2013-07-23 09:25:53,770 - DEBUG - matching consumer entity id found (kafka_system_test_utils)                                         |   |   |   |\n2013-07-23 09:25:53,826 - INFO - no. of unique messages on topic [test_1] sent from publisher  : 2500 (kafka_system_test_utils)  < ---    |   |   |\n2013-07-23 09:25:53,826 - INFO - no. of unique messages on topic [test_1] received by consumer : 2440 (kafka_system_test_utils)  < -------    |   |\n2013-07-23 09:25:53,826 - INFO - Data loss threshold % : 5.0 (kafka_system_test_utils)                                           < -----------    |\n2013-07-23 09:25:53,826 - WARNING - Data loss % on topic : test_1 : 2.4 (kafka_system_test_utils)                                < ---------------\n4. To further troubleshoot the failure, take a look at the log4j messages of each entity under the logs folder:\n<kafka>/system_test $ ls -l mirror_maker_testsuite/testcase_5003/logs\nbroker-4  broker-6  broker-8  console_consumer-11  mirror_maker-12  producer_performance-10  zookeeper-1  zookeeper-3\nbroker-5  broker-7  broker-9  dashboards           mirror_maker-13  zookeeper-0              zookeeper-2\n5. Check if there are any errors in producer performance log:\n<kafka>/system_test $ grep -i error mirror_maker_testsuite/testcase_5003/logs/producer_performance-10/producer_performance.log\n<kafka>/system_test $ grep -i exception mirror_maker_testsuite/testcase_5003/logs/producer_performance-10/producer_performance.log\n<kafka>/system_test $ grep -i fail mirror_maker_testsuite/testcase_5003/logs/producer_performance-10/producer_performance.log\n6. Check if there are any errors in mirror maker logs:\n<kafka>/system_test $ grep -i exception mirror_maker_testsuite/testcase_5003/logs/mirror_maker-12/mirror_maker_12.log\njava.nio.channels.ClosedByInterruptException\nNo partition metadata for topic test_1 due to kafka.common.LeaderNotAvailableException}] for topic [test_1]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)\nNo partition metadata for topic test_1 due to kafka.common.LeaderNotAvailableException}] for topic [test_1]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)\nNo partition metadata for topic test_1 due to kafka.common.LeaderNotAvailableException}] for topic [test_1]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)\nNo partition metadata for topic test_1 due to kafka.common.LeaderNotAvailableException}] for topic [test_1]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)\nNo partition metadata for topic test_1 due to kafka.common.LeaderNotAvailableException}] for topic [test_1]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)\nNo partition metadata for topic test_1 due to kafka.common.LeaderNotAvailableException}] for topic [test_1]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)\njava.nio.channels.ClosedByInterruptException\n<kafka>/system_test $ grep -i exception mirror_maker_testsuite/testcase_5003/logs/mirror_maker-13/mirror_maker_13.log\njava.nio.channels.ClosedByInterruptException\njava.nio.channels.ClosedByInterruptException\nAdding a Test Suite\nTo create a new test suite called \"broker_testsuite\" :\n1. Copy and paste system_test/replication_testsuite => system_test/broker_testsuite\n2. Rename system_test/broker_testsuite/replica_basic_test.py => system_test/broker_testsuite/broker_basic_test.py\n3. Edit system_test/broker_testsuite/broker_basic_test.py and update all ReplicaBasicTest related class name to BrokerBasicTest (as an example)\n4. Follow the flow of system_test/broker_testsuite/broker_basic_test.py and modify the necessary test logic accordingly.\n5. Most of the cases, you may remove the code in the indicated area as shown below\n.\n.\n.\n# ============================================================================== #\n# ============================================================================== #\n#                   Product Specific Testing Code Starts Here:                   #\n# ============================================================================== #\n# ============================================================================== #\n.\n.     REMOVE THE EXISTING CODE IN THIS AREA FOR YOUR NEW TESTSUITE\n.\nexcept Exception as e:\nself.log_message(\"Exception while running test {0}\".format(e))\ntraceback.print_exc()\nfinally:\nif not skipThisTestCase and not self.systemTestEnv.printTestDescriptionsOnly:\nself.log_message(\"stopping all entities - please wait ...\")\nkafka_system_test_utils.stop_all_remote_running_processes(self.systemTestEnv, self.testcaseEnv)\nAdding a Test Case\nTo create a new test case under \"replication_testsuite\" :\n1. Copy and paste system_test/replication_testsuite/testcase_1 => system_test/replication_testsuite/testcase_2\n2. Rename system_test/replication_testsuite/testcase_2/testcase_1_properties.json => system_test/replication_testsuite/testcase_2/testcase_2_properties.json\n3. Update system_test/replication_testsuite/testcase_2/testcase_2_properties.json with the corresponding settings for testcase 2.\nLogging of System Test Framework\nSystem Test has its own logging messages to facilitate troubleshooting. This is not the same as the logging messages in Broker, ZK, Producer, Consumer (which are specified in <kafka>/config/log4j.properties)\nBy default, System Test has log level configured in INFO. To change to DEBUG level :\n1. Modify <kafka>/system_test/logging.conf\n2. In the section shown below, change level=INFO => level=DEBUG\n# ==============================================\n# handlers session\n# ** Change 'level' to INFO/DEBUG in this session\n# ==============================================\n[handler_namedConsoleHandler]\nclass=\"StreamHandler\"\nlevel=INFO\nformatter=namedFormatter\nargs=[]\nSupported Platform\nLinux\nObsolete Files\nThe following are not part of this Python based System Test framework\nsystem_test/broker_failure\nsystem_test/common\nsystem_test/mirror_maker\nsystem_test/producer_perf",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/Kafka+System+Tests",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
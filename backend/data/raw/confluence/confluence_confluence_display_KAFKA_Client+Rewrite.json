{
  "id": "confluence_display_KAFKA_Client+Rewrite",
  "title": "Client Rewrite - Apache Kafka - Apache Software Foundation",
  "content": "Over several years of usage we have run into a number of complaints about the scala client. This is a proposal for addressing these complaints.\nThis would likely be done in the timeframe of a 0.9 release.\nCurrent Problems and Fixes\nHere is a dump of all the problems we have seen and some solutions:\nMove clients to Java to fix scala problems\nJavadoc\nScala version non-compatability\nReadability by non-scala users\nScary stack traces\nLeakage of scala classes/interfaces into java api\nCode cleanup and embeddability\nBoth producer and consumer code are extremely hard to understand\nRedo\nthe request serialization layer to avoid all the custom request definition objects\nEliminate the \"simple\" consumer api and have only a single consumer API with the capabilities of both\nRemove all threads from the consumer\nHave a separate client jar with no depedencies\nGeneralize APIs\nProducer\nGive back a return value containing error code, offset, etc\nConsumer\nEnable static partition assignment for stateful data systems\nEnable consumer-driven offset changes.\nBetter support non-java consumers\nMove to a high-level protocol for consumer group management to centralize complexity on the server for all clients\nImprove performance and operability\nMake the producer fully async to to allow issuing sends to all brokers simultaneously and having multiple in-flight requests simultaneously. This will dramatically reduce the impact of latency on throughput (which is important with replication).\nMove to server-side offset management will allow us to scale this facility which is currently a big scalability problem for high-commit rate consumers due to zk non scalability.\nServer-side group membership will be more scalable with number of partitions then the current consumer co-ordination protocol\nImprove inefficiencies in compression code\nThe idea would be to roll out the new api as a separate jar, leaving the existing client intact but deprecated for one or two releases before removing the old client. This should allow a gradual migration.\nProposed Producer API\nSendResponse send(KafkaMessage... message);\nUsage:\nProducer producer = new Producer(new ProducerConfig(props));\nSendResponse r = producer.send(new KafkaMessage(topic, key, message));\nr.onCompletion(new Runnable() {System.out.println(\"All done\")})\nr.getOffset()\nr.getError()\nChanges from current API\nThe key and message in KafkaMessage would have type Object instead of parameterized types. The parameterized types have not played well with the fact that different topics may take different types and since the serializer is instantiated via reflection (at runtime) the parametric types add no actual type safety.\nThe producer will always attempt to batch data and will always immediately return a SendResponse which acts as a Future to allow the client to await the completion of the request.\nImplementation\nAs today we will have a single background thread that does message collation, serialization, and compression. However this thread will now run an event loop to simultaneously send requests and receive responses from all brokers.\nIn 0.8, the producer throughput is limited due to the synchronous interaction of the producer with the broker. At a time, there can only be one request in flight per broker and that hurts the throughput of the producer severely. The problem with this approach also is that it doesn't take advantage of parallelism on the broker where requests for different partitions can be handled by different request handler threads on the broker, thereby increasing the throughput seen by the producer.\nKey idea behind the design\nRequest pipelining allows multiple requests to the same broker to be in flight. The idea is to do I/O multiplexing using select(), epoll(), wait() and wait for at least one readable or writable socket instead of doing blocking reads and writes. So if a producer sends data to 3 partitions on 2 brokers, the difference between blocking sends/receives and pipelined sends/receives can be understood by looking at the following picture\nDesign requirements\nOrdering should be maintained per partition, if there are no retries.\nExisting producer API should remain completely asynchronous.\nNo APIs changes are necessary to roll this out.\nProposed design\nThere are 2 threads, depending on how metadata requests are handled.\nClient thread - partitions\nSend thread - refreshes metadata, serializes, collates, batches, sends data and receives acknowledgments\nEvent Queuing\nThere are 2 choices here -\nOne queue per partition\nThere will be one queue with partition \"-1\" per topic. The events with null key will enter this queue. There will be on \"undesignated\" partition queue per topic. This is done to prevent fetching metadata on the client thread when the producer sends the first message for a new topic.\nPros:\nMore isolation within a topic. Keys that are high traffic will not affect keys that are lower traffic within a topic. This will protect important low throughput topics like audit data.\nEasier to handle the most common error which is leader transition. It is easier with one queue per partition since you can just ignore the queues for partitions that don't have leaders yet.\nCons:\nMore queues means more memory overhead, especially for tools like MirrorMaker.\nHandling time based event expiration per partition queue complicates the code to some extent.\nOne queue for all topics and partitions.\nPros:\nLess memory overhead since there is only one queue data structure for all events\nCons:\nLess isolation. One high throughput topic or partition can cause data for other topics to be dropped. This especially hurts audit data and can be easily avoided by having multiple queues.\nComplicates batching since you will have to find a way to skip over partitions that don't have leaders and avoid dequeueing their data until a leader can be discovered.\nPartitioning\nPartitioning can happen before the event enters the queue. The advantage is that the event does not require re-queuing if one queue per partition approach is used. Events with null key will be queued to the <topic>-\"-1\" queue.\nMetadata discovery\nWhen a producer sends the first message for a new topic, it enters the <topic>-undesignated queue. The metadata fetch happens on the event thread. There are 2 choices on how the metadata fetch request will work -\nThe metadata fetch is a synchronous request in the event loop.\nPros:\nSimplicity of code\nCons:\nIf leaders only for a subset of partitions have changed, a synchronous metadata request can potentially hurt the throughput for other topics/partitions.\nMetadata fetch is non blocking\nPros:\nMore isolation, better overall throughput\nSerialization\nOne option is doing this before the event enters the queue and after partitioning. Downside is potentially slowing down the client thread if compression or serialization takes long.  Another option is to just do this in the send thread, which seems like a better choice.\nBatching and Collation\nProducer maintains a map of broker to list of partitions that the broker leads. Batch size is per partition. For each broker, if the key is in write state, the producer's send thread will poll the queues for the partitions that the broker leads. Once the batch is full, it will create a ProducerRequest with the partitions and data, compress the data, and writes the request on the socket. This happens in the event thread while handing new requests. The collation logic gets a little complicated if there is only one queue for all topics/partitions.\nCompression\nWhen the producer send thread polls each partition's queue, it compresses the batch of messages that it dequeues.\nEvent loop\nwhile(isRunning)\n{\n// configure any new broker connections\n// select readable keys\n// handle responses\n// select writable keys\n// handle topic metadata requests, if there are non-zero partitions in error\n// handle incomplete requests\n// handle retries, if any\n// handle new requests\n}\nConsumer API\nHere is an example of the proposed consumer API:\nConsumer consumer = new Consumer(props);\nconsumer.addTopic(\"my-topic\"); // consume dynamically assigned partitions\nconsumer.addTopic(\"my-other-topic\", 3); //\nlong timeout = 1000;\nwhile(true) {\nList<MessageAndOffset> messages = consumer.poll(timeout);\nprocess(messages);\nconsumer.commit(); // alternately consumer.commit(topic) or consumer.commit(topic, partition)\n}\nAs before the consumer group is set in the properties and for simplicity each consumer can belong to only one group.\naddTopic is used to change the set of topics the consumer consumes. If the user gives only the topic name it will automatically be assigned a set of partitions based on the group membership protocol below. If the user specifies a partition they will be statically assigned that partition.\nThe actual partition assignment can be made pluggable (see the proposal on group membership below) by setting an assignment strategy in the properties.\nThis api allows alternative methods for managing offsets as today by simply disabling autocommit and not calling commit().\nImplementation\nThe client would be entirely single threaded.\nThe poll() method executes the network event loop, which includes the following:\nChecks for group membership changes\nSends a heartbeat to the controller if needed\nIssues a fetch request to all brokers for which the consumer currently is consuming\nReads any available fetch responses\nIssues metadata requests when requests fail to get the new cluster topology\nThe timeout the user specifies will be purely to ensure we have a mechanism to give control back to the user even when no messages are delivered. It is up to the user to ensure poll() is called again within the heartbeat frequency set for the consumer group. Internally the timeout on our select() may uses a shorter timeout to ensure the heartbeat frequency is met even when no messages are delivered.\nConsumer Group Membership APIs\nWe will introduce a set of RPC apis for managing partition assignment on the consumer side. This will be based on the prototype\nhere\n. This set of APIs is orthogonal to the APIs for producing and consuming data, it is responsible for group membership.\napi\nsender\ndescription\nissue to\nListGroups\nclient\nReturns metadata for one or more groups. If issued with a list of GroupName it returns metadata for just those groups. If no GroupName is given it returns metadata for all active groups. This request can be issued to any server.\nany server\nCreateGroup\nclient\nCreate a new group with the given name and the specified minimum heartbeat frequency. Return the id/host/port of the server acting as the controller for that group.\nIf the ephemeral flag is set the group will disappear when the last client exits.\nany server\nDeleteGroup\nclient\nDeletes a non-emphemeral group (but only if it has no members)\ncontroller\nRegisterConsumer\nclient\nAsk to join the given group. This happens as part of the event loop that gets invoked when the consumer uses the poll() API\ncontroller\nHeartbeat\nclient\nThis heartbeat message must be sent by the consumer to the controller with an SLA specified in the CreateGroup command. The client can and should issues these more frequently to avoid accidentally timing out.\ncontroller\nHeartbeat\nHeartbeat\nVersion            => int16\nCorrelationId      => int64\nClientId           => string\nSessionTimeout     => int64\nHeartbeatResponse\nVersion                => int16\nCorrelationId          => int64\nControllerGeneration   => int64\nErrorCode              => int16 // error code is non zero if the group change is to be initiated\nCreateGroup\nVersion                   => int16\nCorrelationId             => int64\nClientId                  => string\nConsumerGroup             => string\nSessionTimeout            => int64\nTopics                    => [string]\nCreateGroupResponse\nVersion                    => int16\nCorrelationId              => int64\nConsumerGroup              => string\nErrorCode                  => int16\nDeleteGroup\nVersion                   => int16\nCorrelationId             => int64\nClientId                  => string\nConsumerGroup             => [string]\nDeleteGroupResponse\nVersion                    => int16\nCorrelationId              => int64\nDeletedConsumerGroups      => [string]\nErrorCode                  => int16\nRegisterConsumer\nVersion                    => int16\nCorrelationId              => int64\nClientId                   => string\nConsumerGroup              => string\nConsumerId                 => string\nRegisterConsumerResponse\nVersion                    => int16\nCorrelationId              => int64\nClientId                   => string\nConsumerGroup              => string\nPartitionsToOwn            => [{Topic Partition Offset}]\nTopic                    => string\nPartition                => int16\nOffset                   => int64\nErrorCode                  => int16\nListGroups\nVersion                    => int16\nCorrelationId              => int64\nClientId                   => string\nConsumerGroups             => [string]\nListGroupsResponse\nVersion                    => int16\nCorrelationId              => int64\nClientId                   => string\nGroupsInfo                 => [{GroupName, GroupMembers, Topics, ControllerBroker, SessionTimeout}]\nGroupName                => string\nGroupMembers             => [string]\nTopics                   => [string]\nControllerBroker         => Broker\nBroker                => BrokerId Host Port\nBrokerId              => int32\nHost                  => string\nPort                  => int16\nErrorCode                  => int16\nRewindConsumer\nVersion                    => int16\nCorrelationId              => int64\nClientId                   => string\nConsumerGroup              => string\nNewOffsets                 => [{Topic Partition Offset}]\nTopic                    => string\nPartition                => int16\nOffset                   => int64\nRewindConsumerResponse\nVersion                    => int16\nCorrelationId              => int64\nConsumerGroup              => string\nActualOffsets              => [{Topic Partition Offset}]\nTopic                    => string\nPartition                => int16\nOffset                   => int64\nErrorCode                  => int16\nConsumer Group Membership Protocol\nThe use of this protocol would be as follows:\nOn startup or when co-ordinator heartbeat fails -\non startup the consumer issues a list_groups(my_group) to a random broker to find out the location of the controller for its group (each server is the controller for some groups)\nknowing where the controller is, it connects and issues a RegisterConsumer RPC, then it awaits a RegisterConsumerResponse\non receiving the RegisterConsumer request the server sends an error code in the HeartbeatResponse to all alive group members and waits for a new RegisterConsumer request from each consumer, except the one that sent the initial RegisterConsumer.\non receiving an error code in the HeartbeatResponse, all consumers stop fetching, commit offsets, re-discover the controller through list_groups(my_group) and send a RegisterConsumer request to the controller\non receiving RegisterConsumer from all consumers of the group membership change, the controller sends the RegisterConsumerResponse to all the new group members, with the partitions and the respective offsets to restart consumption from. If there are no previous offsets for the consumer group, -1 is returned. The consumer starts fetching from earliest or latest, depending on consumer configuration\non receiving the RegisterConsumerResponse the consumer is now able to start consuming its partitions and must now start sending heartbeat messages back to the controller with the current generation id.\nWhen new partitions are added to existing topics or new topics are created -\non discovering newly created topics or newly added partitions to existing topics, the controller sends an error code in the HeartbeatResponse to all alive group members and waits for a new RegisterConsumer request from each consumer, except the one that sent the initial RegisterConsumer.\non receiving an error code in the HeartbeatResponse, all consumers stop fetching, commit offsets, re-discover the controller through list_groups(my_group) and send a RegisterConsumer request to the controller\non receiving RegisterConsumer from all consumers of the group membership change, the controller sends the RegisterConsumerResponse to all the new group members, with the new partitions and the respective offsets to restart consumption from. For newly added partitions and topics, the offset is set to smallest (0L).\non receiving the RegisterConsumerResponse the consumer is now able to start consuming its partitions and must now start sending heartbeat messages back to the controller with the current generation id.\nOffset Rewind\nwhile(true) {\nList<MessageAndMetadata> messages = consumer.poll(timeout);\nprocess(messages);\nif(rewind_required) {\nList<PartitionOffset> partitionOffsets = new ArrayList<PartitionOffset>();\npartitionOffsets.add(new PartitionOffset(topic, partition, offset));\nrewind_offsets(partitionOffsets);\n}\n}\nthe consumer stops fetching data, sends a RewindConsumer request to the controller and awaits a RewindConsumerResponse\non receiving a RewindConsumer request, the controller sends an error code in the HeartbeatResponse to the current owners of the rewound partitions\non receiving an error code in the HeartbeatResponse, the affected consumers stop fetching, commit offsets and send the RegisterConsumer request to the controller\non receiving a RegisterConsumer request from the affected members of the group, the controller records the new offsets for the specified partitions and sends the new offsets to the respective consumers\non receiving a RegisterConsumerResponse, the consumers start fetching data from the specified offsets\nLow-level RPC API\nTBD we need to design an underlying client network abstraction that can be used to send requests. This should ideally handle metadata and addressing by broker id, multiplex over multiple connections, and support both blocking and non-blocking operations.\nSomething like\nKafkaConnection connection = new KafkaConnection(bootstrapUrl, socket_props);\nList<Responses> connection.poll(timeout, Request...requests);\nThis is an async api so the responses responses returned have no relationship to the request sent. This is meant to be called from within an event loop--i.e. each call to poll corresponds to one iteration of the select() loop. poll() with no request simply checks for any new responses to previous requests or non-client-initiated communication.\nMisc Notes and questions\nCurrently we do a lot of logging and metrics recording in the client. I think instead of handling reporting of metrics and logging we should instead incorporate this feedback in the client apis and allow the user to log or monitor this in the manner they chose. This will avoid dependence on a particular logging or metrics library and seems like a good policy.\nIt is not clear how to support changing the offset either manually or programmatically while consumption is happening. Simply using the api to set the offset will likely collide with commit calls from the active consumer. Perhaps this is not needed?\nHow many jars should we have? I think we could do either\nOption A: kafka-clients.jar and kafka-server.jar or\nOption B: kakfa-common.jar, kafka-producer.jar, kafka-consumer.jar, and kafka-server.jar\nI prefer Option A as it is simpler if we add an AdminApi--we won't need to introduce a whole new jar for it.",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/Client+Rewrite",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
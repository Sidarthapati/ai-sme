{
  "id": "confluence_display_KAFKA_0.8.0+SimpleConsumer+Example",
  "title": "0.8.0 SimpleConsumer Example - Apache Kafka - Apache Software Foundation",
  "content": "Using SimpleConsumer\nWhy use SimpleConsumer?\nThe main reason to use a SimpleConsumer implementation is you want greater control over partition consumption than Consumer Groups give you.\nFor example you want to:\nRead a message multiple times\nConsume only a subset of the partitions in a topic in a process\nManage transactions to make sure a message is processed once and only once\nDownsides of using SimpleConsumer\nThe SimpleConsumer does require a significant amount of work not needed in the Consumer Groups:\nYou must keep track of the offsets in your application to know where you left off consuming.\nYou must figure out which Broker is the lead Broker for a topic and partition\nYou must handle Broker leader changes\nSteps for using a SimpleConsumer\nFind an active Broker and find out which Broker is the leader for your topic and partition\nDetermine who the replica Brokers are for your topic and partition\nBuild the request defining what data you are interested in\nFetch the data\nIdentify and recover from leader changes\nFinding the Lead Broker for a Topic and Partition\nThe easiest way to do this is to pass in a set of known Brokers to your logic, either via a properties file or the command line. These don’t have to be all the Brokers in the cluster, rather just a set where you can start looking for a live Broker to query for Leader information.\nprivate PartitionMetadata findLeader(List<String> a_seedBrokers, int a_port, String a_topic, int a_partition) {\nPartitionMetadata returnMetaData = null;\nloop:\nfor (String seed : a_seedBrokers) {\nSimpleConsumer consumer = null;\ntry {\nconsumer = new SimpleConsumer(seed, a_port, 100000, 64 * 1024, \"leaderLookup\");\nList<String> topics = Collections.singletonList(a_topic);\nTopicMetadataRequest req = new TopicMetadataRequest(topics);\nkafka.javaapi.TopicMetadataResponse resp = consumer.send(req);\nList<TopicMetadata> metaData = resp.topicsMetadata();\nfor (TopicMetadata item : metaData) {\nfor (PartitionMetadata part : item.partitionsMetadata()) {\nif (part.partitionId() == a_partition) {\nreturnMetaData = part;\nbreak loop;\n}\n}\n}\n} catch (Exception e) {\nSystem.out.println(\"Error communicating with Broker [\" + seed + \"] to find Leader for [\" + a_topic\n+ \", \" + a_partition + \"] Reason: \" + e);\n} finally {\nif (consumer != null) consumer.close();\n}\n}\nif (returnMetaData != null) {\nm_replicaBrokers.clear();\nfor (kafka.cluster.Broker replica : returnMetaData.replicas()) {\nm_replicaBrokers.add(replica.host());\n}\n}\nreturn returnMetaData;\n}\nThe call to topicsMetadata() asks the Broker you are connected to for all the details about the topic we are interested in.\nThe loop on partitionsMetadata iterates through all the partitions until we find the one we want. Once we find it, we can break out of all the loops.\nFinding Starting Offset for Reads\nNow define where to start reading data. Kafka includes two constants to help, kafka.api.OffsetRequest.EarliestTime() finds the beginning of the data in the logs and starts streaming from there, kafka.api.OffsetRequest.LatestTime() will only stream new messages. Don’t assume that offset 0 is the beginning offset, since messages age out of the log over time.\npublic static long getLastOffset(SimpleConsumer consumer, String topic, int partition,\nlong whichTime, String clientName) {\nTopicAndPartition topicAndPartition = new TopicAndPartition(topic, partition);\nMap<TopicAndPartition, PartitionOffsetRequestInfo> requestInfo = new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();\nrequestInfo.put(topicAndPartition, new PartitionOffsetRequestInfo(whichTime, 1));\nkafka.javaapi.OffsetRequest request = new kafka.javaapi.OffsetRequest(requestInfo, kafka.api.OffsetRequest.CurrentVersion(),clientName);\nOffsetResponse response = consumer.getOffsetsBefore(request);\nif (response.hasError()) {\nSystem.out.println(\"Error fetching data Offset Data the Broker. Reason: \" + response.errorCode(topic, partition) );\nreturn 0;\n}\nlong[] offsets = response.offsets(topic, partition);\nreturn offsets[0];\n}\nError Handling\nSince the SimpleConsumer doesn't handle lead Broker failures, you have to write a bit of code to handle it.\nif (fetchResponse.hasError()) {\nnumErrors++;\n// Something went wrong!\nshort code = fetchResponse.errorCode(a_topic, a_partition);\nSystem.out.println(\"Error fetching data from the Broker:\" + leadBroker + \" Reason: \" + code);\nif (numErrors > 5) break;\nif (code == ErrorMapping.OffsetOutOfRangeCode())  {\n// We asked for an invalid offset. For simple case ask for the last element to reset\nreadOffset = getLastOffset(consumer,a_topic, a_partition, kafka.api.OffsetRequest.LatestTime(), clientName);\ncontinue;\n}\nconsumer.close();\nconsumer = null;\nleadBroker = findNewLeader(leadBroker, a_topic, a_partition, a_port);\ncontinue;\n}\nHere, once the fetch returns an error, we log the reason, close the consumer then try to figure out who the new leader is.\nprivate String findNewLeader(String a_oldLeader, String a_topic, int a_partition, int a_port) throws Exception {\nfor (int i = 0; i < 3; i++) {\nboolean goToSleep = false;\nPartitionMetadata metadata = findLeader(m_replicaBrokers, a_port, a_topic, a_partition);\nif (metadata == null) {\ngoToSleep = true;\n} else if (metadata.leader() == null) {\ngoToSleep = true;\n} else if (a_oldLeader.equalsIgnoreCase(metadata.leader().host()) && i == 0) {\n// first time through if the leader hasn't changed give ZooKeeper a second to recover\n// second time, assume the broker did recover before failover, or it was a non-Broker issue\n//\ngoToSleep = true;\n} else {\nreturn metadata.leader().host();\n}\nif (goToSleep) {\ntry {\nThread.sleep(1000);\n} catch (InterruptedException ie) {\n}\n}\n}\nSystem.out.println(\"Unable to find new leader after Broker failure. Exiting\");\nthrow new Exception(\"Unable to find new leader after Broker failure. Exiting\");\n}\nThis method uses the findLeader() logic we defined earlier to find the new leader, except here we only try to connect to one of the replicas for the topic/partition. This way if we can’t reach any of the Brokers with the data we are interested in we give up and exit hard.\nSince it may take a short time for ZooKeeper to detect the leader loss and assign a new leader, we sleep if we don’t get an answer. In reality ZooKeeper often does the failover very quickly so you never sleep.\nReading the Data\nFinally we read the data being streamed back and write it out.\n// When calling FetchRequestBuilder, it's important NOT to call .replicaId(), which is meant for internal use only.\n// Setting the replicaId incorrectly will cause the brokers to behave incorrectly.\nFetchRequest req = new FetchRequestBuilder()\n.clientId(clientName)\n.addFetch(a_topic, a_partition, readOffset, 100000)\n.build();\nFetchResponse fetchResponse = consumer.fetch(req);\nif (fetchResponse.hasError()) {\n// See code in previous section\n}\nnumErrors = 0;\nlong numRead = 0;\nfor (MessageAndOffset messageAndOffset : fetchResponse.messageSet(a_topic, a_partition)) {\nlong currentOffset = messageAndOffset.offset();\nif (currentOffset < readOffset) {\nSystem.out.println(\"Found an old offset: \" + currentOffset + \" Expecting: \" + readOffset);\ncontinue;\n}\nreadOffset = messageAndOffset.nextOffset();\nByteBuffer payload = messageAndOffset.message().payload();\nbyte[] bytes = new byte[payload.limit()];\npayload.get(bytes);\nSystem.out.println(String.valueOf(messageAndOffset.offset()) + \": \" + new String(bytes, \"UTF-8\"));\nnumRead++;\na_maxReads--;\n}\nif (numRead == 0) {\ntry {\nThread.sleep(1000);\n} catch (InterruptedException ie) {\n}\n}\nNote that the ‘readOffset’ asks the last read message what the next Offset would be. This way when the block of messages is processed we know where to ask Kafka where to start the next fetch.\nAlso note that we are explicitly checking that the offset being read is not less than the offset that we requested. This is needed since if Kafka is compressing the messages, the fetch request will return an entire compressed block even if the requested offset isn't the beginning of the compressed block. Thus a message we saw previously may be returned again. Note also that we ask for a fetchSize of 100000 bytes. If the Kafka producers are writing large batches, this might not be enough, and might return an empty message set. In this case, the fetchSize should be increased until a non-empty set is returned.\nFinally, we keep track of the # of messages read. If we didn't read anything on the last request we go to sleep for a second so we aren't hammering Kafka when there is no data.\nRunning the example\nThe example expects the following parameters:\nMaximum number of messages to read (so we don’t loop forever)\nTopic to read from\nPartition to read from\nOne broker to use for Metadata lookup\nPort the brokers listen on\nFull Source Code\npackage com.test.simple;\nimport kafka.api.FetchRequest;\nimport kafka.api.FetchRequestBuilder;\nimport kafka.api.PartitionOffsetRequestInfo;\nimport kafka.common.ErrorMapping;\nimport kafka.common.TopicAndPartition;\nimport kafka.javaapi.*;\nimport kafka.javaapi.consumer.SimpleConsumer;\nimport kafka.message.MessageAndOffset;\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\npublic class SimpleExample {\npublic static void main(String args[]) {\nSimpleExample example = new SimpleExample();\nlong maxReads = Long.parseLong(args[0]);\nString topic = args[1];\nint partition = Integer.parseInt(args[2]);\nList<String> seeds = new ArrayList<String>();\nseeds.add(args[3]);\nint port = Integer.parseInt(args[4]);\ntry {\nexample.run(maxReads, topic, partition, seeds, port);\n} catch (Exception e) {\nSystem.out.println(\"Oops:\" + e);\ne.printStackTrace();\n}\n}\nprivate List<String> m_replicaBrokers = new ArrayList<String>();\npublic SimpleExample() {\nm_replicaBrokers = new ArrayList<String>();\n}\npublic void run(long a_maxReads, String a_topic, int a_partition, List<String> a_seedBrokers, int a_port) throws Exception {\n// find the meta data about the topic and partition we are interested in\n//\nPartitionMetadata metadata = findLeader(a_seedBrokers, a_port, a_topic, a_partition);\nif (metadata == null) {\nSystem.out.println(\"Can't find metadata for Topic and Partition. Exiting\");\nreturn;\n}\nif (metadata.leader() == null) {\nSystem.out.println(\"Can't find Leader for Topic and Partition. Exiting\");\nreturn;\n}\nString leadBroker = metadata.leader().host();\nString clientName = \"Client_\" + a_topic + \"_\" + a_partition;\nSimpleConsumer consumer = new SimpleConsumer(leadBroker, a_port, 100000, 64 * 1024, clientName);\nlong readOffset = getLastOffset(consumer,a_topic, a_partition, kafka.api.OffsetRequest.EarliestTime(), clientName);\nint numErrors = 0;\nwhile (a_maxReads > 0) {\nif (consumer == null) {\nconsumer = new SimpleConsumer(leadBroker, a_port, 100000, 64 * 1024, clientName);\n}\nFetchRequest req = new FetchRequestBuilder()\n.clientId(clientName)\n.addFetch(a_topic, a_partition, readOffset, 100000) // Note: this fetchSize of 100000 might need to be increased if large batches are written to Kafka\n.build();\nFetchResponse fetchResponse = consumer.fetch(req);\nif (fetchResponse.hasError()) {\nnumErrors++;\n// Something went wrong!\nshort code = fetchResponse.errorCode(a_topic, a_partition);\nSystem.out.println(\"Error fetching data from the Broker:\" + leadBroker + \" Reason: \" + code);\nif (numErrors > 5) break;\nif (code == ErrorMapping.OffsetOutOfRangeCode())  {\n// We asked for an invalid offset. For simple case ask for the last element to reset\nreadOffset = getLastOffset(consumer,a_topic, a_partition, kafka.api.OffsetRequest.LatestTime(), clientName);\ncontinue;\n}\nconsumer.close();\nconsumer = null;\nleadBroker = findNewLeader(leadBroker, a_topic, a_partition, a_port);\ncontinue;\n}\nnumErrors = 0;\nlong numRead = 0;\nfor (MessageAndOffset messageAndOffset : fetchResponse.messageSet(a_topic, a_partition)) {\nlong currentOffset = messageAndOffset.offset();\nif (currentOffset < readOffset) {\nSystem.out.println(\"Found an old offset: \" + currentOffset + \" Expecting: \" + readOffset);\ncontinue;\n}\nreadOffset = messageAndOffset.nextOffset();\nByteBuffer payload = messageAndOffset.message().payload();\nbyte[] bytes = new byte[payload.limit()];\npayload.get(bytes);\nSystem.out.println(String.valueOf(messageAndOffset.offset()) + \": \" + new String(bytes, \"UTF-8\"));\nnumRead++;\na_maxReads--;\n}\nif (numRead == 0) {\ntry {\nThread.sleep(1000);\n} catch (InterruptedException ie) {\n}\n}\n}\nif (consumer != null) consumer.close();\n}\npublic static long getLastOffset(SimpleConsumer consumer, String topic, int partition,\nlong whichTime, String clientName) {\nTopicAndPartition topicAndPartition = new TopicAndPartition(topic, partition);\nMap<TopicAndPartition, PartitionOffsetRequestInfo> requestInfo = new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();\nrequestInfo.put(topicAndPartition, new PartitionOffsetRequestInfo(whichTime, 1));\nkafka.javaapi.OffsetRequest request = new kafka.javaapi.OffsetRequest(\nrequestInfo, kafka.api.OffsetRequest.CurrentVersion(), clientName);\nOffsetResponse response = consumer.getOffsetsBefore(request);\nif (response.hasError()) {\nSystem.out.println(\"Error fetching data Offset Data the Broker. Reason: \" + response.errorCode(topic, partition) );\nreturn 0;\n}\nlong[] offsets = response.offsets(topic, partition);\nreturn offsets[0];\n}\nprivate String findNewLeader(String a_oldLeader, String a_topic, int a_partition, int a_port) throws Exception {\nfor (int i = 0; i < 3; i++) {\nboolean goToSleep = false;\nPartitionMetadata metadata = findLeader(m_replicaBrokers, a_port, a_topic, a_partition);\nif (metadata == null) {\ngoToSleep = true;\n} else if (metadata.leader() == null) {\ngoToSleep = true;\n} else if (a_oldLeader.equalsIgnoreCase(metadata.leader().host()) && i == 0) {\n// first time through if the leader hasn't changed give ZooKeeper a second to recover\n// second time, assume the broker did recover before failover, or it was a non-Broker issue\n//\ngoToSleep = true;\n} else {\nreturn metadata.leader().host();\n}\nif (goToSleep) {\ntry {\nThread.sleep(1000);\n} catch (InterruptedException ie) {\n}\n}\n}\nSystem.out.println(\"Unable to find new leader after Broker failure. Exiting\");\nthrow new Exception(\"Unable to find new leader after Broker failure. Exiting\");\n}\nprivate PartitionMetadata findLeader(List<String> a_seedBrokers, int a_port, String a_topic, int a_partition) {\nPartitionMetadata returnMetaData = null;\nloop:\nfor (String seed : a_seedBrokers) {\nSimpleConsumer consumer = null;\ntry {\nconsumer = new SimpleConsumer(seed, a_port, 100000, 64 * 1024, \"leaderLookup\");\nList<String> topics = Collections.singletonList(a_topic);\nTopicMetadataRequest req = new TopicMetadataRequest(topics);\nkafka.javaapi.TopicMetadataResponse resp = consumer.send(req);\nList<TopicMetadata> metaData = resp.topicsMetadata();\nfor (TopicMetadata item : metaData) {\nfor (PartitionMetadata part : item.partitionsMetadata()) {\nif (part.partitionId() == a_partition) {\nreturnMetaData = part;\nbreak loop;\n}\n}\n}\n} catch (Exception e) {\nSystem.out.println(\"Error communicating with Broker [\" + seed + \"] to find Leader for [\" + a_topic\n+ \", \" + a_partition + \"] Reason: \" + e);\n} finally {\nif (consumer != null) consumer.close();\n}\n}\nif (returnMetaData != null) {\nm_replicaBrokers.clear();\nfor (kafka.cluster.Broker replica : returnMetaData.replicas()) {\nm_replicaBrokers.add(replica.host());\n}\n}\nreturn returnMetaData;\n}\n}",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
{
  "id": "confluence_display_KAFKA_Kafka+API+Refactoring",
  "title": "Kafka API Refactoring - Apache Kafka - Apache Software Foundation",
  "content": "Current Architecture\nIn 0.8.1.1 with\nin-built offset management\n, server side architecture with the life cycle of the produce/fetch request is summarized below (caller --> callee):\nProduceRequest --> 1) KafkaApis.appendToLocalLog()         --> ReplicaManager.getPartition()        --> Partition.appendMessagesToLeader()\n2) KafkaApis.maybeUnblockDelayedFetch()\n3) RequestChannel.sendResponse()   OR   ProducerRequestPurgatory.watch()\n4) ProducerRequestPurgatory.update()\n------------------------------------------------------------------------------------------------------\nFetchRequest -->   1) KafkaApis.maybeUpdatePartitionHW()   --> ReplicaManager.recordFollowerPosition()   -->   Partition.updateLeaderHWAndMaybeExpandIsr() / maybeIncrementLeaderHW()\n2) ProducerRequestPurgatory.update()    --> DelayedProduce.isSatisfied()              --> KafkaApis.maybeUnblockDelayedFetchRequests()\n3) KafkaApis.readMessageSet             --> ReplicaManager.getReplica()               --> Log.read()\n4) RequestChannel.sendResponse()   OR   FetchRequestPurgatory.watch()\n------------------------------------------------------------------------------------------------------\nProducerRequestPurgatory:\n// called as step 4) of handling produce request, or step 2) of handling fetch request\nProducerRequestPurgatory.update()                       --> DelayedProduce.respond()           --> RequestChannel.sendResponse()\n// any time\nProducerRequestPurgatory.expire()                       --> DelayedProduce.respond()           --> RequestChannel.sendResponse()\nFetchRequestPurgatory:\n// called as step 2) of handling produce request, or inside DelayedProduce.isSatisfied()\nKafkaApis.maybeUnblockDelayedFetchRequests()            --> FetchRequestPurgatory.update()     --> DelayedFetch.respond()             --> RequestChannel.sendResponse()\n// any time\nFetchRequestPurgatory.expire()                          --> DelayedFetch.respond()             --> RequestChannel.sendResponse()\nThe problem\nAs we can see from above, since delayed produce needs to access KafkaApis.maybeUnblockDelayedFetchRequests(), etc, and delayed fetch needs to fetch the data to form the response. As a result, we ended up keeping the appending and fetching logic inside KafkaAPIs and also keeping purgatories/delayed requests inside Kafka APIs to let them access these functions/variables. The problems for this are:\n1) Logic of the append message / read message from Replica Manager leaks into KafkaAPIs, and KafkaAPIs itself becomes very huge containing all purgatory / delayed requests classes.\n2) However, logic for satisfying delayed fetch requests are not correct: it needs to be related to HW modifications. Hence it needs to also access Partition, which will lead to more logic leak if we follow current architecture.\n3) With inbuild offset management, we have to hack the KafkaAPIs and its corresponding delayed requests as follows:\nCommitOffsetRequest --> 1) KafkaApis.appendToLocalLog()         --> OffsetManager.producerRequestFromOffsetCommit()    // returns a new ProducerRequest from the OffsetCommitRequest\n2) KafkaApis.appendToLocalLog()\n3) OffsetManager.putOffsets()   // put the offset into cache\n4) OffsetManager.offsetCommitRequestOpt().get()    // transform back a OffsetCommitResponse\n5) RequestChannel.sendResponse()   OR   ProducerRequestPurgatory.watch()\n------------------------------------------------------------------------------------------------------\nDelayedProduce.respond()  --> 1) if(not timed out) OffsetManager.putOffsets()\n2) OffsetManager.offsetCommitRequestOpt().get()\n3) RequestChannel.sendResponse()\nThe architecture diagram is shown below:\nThe Idea\nIs to refactor the Kafka Apis along with Replica Manager and Offset Manager (Coordinator) such that the produce/fetch purgatories are moved to replica manager, and are isolated from requests (i.e. they may be just purgatories for append and fetch operations). By doing so:\n1) Kafka API becomes thinner, only handling request-level application logic and talk to Request Channel.\n2) Read message and append message logic is moved to Replica Manager, which handles the logic of \"committed\" appending and fetching \"committed data\".\n3) Offset Manager (Coordinator) only needs to talk to the Replica Manager for handling offset commits, no need to hack Kafka Apis and Delayed Fetch requests.\nThis refactoring will also benefit the following new consumer / coordinator development.\nPurgatories and Delayed Operations\nWe refactor the purgatories and delayed requests (now should be called operations) as follows. Here all the module names remain the same, with renaming suggestion in the bracket just for clear indication.\nDelayed Request (Kafka Operation)\nThe base kafka operation just contains:\nkeys: Seq[Any] // currently it is called DelayedRequestKey, but we can rename it as we like\ntimeout: long // timeout value in milliseconds\ncallback: Callback // this is the callback function triggered upon complete, either due to timeout or operation finished\nOne note here is that a new callback instance needs to be created for each operation, since it will need to remember the request object that it needs to respond on. The base callback class can be extended, with the basic parameters:\n// trigger the onComplete function given that whether the operation has succeeded or failed (e.g. timed out).\nonComplete(Boolean)\nBesides these fields, a kafka operation also have the following interface:\n// check if myself is satisfied\nisSatisfied(): Boolean\n// operations upon expiring myself\nexpire() = this.callback.onComplete(false) // can be overridden for recording metrics, etc\nDelayed Produce Request (Message Append Operation)\nMaintains the append metadata, including the replicate condition:\nappendStatus: Map[TopicAndPartition, AppendStatus] // AppendStatus include starting offset, required offset, error code, etc\nreplicateCondition: ReplicateCondition // ReplicationCondition can be just a ack integer, but may be extended in the future\nIn addition, it implements the interface as:\nisSatisfied(replicaManager): Boolean = // access the replicaManager to check if each partition's append status has satisfied the replicate condition\nDelayed Fetch Request (Message Fetch Operation)\nMaintains the fetch metadata, including the fetch min bytes:\nfetchInfo: Map[TopicAndPartition, LogOffsetMetadata] // LogOffsetMetadata include message offset, segment starting offset and relative segment position\nfetchMinBytes: int\nAnd implements the interface as:\nisSatisfied(replicaManager): Boolean = // access the replicaManager to check if the accumulated bytes exceeds the minimum bytes, with some corner case special handling.\nRequest Purgatory (Kafka Purgatory)\nThe base purgatory provides the following APIs:\n// check if an operation is satisfied already; if not, watch it.\nmaybeWatch(operation: Operation): Boolean\n// return a list of operations that are satisfied given the key\nupdate(Any): List[KafkaOperations]\nAnd its expiry reaper will purge the watch list and for each expired operation trigger operation.expire(). This purgatory is generic and can be actually used for different kinds of operations.\nKafka Server Modules\nWith these purgatories and operations, server side modules can be refactored as follows:\nReplica Manager\nReplica Manager maintain metadata of partitions, including their local and remote replicas, and talks to Log Manager for log operations such as log truncation, etc. Here is the proposed API:\n// append messages to leader replicas of the partition, and wait for replicated to other replicas,\n// the callback function will be triggered either when timeout or the replicate condition is satisfied\nappendMessages(Map[TopicAndPartition, ByteBufferMessageSet], ReplicateCondition /* acks, etc */,  long /* timeout */, Callback) {\n// 1. Partition.appendToLocalLog()\n// 2. If can respond now, call Callback.onComplete(true)\n// 3. Otherwise create new DelayedAppend(..., Callback)\n// 4. AppendPurgatory.maybeWatch(append)\n}\n// fetch only committed messages from the leader replica,\n// the callback function will be triggered either when timeout or required fetch info is satisfied\nfetchMessages(Map[TopicAndPartition, FetchInfo], int /* min bytes*/, long /* timeout */, RespondCallback) {\n// 1. Log.read()\n// 2. If can respond now, call Callback.onComplete(true)\n// 3. Otherwise create new DelayedFetch(..., new FetchCallback() { onComplete(): { fetchMessages; RespondCallback.onComplete(true); } } )\n// 4. FetchPurgatory.maybeWatch(append)\n}\n// stop a local replica\nstopReplica(TopicAndPartition, Boolean)\n// make local replica leader of the partitions\nleadPartition(Map[TopicAndPartition, PartitionState])\n// make local replica follower of the partitions\nfollowPartition(Map[TopicAndPartition, PartitionState])\n// get (or create) partition, get (or create) replica, etc..\ngetPartition(TopicAndPartition)\ngetReplica(TopicAndPartition, int)\n..\nCoordinator / Offset Manager\nCoordinator's offset manager will talk to the replica manager for appending messages.\n// trigger the callback only when the offset is committed to replicated logs\nputOffsets(Map[TopicAndPartition, OffsetInfo], RespondCallback) {\n// 1. replicaManager.appendMessage(... , new OffsetCommitCallback{ onComplete (): { putToCache; RespondCallback.onComplete(true); } })\n}\n// access the cache to get the offsets\ngetOffsets(Set[TopicAndPartition]) : Map[TopicAndPartition, OffsetInfo]\nKafka Apis\nNow the Kafka APIs becomes:\nhandleProduce(ProduceRequest) = // call replica-manager's appendMessages with callback sending produce response\nhandleFetch(FetchRequest) = // call replica-manager's fetchMessages with callback sending fetch response\nhandleCommitOffset(CommitOffsetRequest) = // call coordinator's offset manager with callback sending commit offset response\nhandleFetchOffset(FetchOffsetRequest) = // call coordinator's offset manager to get the offset, and then send back the response.\nRequest Handling Workflow\nWith the above refactoring, the request life cycle becomes:\nProduceRequest --> KafkaApis.handleProduce()         --> ReplicaManager.appendMessages()               --> Partition.appendMessagesToLeader()\n--> AppendPurgatory.maybeWatch()\n--> Callback(): RequestChannel.sendResponse()\n------------------------------------------------------------------------------------------------------\nFetchRequest -->   KafkaApis.handleFetch()           --> ReplicaManager.fetchMessages()                --> ReplicaManager.readMessageSet()        --> Log.read()\n--> FetchPurgatory.maybeWatch(new OffsetCommitCallback)\n--> OffsetCommitCallback(): ReplicaManager.readMessageSet()\nRespondCallback.onComplete()\n--> RespondCallback: RequestChannel.sendResponse()\n------------------------------------------------------------------------------------------------------\nFetchRequest -->   KafkaApis.handleCommitOffset()    --> Coordinator.CommitOffsets()                   --> ReplicaManager.appendMessages(new OffsetCommitCallback)        --> Partition.appendMessagesToLeader()\n--> AppendPurgatory.maybeWatch()\n--> OffsetCommitCallback(): OffsetManager.putOffsets()\nRespondCallback.onComplete()\nRespondCallback(): RequestChannel.sendResponse()\n------------------------------------------------------------------------------------------------------\nPartition.maybeIncrementLeaderHW()                      --> ReplicaManager.unblockDelayedFetchRequests() / unblockDelayedProduceRequests()\nPartition.appendMessagesToLeader()                      --> ReplicaManager.unblockDelayedFetchRequests()\nPartition.recordFollowerLOE()                           --> ReplicaManager.unblockDelayedProduceRequests()\nThe caveat\nAs we can see, with fetch request and offset commit request, a nested callback is used (RepondCallback from Kafka APIs for sending the response through channel, and FetchCallback / OffsetCommitCallback for fetching the data for response / putting offset into cache). This nesting is not ideal, but necessary if want to have the strict layered architecture.\nThe new architectural diagram will be:",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/Kafka+API+Refactoring",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
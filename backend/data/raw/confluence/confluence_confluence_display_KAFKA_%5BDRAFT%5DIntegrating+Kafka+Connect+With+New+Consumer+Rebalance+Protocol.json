{
  "id": "confluence_display_KAFKA_%5BDRAFT%5DIntegrating+Kafka+Connect+With+New+Consumer+Rebalance+Protocol",
  "title": "[DRAFT]Integrating Kafka Connect With New Consumer Rebalance Protocol - Apache Kafka - Apache Software Foundation",
  "content": "Background\nKIP-848\nintroduces a new consumer rebalance protocol. It is a paradigm shift from how Consumer Rebalances are performed. In terms of what changes, these are the high level items:\nMove away the rebalancing logic from the clients to brokers. This is the major point and the KIP explains in detail about the issue with thick clients.\nMove away from the global synchronization barrier that the current protocol has to be truly incremental and co-operative.\nOne big proposed change is the re-write of Group Coordinator from scratch in Java.\nLastly, it introduces new APIs and tries to piggyback on the heartbeat APIs to transfer membership details. Currently, the heartbeat are just used to liveness but the KIP feels it would be good to enhance it.\nThe proposed rebalance protocol is based on the concept of a declarative assignment for the group and the use of reconciliation loops to drive members toward their desired assignment. Members can independently converge and the group coordinator takes care of resolving the dependencies - e.g. revoking a partition before it can be assigned - between the members if any.\nKafka Connect\nKafka Connect also has a concept of rebalances but the unit of rebalance is different. In Consumers and inturn Streams/ksqldb, the unit of rebalance is topic-partitions. However, in the case of Kafka Connect the rebalances happen within a bunch of connect workers running in a cluster and what gets “rebalanced” are connectors and tasks. Let’s take a look at few scenarios where a rebalance might be triggered in Kafka Connect:\nA new worker is added to the cluster.\nAn existing worker bounces.\nA worker leaves the group permanently.\nThere’s also a concept of Assignors in Kafka Connect which is performed on the Worker side. Currently, 2 types of assignors are supported: Eager and Incremental Cooperative. The KIP also provides the provision of running Client Side assignors since Kafka Streams relies on Client Side assignors.\nChanges needed in the new Rebalance Protocol\nWhile there are similarities in Kafka Streams and Connect in terms of how they can be onboarded onto the new rebalance protocol. the rebalance protocol would need to be updated to be able to understand Kafka Connect semantics. This is because the new protocol is tailored around Topics/Partitions while Kafka Connect doesn’t deal with those. Here are some of the updates :\nConnect group type\nThe new protocol introduces a concept of\ntypes\nwithin the group coordinator. This is basically to allow supporting different kinds of groups in the future and also to be able to differentiate between old and new consumer groups. Keeping the former reason in mind, we would introduce a new\ntype\ncalled\nconnect\n.\nData Model\nThe KIP introduces a logical data model to capture group membership and their assignments which the group coordinator would use for bookkeeping purposes. There are separate public interfaces which are defined to capture actual requests/responses between brokers <=> group-coordinator. This is again consumer group centric which we would try to leverage and expand for Kafka Connect.\nGroup Protocol\nThe new protocol introduces a new config called\ngroup.protocol\n. The older protocol is called\ngeneric\nwhile the new one is called\nconsumer\n. To maintain clear separation, we would introduce a new possible value for\ngroup.protocol\ncalled\nconnect\n.\nWorker Group and Worker\nWorker Group\nName\nType\nDescription\nGroup ID\nstring\nThe group ID as configured on the worker.The ID uniquely identifies the worker cluster.\nGroup Epoch\nint32\nThe current epoch of the group. The epoch is incremented by the group coordinator when a new assignment is required for the group.\nWorkers\n[]Worker\nThe set of worker in the group.\nWorker\nName\nType\nDescription\nMember ID\nString\nUnique Member ID\nurl\nstring\nThe url of the worker.\nClient Assignors\n[]Assignor\nThe list of client-side assignors supported by the member. The order of this list defined the priority.\nAssignor\nName\nType\nDescription\nName\nstring\nThe unique name of the assignor.\nReason\nint8\nThe reason why the metadata was updated.\nMinimum Version\nint16\nThe minimum version of the metadata schema supported by this assignor.\nMaximum Version\nint16\nThe maximum version of the metadata schema supported by this assignor.\nVersion\nint16\nThe version used to encode the metadata. (For Connect, currently it’s between 0-2 i.e EAGER, COMPATIBLE and SESSIONED.).\nMetadata\nbytes\nThe metadata provided by the worker for this assignor.\nKafka Connect supports Client side Assignors (EAGER and Incremental Cooperative) and that’s what is embedded here. Note that the KIP proposes topic subscriptions(literal and regex based) as part of the data model but we won’t need it here. That’s because a worker can start even without any connectors/tasks running on it while that’s not the case for consumer groups. Also, we have reused the Assignor data model.\nTarget Assignment\nThe target (or desired) assignment of the group. This represents the assignment that all the members will eventually converge to. It is a declarative assignment which is generated by the assignor based on the group state.\nTarget Assignment\nName\nType\nDescription\nGroup ID\nstring\nThe group ID as configured on the worker.The ID uniquely identifies the worker cluster.\nAssignment Epoch\nint32\nThe epoch of the assignment. It represents the epoch of the group used to generate the assignment. It will eventually match the group epoch.\nAssignment Error\nint8\nThe error reported by the assignor.\nWorkers\n[]Worker\nThe assignment for each worker.\nWorker\nName\nType\nDescription\nMemberID\nString\nUnique memberID\nurl\nstring\nThe url of the worker.\nConnectorIds\n[]String\nThe set of connectors assigned to this worker.\nTasks\n[]ConnectorTaskId\nThe tasks assigned to the worker\nConnectorTaskId\nName\nType\nDescription\nConnectorId\nString\nUnique connector ID\nTaskID\nint8\nThe unique task id of a task in a connector\nCurrent Assignment\nThe Current Assignment represents the current epoch and assignment of a worker. Note that workers of a given group could be at a different epoch but they will all eventually converge to the target assignment.\nCurrent Assignment\nName\nType\nDescription\nGroup ID\nstring\nThe group ID as configured on the worker.The ID uniquely identifies the worker cluster.\nWorker URL\nstring\nThe url of the worker.\nWorker Epoch\nint32\nThe current epoch of this worker. The epoch is the assignment epoch of the assignment currently used by this member. This epoch is the one used to fence the worker (e.g. offsets commit).\nError\nint8\nThe error reported by the assignor.\nConnectors\n[]String\nThe current connectors used by the member.\nVersion\nint16\nThe version used to encode the metadata.\nMetadata\nbytes\nThe current metadata used by the member.\nRebalance Process\nThe rebalance process is entirely driven by the group coordinator and revolves around three kinds of epoch: the group epoch, the assignment epoch and the member epoch. The process and the epochs are explained as follows:\nGroup Epoch- Trigger a Rebalance\nThe group coordinator is responsible for triggering a rebalance of the group when the metadata of the group changes. The metadata of the group is used as the input of the assignment function. For tracking this, group epoch is introduced which represents the generation (or the version) of the group metadata. The group epoch is incremented whenever the group metadata is updated. Rebalance is possible in the following cases:\nA new worker joins.\nA worker updates it’s assignors.\nA worker updates its assignors' reason or metadata.\nA worker is fenced or removed from the group by the group coordinator.\nConnectors are added or removed.\nTasks are added/removed from Connectors(Couldn’t find it but think this would be applicable).\nIn all these cases, a new version of the group metadata is persisted by the group coordinator with an incremented group epoch. This also signals that a new assignment is required for the group.\nAssignment Epoch - Compute the group assignment\nWhenever the group epoch is larger than the target assignment epoch, the group coordinator will trigger the computation of a new target assignment based on the latest group metadata. When the new assignment is computed, the group coordinator persists it. The assignment epoch becomes the group epoch of the group metadata used to compute the assignment.\nIn the case of Kafka Connect, we would continue using the Client Side assignors. This would be explained later on in the doc.\nMember Epoch - Reconciliation of the group\nOnce a new target assignment is installed, each worker will independently reconcile their current assignment with their new target assignment. Ultimately, each worker will converge to their target epoch and assignment. The reconciliation process requires three phases:\nThe group coordinator revokes the connectors/tasks which are no longer in the target assignment of the member. It does so by providing the intersection of the Current connectors/tasks and the Target connectors/tasks in the heartbeat response until the worker acknowledges the revocation in the heartbeat response. We can repurpose the\nrebalance.timeout.ms\nconfig to put a cap on the rebalance process or else the worker would be kicked out of the group.\nWhen the group coordinator receives the acknowledgement of the revocation, it updates the worker current assignment to its target assignment (and target epoch) and durably persist it.\nThe group coordinator assigns the new connectors/tasks to the worker. It does so by providing the Target connectors/tasks to the worker while ensuring that connectors/tasks which are not revoked by other workers yet are removed from this set. In other words, new connectors/tasks are incrementally assigned to the worker when they are revoked by the other workers.\nAssignment Process\nWhenever the group epoch is larger than the assignment epoch, the group coordinator must compute a new target assignment for the group. As already mentioned, for Connect we can piggyback on the Client Side Assignors already present.\nThe new target assignment for the group is basically a function of the current group metadata and the current target assignment. One important aspect to note here is that the assignment is declarative now instead of being incremental like it is in the current implementation. In other words, the assignor defines the desired state for the group and let the group coordinator converge to it.\nAssignor Selection\nThe Group Co-ordinator will use the assignors which are supported by all workers and if there are multiple such assignors, then the precedence order of assignments are honoured. This should be straightforward as as of today, Connect supports only 2 assignor modes => Eager and IncrementalCooperative.\nThe client side assignment is executed by the worker. The overall process has the following phases:\nThe group coordinator selects a worker to run the assignment logic. We will get it to it later.\nThe group coordinator notifies the worker to compute the new assignment by returning the COMPUTE_ASSIGNMENT error in its next heartbeat response.\nWhen the worker receives this error, it is expected to call the\nConnectGroupPrepareAssignment\nAPI to get the current group metadata and the current target assignment.\nThe worker computes the new assignment with the relevant assignor.\nThe worker calls the\nConnectGroupInstallAssignment\nAPI to install the new assignment. The group coordinator validates it and persists it.\nThe worker should finish the assignment within\nrebalance.timeout.ms\n.\nWorker Selection\nThe group coordinator can generally pick any workers to run the assignment. However, when the workers support different version ranges, the group coordinator must select a worker which is able to handle all the supported versions. For instance, if we have three workers: A [1-5], B [3-4], C [2-4]. Worker A must be selected because it supports all the other versions in the group.\nIf we can’t find such an overlapping worker, then we will throw a FATAL error.\nIn the current Connect world, the Assignors are generally run on the Leader and the Leader assignment is sticky. We can evaluate if the stickiness is needed anymore or not.\nAssignment Validation\nBefore installing any new assignment, the group coordinator will ensure that the following invariants are met:\nAll connectors/tasks are assigned.\nA connector/task is assigned only once.\nAll workers exists.\nNote that this validation is made with regarding to the metadata used to compute the assignment. The group may have already advanced to a newer group epoch - e.g. a worker could have left during the assignment computation.\nThe installation will be rejected with an INVALID_ASSIGNMENT error if the invariants are not held.\nAssignment Error\nIf the Connect assignor can’t compute the assignment, then it would return an\nerror\nwhich would result in retaining the current assignments.\nMember ID/Heartbeat & Session/Joining &\nLeaving/Fencing\nThese would remain as defined in the KIP.\nFeature Flag\nSimilar to the KIP, a feature flag to enable/disable the new rebalance protocol.\nRebalance Process\nThis is the opposite of what was described above. The major difference from KIP is that, Connect has it’s own WorkerRebalanceListener which is invoked based on assignment or revocation.\nIf the worker is fenced by the group coordinator, it will immediately abandon all its tasks/connectors, stops these resources and invoke WorkerRebalanceListener#onRevoked. It will rejoin the group as a new member afterwards.\nOtherwise, the worker will compute the difference between its currently owned tasks/connectors and the assigned ones, as defined in the heartbeat response.\nIf there are any revoked partitions, it will revoke them, it stops (releases) these resources and call WorkerRebalanceListener#onRevoked.\nIf there are any newly assigned partitions, it starts the assigned connector and tasks and call WorkerRebalanceListener#onAssigned.\nClient Side Assignor\nThe KIP talks about introducing a new interface for Client Side assignors called and deprecating the current ConsumerPartitionAssignor . Since connect has it's own assignor ConnectAssignor we would be extending the same.\nTriggering Rebalances\nThe\nIncrementalCooperativeAssignor\nhas a provision of triggering rebalances after a fixed\nscheduled.rebalance.max.delay.ms\ninterval to allow any departed worker to come back so that it gets the same or similar assignments. Since the older rebalance protocol was triggered entirely through clients, this was possible while the new protocol offloads the rebalancing duties to the Group Coordinator. Having said that, the ClientSideAssignors can still trigger rebalances by setting the\nreason\nfield which the Group Coordinator will need to understand. Here again, we will need to enhance the Group Coordinator logic based on\ngroup.type\nfield\nconnect\n.\nMigration To the New Protocol\nUpgrading to the new protocol or downgrading from it is possible by rolling the workers, assuming that the new protocol is enabled on the server side, with the correct\ngroup.protocol\n. When the first worker that supports the new protocol joins the group, the group is converted from\ngeneric\nto\nconnect\n. Similarly when the last worker supporting the new protocol leaves, the group switches back to the old protocol. Note that the group epoch starts at the current group generation.\nOne important thing to note is that\nJoinGroup, SyncGroup and Heartbeat\ncalls need to be translated into the new protocol. We will rely on ConnectGroupHeartbeat  API for the translation of the 3 APIs. Concretely, the API updates the group state, provides the connectors/tasks owned by the worker, gets back the assignment, and updates the session. The main difference here is that the JoinGroup and SyncGroup does not run continuously. The group coordinator has to trigger it when needed by returning the\nREBALANCE_IN_PROGRESS\nerror in the heartbeat response.\nThe idea is to manage each workers individually while relying on the new engine to do the synchronization between them. Each worker will use rebalance loops to update the group coordinator and collect their assignment. The group coordinator will ensure that a rebalance is triggered when one needs to update it’s assignments.\nAs in the older protocol, the possible worker states would still be\nUNJOINED\nThe client is not part of a group\nPREPARING_REBALANCE\nThe client has sent the join group request, but has not received response\nCOMPLETING_REBALANCE\nThe client has received join group response, but has not received assignment\nSTABLE\nThe client has joined and is sending heartbeats\n<\nNeed to add more details>\nJoinGroup Handling\nSyncGroup Handling\nHeartbeat Handling\nRebalance Triggers\nPublic Interfaces\nKRPC\nNew Errors\nFENCED_MEMBER_EPOCH - The member epoch does not correspond to the member epoch expected by the coordinator.\nCOMPUTE_ASSIGNMENT - The member has been selected by the coordinator to compute the new target assignment of the group.\nUNSUPPORTED_ASSIGNOR - The assignor used by the member or its version range are not supported by the group.\nConnectGroupHeartbeat API\nThe ConnectGroupHeartbeat API is the new core API used by workers to form a group. The API allows workers to advertise their state, assignors and their owned connectors/tasks. The group coordinator uses it to assign/revoke connectors/tasks to/from workers. This API is also used as a liveness check.\nRequest Schema\nThe member must set all the (top level) fields when it joins for the first time or when an error occurs (e.g. request timed out). Otherwise, it is expected to only fill in the fields which have changed since the last heartbeat.\n{\n\"apiKey\": TBD,\n\"type\": \"request\",\n\"listeners\": [\"zkBroker\", \"broker\"],\n\"name\": \"ConnectGroupHeartbeatRequest\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n\"fields\": [\n{ \"name\": \"GroupId\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"groupId\",\n\"about\": \"The group identifier.\" },\n{ \"name\": \"MemberId\", \"type\": \"string\", \"versions\": \"0+\",\n\"about\": \"The member id generated by the server. The member id must be kept during the entire lifetime of the member. For connect, this corresponds\nto workerIds\" },\n{ \"name\": \"MemberEpoch\", \"type\": \"int32\", \"versions\": \"0+\", \"default\": \"-1\",\n\"about\": \"The current member epoch; 0 to join the group; -1 to leave the group.\" },\n{ \"name\": \"InstanceId\", \"type\": \"string\", \"versions\": \"0+\", \"nullableVersions\": \"0+\", \"default\": \"null\",\n\"about\": \"null it not provided or if it didn't change since the last heartbeat; the instance Id otherwise.\" },\n{ \"name\": \"RebalanceTimeoutMs\", \"type\": \"int32\", \"versions\": \"0+\", \"default\": -1,\n\"about\": \"-1 if it didn't chance since the last heartbeat; the maximum time in milliseconds that the coordinator will wait on the member to revoke its partitions otherwise.\" },\n{ \"name\": \"ServerAssignor\", \"type\": \"string\", \"versions\": \"0+\", \"nullableVersions\": \"0+\", \"default\": \"null\",\n\"about\": \"null if not used or if it didn't change since the last heartbeat; the server side assignor to use otherwise.\" },\n{ \"name\": \"ClientAssignors\", \"type\": \"[]Assignor\", \"versions\": \"0+\", \"nullableVersions\": \"0+\", \"default\": \"null\",\n\"about\": \"null if not used or if it didn't change since the last heartbeat; the list of client-side assignors otherwise.\",\n\"fields\": [\n{ \"name\": \"Name\", \"type\": \"string\", \"versions\": \"0+\",\n\"about\": \"The name of the assignor.\" },\n{ \"name\": \"MinimumVersion\", \"type\": \"int16\", \"versions\": \"0+\",\n\"about\": \"The minimum supported version for the metadata.\" },\n{ \"name\": \"MaximumVersion\", \"type\": \"int16\", \"versions\": \"0+\",\n\"about\": \"The maximum supported version for the metadata.\" },\n{ \"name\": \"Reason\", \"type\": \"int8\", \"versions\": \"0+\",\n\"about\": \"The reason of the metadata update.\" },\n{ \"name\": \"Version\", \"type\": \"int16\", \"versions\": \"0+\",\n\"about\": \"The version of the metadata.\" },\n{ \"name\": \"Metadata\", \"type\": \"bytes\", \"versions\": \"0+\",\n\"about\": \"The metadata.\" }\n]},\n{ \"name\": \"ConnectorsAndTasks\", \"type\": \"[]ConnectorsAndTasks\", \"versions\": \"0+\", \"nullableVersions\": \"0+\", \"default\": \"null\",\n\"about\": \"null if it didn't change since the last heartbeat; the connectors/tasks owned by the worker. This will be set only when group.type is equal\nto connect\",\n\"fields\": [\n{ \"name\": \"connectors\", \"type\": \"[]String\", \"versions\": \"0+\",\n\"about\": \"The Connectors assigned to this worker.\" },\n{ \"name\": \"tasks\", \"type\": \"[]ConnectorTaskID\", \"versions\": \"0+\",\n\"about\": \"The tasks assigned to this worker.\" }\n]}\n]\n}\nRequired ACL\nRead Group\nRequest Validation\nINVALID_REQUEST is returned should the request not obey to the following invariants:\nGroupId must be non-empty.\nMemberId must be non-empty.\nMemberEpoch must be >= -1.\nInstanceId, if provided, must be non-empty.\nRebalanceTimeoutMs must be larger than zero in the first heartbeat request.\nServerAssignor and ClientAssignors cannot be used together.\nAssignor.Name\nmust be non-empty.\nAssignor.MinimumVersion must be >= -1.\nAssignor.MaximumVersion must be >= 0 and >= Assignor.MinimumVersion.\nAssignor.Version must be in the >= Assignor.MinimumVersion and <= Assignor.MaximumVersion.\nUNSUPPORTED_ASSIGNOR is returned should the request not obey to the following invariants:\nServerAssignor must be supported by the server.\nClientAssignors' version range must overlap with the other members in the group.\nRequest Handling\nWhen the group coordinator handles a ConnectGroupHeartbeat request:\nLookups the group or creates it.\nCreates the member should the member epoch be zero or checks whether it exists. If it does not exist, UNKNOWN_MEMBER_ID is returned.\nChecks whether the member epoch matches the member epoch in its current assignment. FENCED_MEMBER_EPOCH is returned otherwise. The member is also removed from the group.\nThere is an edge case here. When the group coordinator transitions a member to its target epoch, the heartbeat response with the new member epoch may be lost. In this case, the member will retry with the member epoch that he knows about and his request will be rejected with a FENCED_MEMBER_EPOCH. This is not optimal. Instead, the group coordinator could accept the request if the partitions or connectors/tasks owned by the members are a subset of the target assignments. This could be decided based on the\ntypes\n. If it is the case, it is safe to transition the member to its target epoch again.\nUpdates the members informations if any. The group epoch is incremented if there is any change.\nReconcile the member assignments as explained earlier in this document.\nResponse Schema\nThe group coordinator will only set the Assignment field when the member epoch is smaller than the target assignment epoch. This is done to ensure that the members converge to the target assignment.\n{\n\"apiKey\": TBD,\n\"type\": \"response\",\n\"name\": \"ConnectGroupHeartbeatResponse\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n// Supported errors:\n// - GROUP_AUTHORIZATION_FAILED\n// - NOT_COORDINATOR\n// - COORDINATOR_NOT_AVAILABLE\n// - COORDINATOR_LOAD_IN_PROGRESS\n// - INVALID_REQUEST\n// - UNKNOWN_MEMBER_ID\n// - FENCED_MEMBER_EPOCH\n// - UNSUPPORTED_ASSIGNOR\n// - COMPUTE_ASSIGNMENT\n\"fields\": [\n{ \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"0+\",\n\"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },\n{ \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n\"about\": \"The top-level error code, or 0 if there was no error\" },\n{ \"name\": \"ErrorMessage\", \"type\": \"string\", \"versions\": \"0+\", \"nullableVersions\": \"0+\", \"default\": \"null\",\n\"about\": \"The top-level error message, or null if there was no error.\" },\n{ \"name\": \"MemberEpoch\", \"type\": \"int32\", \"versions\": \"0+\",\n\"about\": \"The member epoch.\" },\n{ \"name\": \"HeartbeatIntervalMs\", \"type\": \"int32\", \"versions\": \"0+\",\n\"about\": \"The heartbeat interval in milliseconds.\" },\n{ \"name\": \"Assignment\", \"type\": \"Assignment\", \"versions\": \"0+\", \"nullableVersions\": \"0+\", \"default\": \"null\",\n\"about\": \"null if not provided; the assignment otherwise.\"\n\"fields\": [\n{ \"name\": \"Error\", \"type\": \"int8\", \"versions\": \"0+\",\n\"about\": \"The assigned error.\" },\n{ \"name\": \"ConnectorsAndTasks\", \"type\": \"[]ConnectorsAndTask\", \"versions\": \"0+\",\n\"about\": \"The assigned connectors/tasks to the member.\",\n\"fields\": [\n{ \"name\": \"connectors\", \"type\": \"[]String\", \"versions\": \"0+\",\"about\": \"The Connectors assigned to this worker.\" },\n{ \"name\": \"tasks\", \"type\": \"[]ConnectorTaskID\", \"versions\": \"0+\",\"about\": \"The tasks assigned to this worker.\" }\n]},\n{ \"name\": \"Version\", \"type\": \"int16\", \"versions\": \"0+\",\n\"about\": \"The version of the metadata.\" },\n{ \"name\": \"Metadata\", \"type\": \"bytes\", \"versions\": \"0+\",\n\"about\": \"The assigned metadata.\" }\n]\n]\n}\nResponse Handling\nIf the response contains no error, the member will reconcile its current assignment towards its new assignment. It does the following:\nIt updates its member epoch.\nIt computes the difference between the old and the new assignment to determine the revoked connectors/tasks and the newly assigned ones. There should be either revoked connectors/tasks or newly assigned connectors/tasks The protocol never does both together.\nIt revokes the connectors/tasks, release all resources, and calls WorkerRebalanceListener#onRevoked.\nIt assigns the new connectors/tasks, calls ConnectAssignor#onAssignment and calls WorkerRebalanceListener#onAssigned.\nAfter a revocation, It sends the next heartbeat immediately to acknowledge it.\nUpon receiving the COMPUTE_ASSIGNMENT error, the worker starts the assignment process.\nUpon receiving the UNKNOWN_MEMBER_ID or FENCED_MEMBER_EPOCH error, the worker abandons all its resources and rejoins with the same member id and the epoch 0.\nConnectGroupPrepareAssignment API\nThe ConnectGroupPrepareAssignment API will be used by the member to get the information to feed its client-side assignor.\nRequest Schema\n{\n\"apiKey\": TBD,\n\"type\": \"request\",\n\"listeners\": [\"zkBroker\", \"broker\"],\n\"name\": \"ConnectGroupPrepareAssignmentRequest\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n\"fields\": [\n{ \"name\": \"GroupId\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"groupId\",\n\"about\": \"The group identifier.\" },\n{ \"name\": \"MemberId\", \"type\": \"string\", \"versions\": \"0+\",\n\"about\": \"The member id assigned by the group coordinator.\" },\n{ \"name\": \"MemberEpoch\", \"type\": \"int32\", \"versions\": \"0+\",\n\"about\": \"The member epoch.\" }\n]\n}\nRequired ACL\nRead Group\nRequest Validation\nINVALID_REQUEST is returned should the request not obey to the following invariants:\nGroupId must be non-empty.\nMemberId must be non-empty.\nMemberEpoch must be >= 0.\nRequest Handling\nWhen the group coordinator handles a ConnectGroupPrepareAssignmentRequest request:\nChecks wether the group exists. If it does not, GROUP_ID_NOT_FOUND is returned.\nChecks whether the member exists. If it does not, UNKNOWN_MEMBER_ID is returned.\nChecks wether the member epoch matches the current member epoch. If it does not, FENCED_MEMBER_EPOCH is returned.\nChecks wether the member is the chosen one to compute the assignment. If it does not, UNKNOWN_MEMBER_ID is returned.\nReturns the group state of the group.\nResponse Schema\n{\n\"apiKey\": TBD,\n\"type\": \"response\",\n\"name\": \"ConnectGroupPrepareAssignmentResponse\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n// Supported errors:\n// - GROUP_AUTHORIZATION_FAILED\n// - NOT_COORDINATOR\n// - COORDINATOR_NOT_AVAILABLE\n// - COORDINATOR_LOAD_IN_PROGRESS\n// - INVALID_REQUEST\n// - INVALID_GROUP_ID\n// - GROUP_ID_NOT_FOUND\n// - UNKNOWN_MEMBER_ID\n// - FENCED_MEMBER_EPOCH\n\"fields\": [\n{ \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"0+\",\n\"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },\n{ \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n\"about\": \"The top-level error code, or 0 if there was no error\" },\n{ \"name\": \"ErrorMessage\", \"type\": \"string\", \"versions\": \"0+\", \"nullableVersions\": \"0+\", \"default\": \"null\",\n\"about\": \"The top-level error message, or null if there was no error.\" },\n{ \"name\": \"GroupEpoch\", \"type\": \"int32\", \"versions\": \"0+\",\n\"about\": \"The group epoch.\" },\n{ \"name\": \"AssignorName\", \"type\": \"string\", \"versions\": \"0+\",\n\"about\": \"The selected assignor.\" },\n{ \"name\": \"Members\", \"type\": \"[]Member\", \"versions\": \"0+\",\n\"about\": \"The members.\", \"fields\": [\n{ \"name\": \"MemberId\", \"type\": \"string\", \"versions\": \"0+\",\n\"about\": \"The member ID.\" },\n{ \"name\": \"MemberEpoch\", \"type\": \"int32\", \"versions\": \"0+\",\n\"about\": \"The member epoch.\" },\n{ \"name\": \"InstanceId\", \"type\": \"string\", \"versions\": \"0+\",\n\"about\": \"The member instance ID.\" },\n{ \"name\": \"Assignor\", \"type\": \"Assignor\", \"versions\": \"0+\",\n\"about\": \"The information of the selected assignor\",\n\"fields\": [\n{ \"name\": \"Version\", \"type\": \"int16\", \"versions\": \"0+\",\n\"about\": \"The version of the metadata.\" },\n{ \"name\": \"Reason\", \"type\": \"int8\", \"versions\": \"0+\",\n\"about\": \"The reason of the metadata update.\" },\n{ \"name\": \"Metadata\", \"type\": \"bytes\", \"versions\": \"0+\",\n\"about\": \"The assignor metadata.\" }\n]},\n{ \"name\": \"ConnectorsAndTasks\", \"type\": \"[]ConnectorsAndTask\", \"versions\": \"0+\",\n\"about\": \"The assigned connectors/tasks to the member.\",\n\"fields\": [\n{ \"name\": \"connectors\", \"type\": \"[]String\", \"versions\": \"0+\",\"about\": \"The Connectors assigned to this worker.\" },\n{ \"name\": \"tasks\", \"type\": \"[]ConnectorTaskID\", \"versions\": \"0+\",\"about\": \"The tasks assigned to this worker.\" }\n]}\n]}\n]\n}\nResponse Handling\nIf the response contains no error, the member calls the client side assignor with the group state.\nUpon receiving the UNKNOWN_MEMBER_ID error, the consumer abandon the process.\nUpon receiving the FENCED_MEMBER_EPOCH error, the consumer retries when receiving its next heartbeat response with its member epoch.\nConnectGroupInstallAssignment API\nThe ConnectGroupInstallAssignment API will be used by the member to install a new assignment for the group. The new assignment is the result of the client-side assignor.\nRequest Schema\n{\n\"apiKey\": TBD,\n\"type\": \"request\",\n\"listeners\": [\"zkBroker\", \"broker\"],\n\"name\": \"ConnectGroupInstallAssignment\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n\"fields\": [\n{ \"name\": \"GroupId\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"groupId\",\n\"about\": \"The group identifier.\" },\n{ \"name\": \"MemberId\", \"type\": \"string\", \"versions\": \"0+\",\n\"about\": \"The member id assigned by the group coordinator.\" },\n{ \"name\": \"MemberEpoch\", \"type\": \"int32\", \"versions\": \"0+\",\n\"about\": \"The member epoch.\" },\n{ \"name\": \"GroupEpoch\", \"type\": \"int32\", \"versions\": \"0+\",\n\"about\": \"The group epoch.\" },\n{ \"name\": \"Error\", \"type\": \"int8\", \"versions\": \"0+\",\n\"about\": \"The assignment error; or zero if the assignment is successful.\" },\n{ \"name\": \"Members\", \"type\": \"[]Member\", \"versions\": \"0+\",\n\"about\": \"The members.\", \"fields\": [\n{ \"name\": \"MemberId\", \"type\": \"string\", \"versions\": \"0+\",\n\"about\": \"The member ID.\" },\n{ \"name\": \"ConnectorsAndTasks\", \"type\": \"[]String\", \"versions\": \"0+\",\n\"about\": \"The assigned topic-partitions to the member.\",\n\"fields\": [\n{ \"name\": \"Connectors\", \"type\": \"string\", \"versions\": \"0+\",\n\"about\": \"The connectors assigned to this worker\" },\n{ \"name\": \"tasks\", \"type\": \"[]ConnectorTaskID\", \"versions\": \"0+\",\n\"about\": \"The tasks assigned to this worker.\" }\n]},\n{ \"name\": \"Version\", \"type\": \"int32\", \"versions\": \"0+\",\n\"about\": \"The metadata version.\" }\n{ \"name\": \"Metadata\", \"type\": \"bytes\", \"versions\": \"0+\",\n\"about\": \"The metadata bytes.\" }\n]}\n]\n}\nRequired ACL\nRead Group\nRequest Validation\nINVALID_REQUEST is returned should the request not obey to the following invariants:\nGroupId must be non-empty.\nMemberId must be non-empty.\nMemberEpoch must be >= 0.\nBoth Partitions and ConnectorsAndTasks are set.\nRequest Handling\nWhen the group coordinator handles a ConnectGroupInstallAssignment request:\nChecks wether the group exists. If it does not, GROUP_ID_NOT_FOUND is returned.\nChecks wether the member exists. If it does not, UNKNOWN_MEMBER_ID is returned.\nChecks wether the member epoch matches the current member epoch. If it does not, FENCED_MEMBER_EPOCH is returned.\nChecks wether the member is the chosen one to compute the assignment. If it does not, UNKNOWN_MEMBER_ID is returned.\nValidates the assignment based on the information used to compute it. If it is not valid, INVALID_ASSIGNMENT is returned.\nInstalls the new target assignment.\nResponse Schema\n{\n\"apiKey\": TBD,\n\"type\": \"response\",\n\"name\": \"ConnectGroupInstallAssignment\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n// Supported errors:\n// - GROUP_AUTHORIZATION_FAILED\n// - NOT_COORDINATOR\n// - COORDINATOR_NOT_AVAILABLE\n// - COORDINATOR_LOAD_IN_PROGRESS\n// - INVALID_REQUEST\n// - INVALID_GROUP_ID\n// - GROUP_ID_NOT_FOUND\n// - UNKNOWN_MEMBER_ID\n// - FENCED_MEMBER_EPOCH\n// - INVALID_ASSIGNMENT\n\"fields\": [\n{ \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"0+\",\n\"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },\n{ \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n\"about\": \"The top-level error code, or 0 if there was no error\" },\n{ \"name\": \"ErrorMessage\", \"type\": \"string\", \"versions\": \"0+\", \"nullableVersions\": \"0+\", \"default\": \"null\",\n\"about\": \"The top-level error message, or null if there was no error.\" }\n]\n}\nResponse Handling\nIf the response contains no error, the member is done.\nUpon receiving the FENCED_MEMBER_EPOCH error, the worker retries when receiving its next heartbeat response with its member epoch.\nUpon receiving any other errors, the worker abandon the process.\nRecords\nThis section describes the new record types required for the new protocol. The storage layout is based on the data model described earlier in this document.\nThey will be persisted in the __worker_offsets compacted topic. The compacted topic based storage requires a dedicated key type per record type in order for the compaction to work. The current protocol already uses versions from 0 to 2 (included) for the keys.\nGroup Metadata\nGroups can be rather large so we propose to use several records to store a group in order to not be limited by the maximum batch size (1MB by default). Therefore we propose to store group metadata with two records types: the ConnectWorkerGroupMetadata and the ConnectWorkerGroupMemberMetadata. Note that since these messages are independent of Consumer Groups, we are introducing new record types.\nA group with X members will be stored with X+2 records. One ConnectWorkerGroupMemberMetadata per member, one ConnectWorkerGroupConnectorsTasksMetadata, and one ConnectWorkerGroupMetadata for the group at the end. Atomicity is not a concern here. All the records can be applied independently.\nMoreover, the whole group does not necessarily have to be written for every epoch. Members who have not changed could be omitted as the compacted topic will retain their previous state anyway.\nWhen a member is deleted, a tombstone for it is written to the partition.\nConnectWorkerGroupMetadataKey\n{\n\"type\": \"data\",\n\"name\": \"ConnectWorkerGroupMetadataKey\",\n\"validVersions\": \"3\",\n\"flexibleVersions\": \"none\",\n\"fields\": [\n{ \"name\": \"GroupId\", \"type\": \"string\", \"versions\": \"3\" }\n]\n}\nConnectWorkerGroupMetadataValue\n{\n\"type\": \"data\",\n\"name\": \"ConnectWorkerGroupMetadataValue\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n\"fields\": [\n{ \"name\": \"Epoch\", \"versions\": \"0+\", \"type\": \"int32\" }\n],\n}\nConnectWorkerGroupConnectorsTasksMetadataKey\n{\n\"type\": \"data\",\n\"name\": \"ConnectWorkerGroupConnectorsTasksMetadataKey\",\n\"validVersions\": \"4\",\n\"flexibleVersions\": \"none\",\n\"fields\": [\n{ \"name\": \"GroupId\", \"type\": \"string\", \"versions\": \"4\" }\n]\n}\nConnectWorkerGroupConnectorsTasksMetadataValue\n{\n\"type\": \"data\",\n\"name\": \"ConnectWorkerGroupConnectorsTasksMetadataValue\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n\"fields\": [\n{ \"name\": \"Epoch\", \"versions\": \"0+\", \"type\": \"int32\" },\n{ \"name\": \"ConnectorsTasks\", \"versions\": \"0+\",\n\"type\": \"[]ConnectorsTasks\", \"fields\": [\n{ \"name\": \"Connectors\", \"versions\": \"0+\", \"type\": \"[]String\" },\n{ \"name\": \"Tasks\", \"versions\": \"0+\", \"type\": \"[]ConnectorsTasks\" }\n]}\n],\n}\nConnectWorkerGroupMemberMetadataKey\n{\n\"type\": \"data\",\n\"name\": \"ConnectWorkerGroupMemberMetadataKey\",\n\"validVersions\": \"5\",\n\"flexibleVersions\": \"none\",\n\"fields\": [\n{ \"name\": \"GroupId\", \"type\": \"string\", \"versions\": \"5\" },\n{ \"name\": \"MemberId\", \"type\": \"string\", \"versions\": \"5\" }\n]\n}\nConnectWorkerGroupMemberMetadataValue\n{\n\"type\": \"data\",\n\"name\": \"ConnectWorkerGroupMemberMetadataValue\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n\"fields\": [\n{ \"name\": \"GroupEpoch\", \"versions\": \"0+\", \"type\": \"int32\" },\n{ \"name\": \"InstanceId\", \"versions\": \"0+\", \"type\": \"string\" },\n{ \"name\": \"ClientId\", \"versions\": \"0+\", \"type\": \"string\" },\n{ \"name\": \"ClientHost\", \"versions\": \"0+\", \"type\": \"string\" },\n{ \"name\": \"SubscribedTopicNames\", \"versions\": \"0+\", \"type\": \"[]string\" },\n{ \"name\": \"SubscribedTopicRegex\", \"versions\": \"0+\", \"type\": \"string\" },\n{ \"name\": \"Assignors\", \"versions\": \"0+\",\n\"type\": \"[]Assignor\", \"fields\": [\n{ \"name\": \"Name\", \"versions\": \"0+\", \"type\": \"string\" },\n{ \"name\": \"MinimumVersion\", \"versions\": \"0+\", \"type\": \"int16\" },\n{ \"name\": \"MaximumVersion\", \"versions\": \"0+\", \"type\": \"int16\" },\n{ \"name\": \"Reason\", \"versions\": \"0+\", \"type\": \"int8\" },\n{ \"name\": \"Version\", \"versions\": \"0+\", \"type\": \"int16\" },\n{ \"name\": \"Metadata\", \"versions\": \"0+\", \"type\": \"bytes\" }\n]}\n],\n}\nTarget Assignment\nThe target assignment is stored in a single record.\nConnectWorkerGroupTargetAssignmentKey\n{\n\"type\": \"data\",\n\"name\": \"ConnectWorkerGroupTargetAssignmentKey\",\n\"validVersions\": \"6\",\n\"flexibleVersions\": \"none\",\n\"fields\": [\n{ \"name\": \"GroupId\", \"type\": \"string\", \"versions\": \"5\" }\n]\n}\nConnectWorkerGroupTargetAssignmentValue\n{\n\"type\": \"data\",\n\"name\": \"GroupTargetAssignmentValue\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n\"fields\": [\n{ \"name\": \"AssignmentEpoch\", \"versions\": \"0+\", \"type\": \"int32\" },\n{ \"name\": \"Members\", \"versions\": \"0+\", \"type\": \"[]Member\", \"fields\": [\n{ \"name\": \"MemberId\", \"versions\": \"0+\", \"type\": \"string\" },\n{ \"name\": \"Error\", \"versions\": \"0+\", \"type\": \"int8\" },\n{ \"name\": \"ConnectorsTasks\", \"versions\": \"0+\", \"type\": \"[]ConnectorsTasks\", \"fields\":\n[\n{ \"name\": \"Connectors\", \"versions\": \"0+\", \"type\": \"[]String\" },\n{ \"name\": \"Tasks\", \"versions\": \"0+\", \"type\": \"[]ConnectorsTasks\" }\n]\n},\n{ \"name\": \"Version\", \"versions\": \"0+\", \"type\": \"int16\" },\n{ \"name\": \"Metadata\", \"versions\": \"0+\", \"type\": \"bytes\" }\n]\n]\n}\nCurrent Member Assignment\nThe current member assignment represents, as the name suggests, the current assignment of a given member.\nWhen a member is deleted from the group, a tombstone for it is written to the partition.\nConnectWorkerGroupCurrentMemberAssignmentKey\n{\n\"type\": \"data\",\n\"name\": \"ConnectWorkerGroupCurrentMemberAssignmentKey\",\n\"validVersions\": \"7\",\n\"flexibleVersions\": \"none\",\n\"fields\": [\n{ \"name\": \"GroupId\", \"type\": \"string\", \"versions\": \"7\" },\n{ \"name\": \"MemberId\", \"type\": \"string\", \"versions\": \"7\" },\n]\n}\nConnectWorkerGroupCurrentMemberAssignmentValue\n{\n\"type\": \"data\",\n\"name\": \"ConnectGroupCurrentMemberAssignmentValue\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n\"fields\": [\n{ \"name\": \"MemberEpoch\", \"versions\": \"0+\", \"type\": \"int32\" },\n{ \"name\": \"Error\", \"versions\": \"0+\", \"type\": \"int8\" },\n{ \"name\": \"ConnectorsTasks\", \"versions\": \"0+\", \"type\": \"[]ConnectorsTasks\", \"fields\":\n[\n{ \"name\": \"Connectors\", \"versions\": \"0+\", \"type\": \"[]String\" },\n{ \"name\": \"Tasks\", \"versions\": \"0+\", \"type\": \"[]ConnectorsTasks\" }\n]\n},\n{ \"name\": \"Version\", \"versions\": \"0+\", \"type\": \"int16\" },\n{ \"name\": \"Metadata\", \"versions\": \"0+\", \"type\": \"bytes\" }\n],\n}\nGroup Configurations\nConnectWorkerGroupConfigurationKey\n{\n\"type\": \"data\",\n\"name\": \"ConnectWorkerGroupConfigurationKey\",\n\"validVersions\": \"8\",\n\"flexibleVersions\": \"none\",\n\"fields\": [\n{ \"name\": \"GroupId\", \"type\": \"string\", \"versions\": \"8\" }\n]\n}\nConnectWorkerGroupConfigurationValue\n{\n\"type\": \"data\",\n\"name\": \"ConnectWorkerGroupConfigurationValue\",\n\"validVersions\": \"0\",\n\"flexibleVersions\": \"0+\",\n\"fields\": [\n{ \"name\": \"Configurations\", \"versions\": \"0+\", \"type\": \"[]Configuration\",\n\"fields\": [\n{ \"name\": \"Name\", \"type\": \"string\", \"versions\": \"0+\",\n\"about\": \"The name of the configuration key.\" },\n{ \"name\": \"Value\", \"type\": \"string\", \"versions\": \"0+\",\n\"about\": \"The value of the configuration.\" }\n]}\n]\n}\nBroker Metrics\nWe can add them later on.\nClient side API\nConnectAssignor\npackage org.apache.kafka.connect.runtime;\nimport org.apache.kafka.clients.consumer.Assignor;\nimport org.apache.kafka.connect.runtime.distributed.ExtendedAssignment;\nimport org.apache.kafka.connect.runtime.distributed.ExtendedWorkerState;\nimport org.apache.kafka.connect.runtime.distributed.WorkerCoordinator;\nimport java.util.List;\npublic interface ConnectAssignor {\nclass Group {\n/**\n* The members.\n*/\nList<GroupMember> members;\n/**\n* Connector's and tasks metadata.\n*/\nWorkerCoordinator.ConnectorsAndTasks connectorsAndTasks;\n}\nclass GroupMember {\n/**\n* The member ID.\n*/\nString memberId;\n/**\n* The instance ID if provided.\n*/\nOptional<String> instanceId;\n/**\n* The reason reported by the member.\n*/\nbyte reason;\n/**\n* The version of the metadata encoded in {{@link GroupMember#metadata()}}.\n*/\nint version;\n/**\n* The custom metadata provided by the member as defined\n* by {{@link PartitionAssignor#metadata()}}.\n*/\nByteBuffer metadata;\n/**\n* The worker state signifying the assigned connectors and\n* tasks.\n* Note\n*/\nExtendedWorkerState workerState;\n}\nclass Assignment {\n/**\n* The assignment error.\n*/\nbyte error;\n/**\n* The member assignment.\n*/\nList<MemberAssignment> members;\n}\nclass MemberAssignment {\n/**\n* The member ID.\n*/\nString memberId;\n/**\n* The error reported by the assignor.\n*/\nbyte error;\n/**\n* The version of the metadata encoded in {{@link GroupMember#metadata()}}.\n*/\nint version;\n/**\n* The custom metadata provided by the assignor.\n*/\nByteBuffer metadata;\n/**\n* The worker state signifying the assigned connectors and\n* tasks.\n*/\nExtendedAssignment assignment;\n}\nclass Metadata {\n/**\n* The reason reported by the assignor.\n*/\nbyte reason;\n/**\n* The version of the metadata encoded in {{@link Metadata#metadata()}}.\n*/\nint version;\n/**\n* The custom metadata provided by the assignor.\n*/\nByteBuffer metadata;\n}\n/**\n* Unique name for this assignor.\n*/\nString name();\n/**\n* The minimum version.\n*/\nint minimumVersion();\n/**\n* The maximum version.\n*/\nint maximumVersion();\n/**\n* Return serialized data that will be sent to the assignor.\n*/\nMetadata metadata();\n/**\n* Perform the group assignment given the current members and\n* topic metadata.\n*\n* @param group The group state.\n* @return The new assignment for the group.\n*/\nAssignment assign(Group group);\n/**\n* Callback which is invoked when the member received a new\n* assignment from the assignor/group coordinator.\n*/\nvoid onAssignment(MemberAssignment assignment);\n}\nWorker and Assignment Metadata\nWe can reuse the current metadata encoded in the protocol.\nConnect REST API Endpoints\nAll the APIs should work the way they work currently.\nCase Studies\nLet’s take a look at few example scenarios to understand how the rebalancing would work in the new protocol. I am taking a few illustrations from\nKIP-415\nto keep it familiar. Similar to the KIP,\nfirst letter of the recourse is a Connector instance (e.g. Connector A, Connector B, etc). Second letter is type: C for Connector, T for task. Number is regular task numbering. 0 for Connectors, greater or equal to 1 for Tasks. W represents a Worker, with W1 and W2, etc being different Workers joining the same group. Primes are used to represent a Worker that was member of the group and rejoins soon after a short period of being offline.\nThe config topic contains the following connectors/tasks => AC0, AT1, AT2, BC0, BT1.\nLet’s use the IncrementalCooperativeAssignor as the client side assignor.\nEmpty Group\nGroup (epoch=0)\nEmpty\nTarget Assignment (epoch=0)\nEmpty\nMember Assignment\nEmpty\nW1 joins the Group\nThe coordinator bumps the group epoch to 1, adds W1 to the group, and creates an empty member assignment.\nGroup (epoch=1)\nW1\nTarget Assignment (epoch=0)\nEmpty\nMember Assignment\nW1 - epoch=0, partitions=[]\nW1 is selected as the member to run the client assignor. All connectors and tasks would be assigned to it and installed as target assignment\nGroup (epoch=1)\nW1\nTarget Assignment (epoch=1)\nW1 - connectorsAndTasks=[AC0, AT1, AT2, BC0, BT1]\nMember Assignment\nW1 - epoch=0, connectorsAndTasks=[]\nWhen W1 heartbeats, the group coordinator transitions it to its target epoch/assignment because it does not have any connectors/tasks to revoke. The group coordinator updates the member assignment and replies with the new epoch 1 and all the assignments.\nGroup (epoch=1)\nW1\nTarget Assignment (epoch=1)\nW1 - connectorsAndTasks=[AC0, AT1, AT2, BC0, BT1]\nMember Assignment\nW1 - epoch=1, connectorsAndTasks=[AC0, AT1, AT2, BC0, BT1]\nSince this is the first member in the group, there won’t be any rebalances based on\nscheduled.rebalance.max.delay.ms\n.\nW2 joins the Group\nThe coordinator adds the member to the group and bumps the group epoch to 2.\nGroup (epoch=2)\nW1\nW2\nTarget Assignment (epoch=1)\nW1 - connectorsAndTasks=[AC0, AT1, AT2, BC0, BT1]\nMember Assignment\nW1 - epoch=1, connectorsAndTasks=[AC0, AT1, AT2, BC0, BT1]\nW2 - epoch=0, connectorsAndTasks=[]\nW1 is chosen as the member to run the client side assignor. It computes and sends the assignments to coordinator which would install it.\nGroup (epoch=2)\nW1\nW2\nTarget Assignment (epoch=2)\nW1 - connectorsAndTasks=[AC0, AT1, AT2]\nW2- connectorsAndTasks=[BC0, BT1]\nMember Assignment\nW1 - epoch=1, connectorsAndTasks=[AC0, AT1, AT2, BC0, BT1]\nW2 - epoch=1, connectorsAndTasks=[]\nW2 can be transitioned to epoch1 but it can’t have BC0, BT1 until W1 revokes them.\nWhen W1 heartbeats the next time, it would be asked to revoke them. When it acknowledges the revocation, it coordinator would transition it to epoch 2.\nGroup (epoch=2)\nW1\nW2\nTarget Assignment (epoch=2)\nW1 - connectorsAndTasks=[AC0, AT1, AT2]\nW2- connectorsAndTasks=[BC0, BT1]\nMember Assignment\nW1 - epoch=2, connectorsAndTasks=[AC0, AT1, AT2]\nW2 - epoch=1, connectorsAndTasks=[]\nWhen W2 heartbeats, it would get it’s assignments and be bumped to epoch 2.\nGroup (epoch=2)\nW1\nW2\nTarget Assignment (epoch=2)\nW1 - connectorsAndTasks=[AC0, AT1, AT2]\nW2- connectorsAndTasks=[BC0, BT1]\nMember Assignment\nW1 - epoch=2, connectorsAndTasks=[AC0, AT1, AT2]\nW2 - epoch=2, connectorsAndTasks=[BC0, BT1]\nWorker Leaving the Group\nLets assume epoch is at 35 with 3 workers\nGroup (epoch=35)\nW1\nW2\nW3\nTarget Assignment (epoch=35)\nW1 - connectorsAndTasks=[AC0, AT1]\nW2- connectorsAndTasks=[BC0, BT1]\nW3- connectorsAndTasks=[AT2]\nMember Assignment\nW1 - epoch=35, connectorsAndTasks=[AC0, AT1]\nW2 - epoch=35, connectorsAndTasks=[BC0, BT1]\nW3 - epoch=35, connectorsAndTasks=[AT2]\nLet’s say W2 fails to heartbeat and the group coordinator kicks it out after the session timeout expires and bump the group epoch.\nGroup (epoch=36)\nW1\nW2\nTarget Assignment (epoch=35)\nW1 - connectorsAndTasks=[AC0, AT1]\nW2- connectorsAndTasks=[BC0, BT1]\nW3- connectorsAndTasks=[AT2]\nMember Assignment\nW1 - epoch=35, connectorsAndTasks=[AC0, AT1]\nW3 - epoch=35, connectorsAndTasks=[AT2]\nW3 is chosen to run the client side assignor. However, this time it won’t change any assignments to give the failing worker a chance to come back.\nGroup (epoch=36)\nW1\nW3\nTarget Assignment (epoch=36)\nW1 - connectorsAndTasks=[AC0, AT1]\nW3- connectorsAndTasks=[AT2]\nMember Assignment\nW1 - epoch=35, connectorsAndTasks=[AC0, AT1]\nW3 - epoch=35, connectorsAndTasks=[AT2]\nWhen W1 and W3 heartbeat, they would transition to epoch 36 and get their new assignments (which would be the same).\nGroup (epoch=36)\nW1\nW3\nTarget Assignment (epoch=36)\nW1 - connectorsAndTasks=[AC0, AT1]\nW3- connectorsAndTasks=[AT2]\nMember Assignment\nW1 - epoch=36, connectorsAndTasks=[AC0, AT1]\nW3 - epoch=36, connectorsAndTasks=[AT2]\nAt this point, there are unassigned connectors. The chosen member would trigger another rebalance after delay\nd\n. This is in line with what\nIncrementalCooperativeProtocol\nsupports. It would do so by updating the reason field which would instruct the Group Coordinator to trigger another rebalance.\nGroup (epoch=37)\nW1\nW3\nTarget Assignment (epoch=36)\nW1 - connectorsAndTasks=[AC0, AT1]\nW3- connectorsAndTasks=[AT2]\nMember Assignment\nW1 - epoch=36, connectorsAndTasks=[AC0, AT1]\nW2 - epoch=36, connectorsAndTasks=[AT2]\nW3 is chosen to run the client side assignor. It would recompute the assignments and send them to the Group Coordinator.\nGroup (epoch=37)\nW1\nW3\nTarget Assignment (epoch=37)\nW1 - connectorsAndTasks=[AC0, AT1, BC0]\nW3- connectorsAndTasks=[BC1, AT2]\nMember Assignment\nW1 - epoch=36, connectorsAndTasks=[AC0, AT1]\nW3 - epoch=36, connectorsAndTasks=[AT2]\nWhen W1 and W3 would heartbeat, they would receive their assignments, thereby marking the rebalance as done.\nGroup (epoch=37)\nW1\nW3\nTarget Assignment (epoch=37)\nW1 - connectorsAndTasks=[AC0, AT1, BC0]\nW3- connectorsAndTasks=[BC1, AT2]\nMember Assignment\nW1 - epoch=37, connectorsAndTasks=[AC0, AT1, BC0]\nW3 - epoch=37, connectorsAndTasks=[BC1, AT2]\nWorker Bounces\nThis is the case for which incremental cooperative rebalance protocol was purpose built for. Basically, giving the worker to come back. Let’s assume a similar setup as the previous example and let’s say W2 becomes unresponsive\nInitial State\nGroup (epoch=35)\nW1\nW2\nW3\nTarget Assignment (epoch=35)\nW1 - connectorsAndTasks=[AC0, AT1]\nW2- connectorsAndTasks=[BC0, BT1]\nW3- connectorsAndTasks=[AT2]\nMember Assignment\nW1 - epoch=35, connectorsAndTasks=[AC0, AT1]\nW2 - epoch=35, connectorsAndTasks=[BC0, BT1]\nW3 - epoch=35, connectorsAndTasks=[AT2]\nW2 becomes unresponsive and it kicked out by Group Coordinator\nGroup (epoch=36)\nW1\nW2\nTarget Assignment (epoch=35)\nW1 - connectorsAndTasks=[AC0, AT1]\nW2- connectorsAndTasks=[BC0, BT1]\nW3- connectorsAndTasks=[AT2]\nMember Assignment\nW1 - epoch=35, connectorsAndTasks=[AC0, AT1]\nW3 - epoch=35, connectorsAndTasks=[AT2]\nW3 is chosen to run the client side assignor. However, this time it won’t change any assignments to give the failing worker a chance to come back.\nGroup (epoch=36)\nW1\nW3\nTarget Assignment (epoch=36)\nW1 - connectorsAndTasks=[AC0, AT1]\nW3- connectorsAndTasks=[AT2]\nMember Assignment\nW1 - epoch=35, connectorsAndTasks=[AC0, AT1]\nW3 - epoch=35, connectorsAndTasks=[AT2]\nWhen W1 and W3 heartbeat, they would transition to epoch 36 and get their new assignments (which would be the same).\nGroup (epoch=36)\nW1\nW3\nTarget Assignment (epoch=36)\nW1 - connectorsAndTasks=[AC0, AT1]\nW3- connectorsAndTasks=[AT2]\nMember Assignment\nW1 - epoch=36, connectorsAndTasks=[AC0, AT1]\nW3 - epoch=36, connectorsAndTasks=[AT2]\nSince W3 would wait for\nd\ndelay before triggering another rebalance, but let’s say before that W2 joins back. It may join with the last know epoch that it had seen or even with epoch 0. Let’s say it joins with epoch 35. The Group coordinator would notice this, trigger a rebalance and update the group epoch to 37.\nGroup (epoch=36)\nW1\nW2\nW3\nTarget Assignment (epoch=36)\nW1 - connectorsAndTasks=[AC0, AT1]\nW3- connectorsAndTasks=[AT2]\nMember Assignment\nW1 - epoch=36, connectorsAndTasks=[AC0, AT1]\nW3 - epoch=36, connectorsAndTasks=[AT2]\nSince this would anyways trigger a new assignment, W3 if chosen would be asked to re-assign. W3 would notice that there is a rebalance delay in process so it would go ahead and re-assign the same assignments and send them back to the coordinator. After this, the group would finally transition to epoch 37 with original assignments. Final states:\nGroup (epoch=37)\nW1\nW2\nW3\nTarget Assignment (epoch=37)\nW1 - connectorsAndTasks=[AC0, AT1]\nW2- connectorsAndTasks=[BC0, BT1]\nW3- connectorsAndTasks=[AT2]\nMember Assignment\nW1 - epoch=37, connectorsAndTasks=[AC0, AT1]\nW2 - epoch=37, connectorsAndTasks=[BC0, BT1]\nW3 - epoch=37, connectorsAndTasks=[AT2]",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/%5BDRAFT%5DIntegrating+Kafka+Connect+With+New+Consumer+Rebalance+Protocol",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
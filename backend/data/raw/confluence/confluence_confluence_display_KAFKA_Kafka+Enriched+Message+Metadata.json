{
  "id": "confluence_display_KAFKA_Kafka+Enriched+Message+Metadata",
  "title": "Kafka Enriched Message Metadata - Apache Kafka - Apache Software Foundation",
  "content": "Update\n: the scope of this proposal is narrowed to the kafka core properties with the focus on compression / log compaction only now. We leave other issues such as auditing that may involve application properties for future discussion.\nMotivation\nWe have been discussing about several Kafka problems:\nLog cleaning dependence on the log rolling policy (KAFKA-979):\ntoday we clean up log data at the granularity of log segments, and only considering non-active log segments. As a result, if a log does not roll for a long time, the specified log cleaning policy may not be honored. This can cause unexpected amount of data duplicates when the consumer offsets are reset to \"smallest\".\nLog segment timestamp not persist during partition migration / broker restart (KAFKA-881 / KAFKA-1379): related to the previous issue\n(KAFKA-979)\n, today the time-based log rolling mechanism depends on the creation time of the log segments, which will be changed when partition migrates or broker restarts, violating the log rolling policy. Requests about adding timestamps into the messages as well as the index files have also been proposed (\nKAFKA-1403\n).\nMirror Maker de-compress / re-compress issue (KAFKA-1001):\nMM need to always do decompression at the consumer side and then re-compress the messages at the producer side in case there are keyed messages; this can lead to high CPU / memory usage and also risk of data loss due to too-large-message after re-compression.\nBroker de-compress / re-compress issue (KAFKA-527): broker needs to de-/re-compress messages just for assigning their offsets upon receiving compressed messages, leading to high CPU / memory usage.\nThese issues\nmay be independently resolvable with separate solutions\n, but they actually come from the same root cause: some per-message metadata are either lacking (such as timestamps for log cleaning, wrapped offsets for avoiding de-/re-compression on broker / mirror maker, control messages) or being written as part of the message content, which requires Kafka to open the message content (including de-serialize / de-compress) thought it should not care about other values. Therefore, by enriching our current message metadata it is possible for us to kill them all in one stone. This page is made to inspire discussions about feasibility of this \"one stone\" approach.\nCurrent Message Format\nEnriching message metadata would be a wire protocol change. Fortunately that is affordable since we already add versions of message protocols in 0.8. The current request protocol can be found\nhere\n. In short, a message is formatted as the following:\nMessageAndOffset => MessageSize Offset Message\nMessageSize => int32\nOffset => int64\nMessage => Crc MagicByte Attributes KeyLength Key ValueLength Value\nCrc => int32\nMagicByte => int8\nAttributes => int8\nKeyLength => int32\nKey => bytes\nValueLength => int32\nValue => bytes\nThe magic byte (int8) contains the version id of the message, currently set to 0.\nThe attribute byte (int8)\nholds metadata attributes about the message. The lowest 2 bits contain the compression codec used for the message. The other bits are currently set to 0.\nThe key / value field can be omitted if the keylength / valuelength field is set to -1.\nFor compressed message, the offset field stores the last wrapped message's offset.\nProposed New Message Format\nWe would like to add the \"enqueue\" timestamp that is set by the broker upon receiving the message as the first class of the message header.\nWe would like to add someÂ \"Kafka properties\" to the message metadata that are core to Kafka that brokers care about them. Examples include:\nTimestamps upon reception (for any messages).\nNumber of wrapped messages (for compressed messages).\nWrapped message set relative offsets honor-ship (for compressed messages).\nIndicator whether the wrapped message set contain keys (for compressed messages).\n...\nHere is the proposed new message format:\nMessageAndOffset => MessageSize Offset Message\nMessageSize => int32\nOffset => int64\nMessage => Crc MagicByte Attributes Timestamp KafkaTagLength [KafkaTag] KeyLength Key ValueLength Value\nCrc => int32\nMagicByte => int8\nAttributes => int8\nTimestamp => int32\nKafkaTagLength = > int32\nKafkaTag =>\nKafkaTagId => int8\nTagValue => [different types]\nKeyLength => int32\nKey => bytes\nValueLength => int32\nValue => bytes\nHere is a summary of the changes:\nMagicByte value is set to \"1\".\nFor compressed message, the offset field stores the starting offset (i.e. the offset of the first wrapped message).\nThe inner compressed messages' offset field will store the relative offset against the starting offset (i.e., 0, 1, 2 ...)\nThe offset of the inner message will then be the starting offset + relative offset.\nWith log compaction turned on, the relative offsets will be non consecutive.\nWhen the compressed message is replicated to other clusters, the relative offsets need to be ignored and the offset of the inner message is then calculated as the starting offset + i (if it is the i-th wrapped message)\nWe will use the lowest 4 bits of the Attribute byte to indicate the type of the message:\nnormal - uncompressed\nnormal - gzip compressed\nnormal - snappy compressed\n...\n... (below are future possible types)\n...\ncontrol - leader epoch (from leader)\ncontrol - start transaction (from transaction coordinator)\ncontrol - commit transaction (from transaction coordinator)\ncontrol - abort transaction (from transaction coordinator)\n...\nKafka tags are identified by pre-defined IDs, and hence can be stored in a compact way. For example:\n0: timestamp => int64, Unix timestamp set by the broker upon receipt, and hence can change while the message is going through the pipeline via MM.\n32: num_messages => int32, number of wrapped messages in the message set; this is only used for the compressed messages.\n...\nKafkaTagsLength specifies the total bytes of the tags field, which can be used iterate through the tags or skip the whole collection directly.\nBroker reading an unknown tag id will simply ignore the tag, and if there is a necessary tag that is not present it will use some default value / log exceptions. By doing this Kafka tag protocol change would not require a strict broker / client upgrade.\nWith the new format each message's metadata size increased by 8 bytes in the best case (KafkaTagsLength = -1 indicate empty tags).\nUsing the New Message Metadata\nHere is a brief description about how we are going to use the new metadata to solve the above mentioned issues.\nBroker Offset Reassignment (KAFKA-527)\nWhen producer compressed the message, write the relative offset value in the raw message's offset field. Leave the wrapped message's offset blank.\nWhen broker receives a compressed message, it only needs to set the wrapped message's offset and hence do not need to de-/re-compress message sets.\nWhen the log cleaner is compacting log segments, when merging multiple wrapped messages into one it needs to update the raw message's relative offset values (note this will leave \"holes\" inside the new wrapped message).\nMirrorMaker Re-factoring (KAFKA-1001)\nIf non of the wrapped raw messages contains key, the producer can set the non-keyed indicator of the compressed message to true; otherwise set to false.\nWhen MM's consumers gets the compressed message, if the non-keyed indicator is set it does not need to de-compress it, otherwise it needs to compress it.\nIf it does not de-compress the message, reset the honor-ship flag of the relative message so that they will be treated as continuous offsets.\nThe new producer's API needs to be augmented to send an already compressed message (and hence not adding another message metadata into it any more).\nWhen consumers decompress message set, it will return the message with its offset either by the \"message set starting offset\" + \"relative offset\" if the honor-ship flag is set; or \"message set starting offset\" + \"index of the message in set\" otherwise.\nLog Compaction / Log Cleaning (KAFKA-881, KAFKA-979)\nAdd the timestamp field into the index file, which will then look like <offset, time-stamp, physical position>.\nThe log compaction and log cleaning method can now be incorporated in the same background thread, who will do the following upon waken up (remember a topic-partition can either be compacted or cleaned, but not both):\nIf compaction is used:\nMerge multiple consecutive message sets into one => just need to remember the starting offset of the first message set and changing the relative offset values.\nIf the relative offset honor-ship is not set do not use the relative offset but just use the index of the message in the set.\nIf log cleaning is used:\nLoop over the segments from head to tail, checking the timestamp tag of the last message from the index file. If the timestamp is old enough, delete the whole segment.\nFor the first segment whose last message's timestamp is still not old enough, do binary search based on index file's timestamp and do a head truncation on that segment file.\nWith this the log cleaning mechanism is no longer dependent on the log rolling policy.",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Enriched+Message+Metadata",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
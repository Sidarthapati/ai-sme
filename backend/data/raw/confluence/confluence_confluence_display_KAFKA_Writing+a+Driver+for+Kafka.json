{
  "id": "confluence_display_KAFKA_Writing+a+Driver+for+Kafka",
  "title": "Writing a Driver for Kafka - Apache Kafka - Apache Software Foundation",
  "content": "Writing a Driver for Kafka\nThis is an attempt to document Kafka's wire format and implementation details to encourage others to write drivers in various languages. Much of this document is based off of the Kafka\nWire Format\narticle by Jeffrey Damick and Taylor Gautier.\nStatus of this Document\nI'm currently in the process of verifying many of the things said here, to make sure they're actually a result of the protocol and not some quirk of our driver. I've tried to flag those with \"FIXME\" notes, but I'm sure I've missed a few.\nI really want to make this into the document that I wish we had at Datadog when we first started working on Kafka driver code. Corrections and comments would be greatly appreciated. Please drop me an email at\ndave@datadoghq.com\n.\nGround Rules\nIf you haven't read the\ndesign doc\n, read it. There are a few things that are outdated, but it's a great overview of Kafka.\nSome really high level takeaways to get started:\nKafka has topics, and topics have numbered partitions starting from 0. A topic can be created at runtime just by writing to it, but the number of partitions per topic is determined by broker configuration.\nKafka stores messages on disk, in a series of large, append-only log files broken up into segments. Each topic+partition is a directory of these segment files. For more details, see\nWhat are Segment Files\n.\nAn offset is just the byte offset in a given log for a topic+partition. The messages don't have any other unique identifier. They're simply stored back to back in the segment files, and you ask for them by their byte offset.\nWhen producing messages, the driver has to specify what topic and partition to send the message to. When requesting messages, the driver has to specify what topic, partition,\nand offset\nit wants them pulled from.\nWhile you can request \"old\" messages if you know their topic, partition, and offset, Kafka does not have a message index. You cannot efficiently query Kafka for the N-1000th message, or ask for all messages written between 30 and 35 minutes ago.\nKafka tends to do the simplest thing possible and relies on smarter clients to keep bookkeeping. The broker does not keep track of what the client has read. More advanced setups use ZooKeeper to help with this tracking, but that is currently beyond the scope of this document.\nThe protocol\nis a work in progress\n, and new point releases can introduce backwards incompatibile changes.\nThe broker runs on port 9092 by default.\nIf you are testing with multiple brokers on the same machine, you'll need to change both the port that they listen on (in the config file), as well as the JMX port they use. To specify a different JMX port, set the environment property JMX_PORT when starting Kafka.\nBasic Objects\nRequest Header (all single non-multi requests begin with this)\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                       REQUEST_LENGTH                          |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|         REQUEST_TYPE          |        TOPIC_LENGTH           |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                                                               /\n/                    TOPIC (variable length)                    /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                           PARTITION                           |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nREQUEST_LENGTH = int32 // Length in bytes of entire request (excluding this field)\nREQUEST_TYPE   = int16 // See table below\nTOPIC_LENGTH   = int16 // Length in bytes of the topic name\nTOPIC = String // Topic name, ASCII, not null terminated\n// This becomes the name of a directory on the broker, so no\n// chars that would be illegal on the filesystem.\nPARTITION = int32 // Partition to act on. Number of available partitions is\n// controlled by broker config. Partition numbering\n// starts at 0.\n============  =====  =======================================================\nREQUEST_TYPE  VALUE  DEFINITION\n============  =====  =======================================================\nPRODUCE         0    Send a group of messages to a topic and partition.\nFETCH           1    Fetch a group of messages from a topic and partition.\nMULTIFETCH      2    Multiple FETCH requests, chained together\nMULTIPRODUCE    3    Multiple PRODUCE requests, chained together\nOFFSETS         4    Find offsets before a certain time (this can be a bit\nmisleading, please read the details of this request).\n============  =====  =======================================================\nVery similar to the Request-Header is the multi-request header used for requesting more than one topic-partition combo at a time.  Either for multi-produce, or multi-fetch.\nMulti-Request Header (more than one topic-partition combo)\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                       REQUEST_LENGTH                          |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|         REQUEST_TYPE          |    TOPICPARTITION_COUNT       |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nREQUEST_LENGTH       = int32 // Length in bytes of entire request (excluding this field)\nREQUEST_TYPE         = int16 // See table above\nTOPICPARTITION_COUNT = int16 // number of unique topic-partition combos in this request\nResponse Header (all responses begin with this 6 byte header)\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                        RESPONSE_LENGTH                        |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|         ERROR_CODE            |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nRESPONSE_LENGTH = int32 // Length in bytes of entire response (excluding this field)\nERROR_CODE = int16 // See table below.\n================  =====  ===================================================\nERROR_CODE        VALUE  DEFINITION\n================  =====  ===================================================\nUnknown            -1    Unknown Error\nNoError             0    Success\nOffsetOutOfRange    1    Offset requested is no longer available on the server\nInvalidMessage      2    A message you sent failed its checksum and is corrupt.\nWrongPartition      3    You tried to access a partition that doesn't exist\n(was not between 0 and (num_partitions - 1)).\nInvalidFetchSize    4    The size you requested for fetching is smaller than\nthe message you're trying to fetch.\n================  =====  ===================================================\nFIXME: Add tests to verify all these codes.\nFIXME: Check that there weren't more codes added in 0.7.\nMessage (Kafka 0.6 and earlier)\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                             LENGTH                            |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|     MAGIC       |                   CHECKSUM                  |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n| CHECKSUM (cont.)|                    PAYLOAD                  /\n+-+-+-+-+-+-+-+-+-+                                             /\n/                         PAYLOAD (cont.)                       /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nLENGTH   = int32 // Length in bytes of entire message (excluding this field)\nMAGIC    = int8  // 0 is the only valid value\nCHECKSUM = int32 // CRC32 checksum of the PAYLOAD\nPAYLOAD  = Bytes[] // Message content\nThe offsets to request messages are just byte offsets. To find the offset of the next message, take the offset of this message (that you made in the request), and add LENGTH + 4 bytes (length of this message + 4 byte header to represent the length of this message).\nStarting with version 0.7, Kafka added an extra field for compression:\nMessage (Kafka 0.7 and later)\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                             LENGTH                            |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|     MAGIC       |  COMPRESSION  |           CHECKSUM          |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|      CHECKSUM (cont.)           |           PAYLOAD           /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+                             /\n/                         PAYLOAD (cont.)                       /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nLENGTH = int32 // Length in bytes of entire message (excluding this field)\nMAGIC = int8 // 0 = COMPRESSION attribute byte does not exist (v0.6 and below)\n// 1 = COMPRESSION attribute byte exists (v0.7 and above)\nCOMPRESSION = int8 // 0 = none; 1 = gzip; 2 = snappy;\n// Only exists at all if MAGIC == 1\nCHECKSUM = int32  // CRC32 checksum of the PAYLOAD\nPAYLOAD = Bytes[] // Message content\nNote that compression is end-to-end. Meaning that the Producer is responsible for sending the compressed payload, it's stored compressed on the broker, and the Consumer is responsible for decompressing it. Gzip gives better compression ratio, snappy gives faster performance.\nLet's look at what compressed messages act like:\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|          CP1         |         CP2        |         CP3         |\n| M1 | M2 | M3 | M4... | M12 | M13 | M14... | M26 | M27 | M28 ... |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nIn this scenario, let's say that\nM1\n,\nM2\n, etc. represent complete,\nuncompressed\nmessages (including headers) that the user of your library wants to send. What your driver needs to do is take\nM1\n,\nM2\n... up to some predetermined number/size, concatenate them together, and then compress them using gzip or snappy. The result (\nCP1\nin  this case) becomes the\nPAYLOAD\nfor the\ncompressed\nmessage\nCM1\nthat your library will send to Kafka.\nIt also means that we have to be careful about calculating the offsets. To Kafka,\nM1\n,\nM2\n, don't really exist. It only sees the\nCM1\nyou send. So when you make calculations for the offset you can fetch next, you have to make sure you're doing it on the boundaries of the compressed messages, not the inner messages.\nFIXME: Haven't implemented compression yet, need to verify this is correct.\nInteractions\nProduce\nTo produce messages from the driver and send to Kafka, use the following format:\nProduce Request\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                         REQUEST HEADER                        /\n/                                                               /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                         MESSAGES_LENGTH                       |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                                                               /\n/                            MESSAGES                           /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nMESSAGES_LENGTH = int32 // Length in bytes of the MESSAGES section\nMESSAGES = Collection of MESSAGES (see above)\nThere is no response to a\nPRODUCE\nRequest. There is currently no way to tell if the produce was successful or not. This is\nbeing worked\n.\nMulti-Produce\nThe multi-produce request has a different header, with the (topic-length/topic/message_length/messages) repeated many times.\nMulti-Produce Request\nHere is the general format of the multi-produce request, see multi-request header above.\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                   MULTI-REQUEST HEADER                        /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                   TOPIC-PARTION/MESSAGES (n times)             |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nPer Topic-Partition (repeated n times)\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|        TOPIC_LENGTH           |  TOPIC (variable length)      /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                           PARTITION                           |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                         MESSAGES_LENGTH                       |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                            MESSAGES                           /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nThe TOPIC_LENGTH, TOPIC, PARTITION, MESSAGES_LENGTH are documented above for size.\nFetch\nReading messages from a specific topic/partition combination.\nFetch Request\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                         REQUEST HEADER                        /\n/                                                               /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                             OFFSET                            |\n|                                                               |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                            MAX_SIZE                           |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nREQUEST_HEADER = See REQUEST_HEADER above\nOFFSET   = int64 // Offset in topic and partition to start from\nMAX_SIZE = int32 // MAX_SIZE of the message set to return\nFetch Response\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                          RESPONSE HEADER                      /\n/                                                               /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                        MESSAGES (0 or more)                   /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nEdge case behavior:\nIf you request an offset that does not exist for that topic/partition combination, you will get an OffsetOutOfRange error. While Kafka keeps messages persistent on disk, it also deletes old log files to save space.\nFIXME: VERIFY – If you request a fetch from a partition that does not exist, you will get a WrongPartition error.\nFIXME: VERIFY – If the MAX_SIZE you specify is smaller than the largest message that would be fetched, you will get an InvalidFetchSize error.\nFIXME: VERIFY – What happens when you ask for an offset that's in the middle of a message? It just sends you the chunk without checking?\nFIXME – Try invalid topic, invalid partition reading\nFIXME – Look at InvalidMessageSizeException\nNormal, but possibly unexpected behavior:\nIf you ask the broker for up to 300K worth of messages from a given topic and partition, it will send you the appropriate headers followed by a 300K chunk worth of the message log. If 300K ends in the middle of a message, you get  half a message at the end. If it ends halfway through a message header, you get a broken header. This is not an error, this is Kafka pushing complexity outward to the driver to make the broker simple and fast.\nKafka stores its messages in log files of a configurable size (512MB by default) called segments. A fetch of messages will not cross the segment boundary to read from multiple files. So if you ask for a fetch of 300K's  worth of messages and the offset you give is such that there's only one message at the end of that segment file, then you will get just one message back. The next time you call fetch with the following offset, you'll get a full set of messages from the next segment file. Basically, don't make any assumptions about how many messages are remaining from how many you got in the last fetch.\nMulti-Fetch\nMulti-Fetch Request\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                   MULTI-REQUEST HEADER                        /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|             TOPIC-PARTION-FETCH-REQUEST  (n times )           |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nREQUEST_HEADER = See MULTI_REQUEST_HEADER above\nOFFSET   = int64 // Offset in topic and partition to start from\nMAX_SIZE = int32 // MAX_SIZE of the message set to return\nThe TOPIC_LENGTH, TOPIC, PARTITION, MESSAGES_LENGTH are documented above for size.\nPer Topic-Partition-Fetch- Request (repeated n times)\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|        TOPIC_LENGTH           |  TOPIC (variable length)      /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                           PARTITION                           |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                         OFFSET                                |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                            MAX_SIZE                           /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nOffsets\nOffsets Request\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                         REQUEST HEADER                        /\n/                                                               /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                              TIME                             |\n|                                                               |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                     MAX_NUMBER (of OFFSETS)                   |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nTIME = int64 // Milliseconds since UNIX Epoch.\n// -1 = LATEST\n// -2 = EARLIEST\nMAX_NUMBER = int32 // Return up to this many offsets\nOffsets Response\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                         RESPONSE HEADER                       /\n/                                                               /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                         NUMBER_OFFSETS                        |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n/                       OFFSETS (0 or more)                     /\n/                                                               /\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nNUMBER_OFFSETS = int32 // How many offsets are being returned\nOFFSETS = int64[] // List of offsets\nThis one can be deceptive. It is\nnot\na way to get the the precise offset that occurred at a specific time. Kafka doesn't presently track things at that level of granularity, though there is a\nproposal to do so\n. To understand how this request really works, you should know how Kafka stores data. If you're unfamiliar with segment files, please see\nWhat are Segment Files\n.\nWhat Kafka does here is return up to MAX_NUMBER of offsets, sorted in descending order, where the offsets are:\nThe first offset of every segment file for a given partition with a modified time less than\nTIME\n.\nIf the last segment file for the partition is not empty and was modified earlier than\nTIME\n, it will return both the first offset for that segment and the high water mark. The high water mark is not the offset of the last message, but rather the offset that the next message sent to the partition will be written to.\nThere are special values for\nTIME\nindicating the earliest (-2) and latest (-1) time, which will fetch you the first and last offsets, respectively. Note that because offsets are pulled in descending order, asking for the earliest offset will always return you a list with a single element.\nBecause segment files are quite large and fine granularity is not possible, you may find yourself using this call mostly to get the beginning and ending offsets.\nWhat are segment files?\nSay your Kafka broker is configured to store its log files in\n/tmp/kafka-logs\nand you have a topic named \"dogs\", with two partitions. Kafka will create a directory for each partition:\n/tmp/kafka-logs/dogs-0\n/tmp/kafka-logs/dogs-1\nInside each of these partition directories, it will store the log for that topic+parition as a series of segment files. So for instance, in\ndogs-0\n, you might have:\n00000000000000000000.kafka\n00000000000536890406.kafka\n00000000001073761356.kafka\nEach file is named after the offset represented by the first message in that file. The size of the segments are configurable (512MB by default). Kafka will write to the current segment file until it goes over that size, and then will write the next message in new segment file. The files are actually slightly larger than the limit, because Kafka will finish writing the message – a single message is never split across multiple files.\nZooKeeper\nKafka relies on\nZooKeeper\nin order to coordinate multiple brokers and consumers. If you're unfamiliar with ZooKeeper, just think of it as a server that allows you to atomically create nodes in a tree, assign values to those nodes, and sign up for notifications when a node or its children get modified. Nodes can be either be permanent or ephemeral, the latter meaning that the nodes will disappear if the process that created them disconnects (after some timeout delay).\nWhile creating the nodes we care about, you'll often need to create the intermediate nodes that they are children of. For instance, since offsets are stored at\n/consumers/[consumer_group]/offsets/[topic]/[broker_id]-[partition_id]\n, something has to create\n/consumers\n,\n/consumers/[consumer_group]\n, etc. All nodes have values associated with them in ZooKeeper, even if Kafka doesn't use them for anything. To make debugging easier, the value that should be stored at an intermediate node is the ID of the node's creator. In practice that means that the first Consumer you create will need to make this skeleton structure and store its ID as the value for\n/consumers\n,\n/consumers/[consumer_group]\n, etc.\nZooKeeper has Java and C libraries, and can be run as a cluster.\nBasic Responsibilities\nKafka Brokers, Consumers, and Producers all have to coordinate using ZooKeeper. The following is a high level overview of their ZooKeeper interactions. We assume here that a Consumer only consumes one topic\nKafka Broker\nWhen starting up:\nPublish its\nbrokerid\n– a simple integer ID that uniquely identifies it.\nPublish its location, so that Producers and Consumers can find it.\nPublish all topics presently in the Broker, along with the number of partitions for each topic.\nWhenever a new topic is created:\nPublish the topic, along with the number of partitions in the topic\nProducer\nRead the locations for all Brokers with a given topic, so that we know where to send messages to.\nConsumer\nPublish what ConsumerGroup we belong to.\nPublish our unique ID so other Consumers can see us.\nDetermine which partitions on which Brokers this Consumer is responsible for, and publish its ownership of them.\nPublish what offset we've successfully read up to for every partition that we're responsible for.\nEvery Consumer belongs to exactly one ConsumerGroup. A Consumer can read from multiple brokers and partitions, but every unique combination of (ConsumerGroup, Broker, Topic, Partition) is read by only one and only one Consumer. ConsumerGroups allow you to easily support topic or queue semantics. If your Consumers are all in separate ConsumerGroups, each message goes to every Consumer. If all your Consumers are in the same ConsumerGroup, then each message goes to only one Consumer.\nWell, for the most part. The orchestration Kafka uses guarantees at least once delivery, but has edge cases that may yield duplicate delivery even in a queue (one ConsumerGroup) arrangement.\nKafka Broker\nAll these nodes are written by the Kafka Broker. Your client just needs to be able to read this broker data and understand its limitations.\nRole\nZooKeeper Path\nType\nData Description\nID Registry\n/brokers/ids/[0..N]\nEphemeral\nString in the format of \"creator:host:port\" of the broker.\nTopic Registry\n/brokers/topics/[topic]/[0..N]\nEphemeral\nNumber of partitions that topic has on that Broker.\nSo let's take the example of the following hypothetical broker:\nBroker ID is 2 (\nbrokerid=2\nin the Kafka config file)\nRunning on IP 10.0.0.12\nUsing port 9092\nTopics:\n\"dogs\" with 4 partitions\n\"mutts\" with 5 partitions\nThen the broker would register the following:\n/brokers/ids/2 = 10.0.0.12-1324306324402:10.0.0.12:9092\n/brokers/topics/dogs/2 = 4\n/brokers/topics/mutts/2 = 5\nSome things to note:\nBroker IDs don't have to be sequential, but they do have to be integers. They are a config setting, and not randomly generated. If a Kafka server goes offline for some reason and comes back an hour later, it should be reconnecting with the same Broker ID.\nThe ZooKeeper hierarchy puts individual brokers under topics because Producers and Consumers will want to put a watch on a specific topic node, to get notifications when new brokers enter or leave the pool.\nThe Broker's description is formatted such that it's\ncreator:host:port\n. The host will also up as part of the creator because of the version of UUID that Kafka's using, but don't rely on that behavior. Always split on \":\" and extract the host that will be the second element.\nThese nodes are ephemeral, so if the Broker crashes or is disconnected from the network, it will automatically be removed. But this removal is not instantaneous, and it might show up for a few seconds. This can cause errors when a broker crashes and is restarted, and subsequently tries to re-create its still existent Broker ID registry node.\nProducer\nReads:\n/brokers/topics/[topic]/[0..N]\n, so that it knows what Broker IDs are available for this topic, and how many partitions they have.\n/brokers/ids/[0..N]\n, to find the address of the Brokers, so it knows how to connect to them.\nWatches:\n/brokers/topics/[topic]\n, so that it knows when Brokers enter and exit the pool.\n/brokers/ids\n, so that it can update the Broker addresses in case you bring down a Broker and bring it back up under a different IP/port.\nProducers are fairly straightforward (with one caveat), and a Producer never has to write anything to ZooKeeper. The basic operation goes like this:\nA Producer is created for a topic.\nThe Producer reads the Broker-created nodes in\n/brokers/ids/[0..N]\nand sets up an internal mapping of Broker IDs => Kafka connections.\nThe Producer reads the nodes in\n/brokers/topics/[topic]/[0..N]\nto find the number of partitions it can send to for each Broker.\nThe Producer takes every Broker+Partition combination and puts them in an internal list.\nWhen a Producer is asked to send a message set, it picks from one of it's Broker+Partition combinations, looks up the appropriate Broker address, and sends the message set to that Broker, for that topic and partition. The precise mechanism for choosing a destination is undefined, but debugging would probably be easier if you ordered them by Broker+Partition (e.g. \"0-3\") and used a hash function to pick the index you wanted to send to. You could also just make it randomly choose.\nThe Producer's internal mappings change when:\nWhen a Broker leaves the pool or a new Broker enters it.\nThe number of partitions a Broker publishes for a topic changes.\nThe latter is actually\nextremely\ncommon, which brings us to the only tricky part about Producers – dealing with new topics.\nCreating New Topics\nTopics are not pre-determined. You create them just by sending a new message to Kafka for that topic. So let's say you have a number of Brokers that have joined the pool and don't list themselves in\n/brokers/topics/[topic]/[0..N]\nfor the topic you're interested in. They haven't done so because those topics don't exist on those Brokers yet. But our Producer knows the Brokers themselves exist, because they are in the Broker registry at\n/brokers/ids/[0..N]\n. We definitely need to send messages to them, but what partitions are safe to send to? Brokers can be configured differently from each other and topics can be configured on an individual basis, so there's no way to infer the definitive answer by looking at what's in ZooKeeper.\nThe solution is that for new topics where the number of available partitions on the Broker is unknown, you should just send to partition 0. Every Broker will at least have that one partition available. As soon as you write it and the topic comes into existence on the Broker, the Broker will publish all available partitions in ZooKeeper. You'll get notified by the watch you put on\n/brokers/topics/[topic]\n, and you'll add the new Broker+Partitions to your destination pool.\nConsumer\nFIXME: Go over all the registration stuff that needs to happen.\nRebalancing\nOccurs: When Brokers or Consumers enter or leave the pool.\nObjectives:\nAll Consumers in a ConsuerGroup will come to a consensus as to who is consuming what.\nEach Broker+Topic+Partition combination is consumed by one and only one Consumer, even if it means that some Consumers don't get anything at all.\nA Consumer should try to have as many partitions on the same Broker as possible, so sort the list by [Broker ID]-[Partition] (0-0, 0-1, 0-2, etc.), and assign them in chunks.\nConsumers are sorted by their Consumer IDs. If there are three Consumers, two Brokers, and three partitions in each, the split might look like:\nConsumer A: [0-0, 0-1]\nConsumer B: [0-2, 1-0]\nConsumer C: [1-1, 1-2]\nIf the distribution can't be even and some Consumers must have more partitions than others, the extra partitions always go to the earlier consumers on the list. So you could have a distribution like 4-4-4-4 or 5-5-4-4, but never 4-4-4-5 or 4-5-4-4.",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/Writing+a+Driver+for+Kafka",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
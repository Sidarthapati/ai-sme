{
  "id": "confluence_display_KAFKA_Incremental+Cooperative+Rebalancing%3A+Support+and+Policies",
  "title": "Incremental Cooperative Rebalancing: Support and Policies - Apache Kafka - Apache Software Foundation",
  "content": "References:\nKafka Client-side Assignment Proposal\nA Guide To The Kafka Protocol#GroupMembershipAPI\nKafkaConsumer Javadoc\nKIP-26 - Add Kafka Connect framework for data import/export\nKIP-62: Allow consumer to send heartbeats from a background thread\nKIP-134: Delay initial consumer group rebalance\nIntroduction\nRebalancing between distributed application processes in Apache Kafka was enhanced considerably when it was decoupled as logic from Kafka brokers and was moved as responsibility to the clients and specifically to Kafka Consumer. This pattern has been working robustly for quite a while now and has been used successfully, besides Kafka Consumer itself, in Kafka Streams, Kafka Connect, and other applications outside the Apache Kafka code repository. It is based on a simple principle: when a distributed process leaves or joins the group, all the members of this group stop processing and coordinate in order to redistribute the resources they share, such as their assignments to Kafka topic partitions. At the start of rebalancing, the processes will have any assigned resources to them revoked. At the end of rebalancing they will receive a new assignment of resources. At any time, one process in the group is the designated leader. The group leader is responsible for computing the assignment of resources within the group during a rebalance.\nIn this proposal, we introduce necessary improvements to the existing rebalancing procedure and propose extensions to the client side protocols - also known as embedded protocols - that will allow Kafka applications to perform rebalancing incrementally and in cooperation. This will allow Kafka applications to scale even further under circumstances where the existing rebalancing mechanism is reaching its limits or incurs unnecessary costs. Before we discuss the general approach and each design in detail, we list definitions of common terms that are used in this proposal and have been used in Kafka proposals in the past.\nDefinitions\nGroup\n:\nIn this context the term\ngroup\nis used to describe a group of distributed processes that use Kafka\nto run in cooperation as a common group.\nResource\n:\nA type of resource to be shared and distributed among the members of a\ngroup\n. Common types of resources include Kafka topic-partitions that are assigned to Kafka applications, as well as connectors and tasks that are distributed among Kafka Connect workers. Resources can be application-defined and Kafka coordinator is agnostic of their definition.\nRebalance/Rebalancing\n: the procedure that is followed by a number of distributed processes that use Kafka clients and/or the Kafka coordinator to form a common\ngroup\nand distribute a set of\nresources\namong the members of the group.\nGroup Membership Protocol:\nA protocol that is used between Kafka applications and the Kafka coordinator to define groups of distributed processes within a specific Kafka application.\nRequest/Response Types used by the Group Membership Protocol\n: The procedure that defines membership within a\ngroup\nduring\nrebalancing\nis implemented by using messages between processes of a\ngroup\nand the Kafka coordinator of the following types:\nJoinGroupRequest\nJoinGroupResponse\nSyncGroupRequest\nSyncGroupRespose\nHeartbeatRequest\nHeartbeatRespose\nEmbedded Protocol\n: A protocol that is used by Kafka applications and consists of the definition of types of\nresources\nand an algorithm that uses\nrebalancing\nas a process in order to distributed these resources among a\ngroup\nin a Kafka application.\nClass Types used by the Embedded Protocol\n: So far, all embedded protocols have been using two classes to describe and distribute resources among the group:\nAssignment\nSubscri\np\ntion\nMotivation\nCurrently, rebalancing is based on two protocols. First, a core, lower-level protocol that ensures group membership. Kafka coordinator and any Kafka component that forms groups (clients, Connect or others) are aware of the core protocol that is based on the definition of the rebalance request/response types mentioned above. Then, on top of the core protocol, a second protocol is defined that is responsible to describe resources which need to be distributed among the members of a group. This type of protocol is called embedded protocol and its logic is piggybagged within the core protocol. Because of this layering, the Kafka coordinator remains agnostic of an embedded protocol and only the members of the group, including the group leader, are using it. Since the resource aware protocols can be embedded to the core protocol without the latter being changed, it makes sense for several embedded protocols to exist on top of a single core group membership protocol. Every embedded protocol defines and distributes a different set of resources during rebalancing. However, up to now, across all embedded protocols the resources are exchanged (and therefore balanced) in an all-or-nothing fashion: in every rebalancing round, each member is releasing its resources, stopping at the same time any processing related to these resources and requests to re-join the group.\nThis situation, known also as stop-the-world effect, has proven inflexible and expensive at large scale and under specific circumstances. Specifically, there are two dimensions in which a more selective management of resources and the ability to cope with temporary imbalances could allow Kafka applications to scale even better:\nA resource that does not have to be given up because, after rebalancing, its reassignment will happen to the same process. In this case, the overhead of releasing and re-acquiring the same resources can be avoided. Furthermore, the application does not have to stall completely. Resources that do not have to be redistributed can keep being utilized. Normally, only a few resources will need to change hands and because of that the application will notice only a partial, and therefore potentially shorter, interruption due to rebalancing.\nIsolated support of heterogeneous resources. It is already the case that members of a group do not have to share resources that belong to the same application. For instance, Kafka Connect shares across a cluster of Connect workers multiple connectors and tasks of different types and different users. A worker leaving the group does not have to interrupt connectors that don't even run on this worker, or even stop connectors that have only a few tasks running on this worker. The connectors can keep running in the remaining workers until the full load is redistributed across the Connect cluster or another worker joins the group.\nBased on these two general observations, here we will examine a set of specific use cases that can benefit from incremental cooperative rebalancing that is more flexible and can handle temporary imbalances without enforcing\nstop-the-world\nacross the board. These use cases are:\nKubernetes process death\n. A node hosting a specific Kafka client process dies, but the underlying orchestrator framework is able to replace this node quickly and restore the process. A good example is a Kafka Streams application, that has state that is persisted and can be restored quickly when the process comes back up to a selected node by the orchestrator. In this case, it is often better for the application to endure a short imbalance and temporarily decrease its running tasks, instead of performing two rebalances only to return to the state that the group was operating before the node failure. Here, Kubernetes is used as a convenient placeholder for this use case's name, but the situation is equivalent across different orchestrators and cluster managers.\nRolling bounce\n. Similar to what might happen unexpectedly with a node failure, can occur with an intentional rolling upgrade of a cluster running Kafka applications. In this case, we have control over how and when a nodes comes back up from maintenance and same as in the case of failure, we would prefer to tolerate a temporary imbalance rather than perform an unnecessary and disruptive redistribution of resources several times.\nScale up/down\n. In cases where a cluster of Kafka applications scales up or down, the applications that are not affected by this scaling should not be interrupted. Even more, it would be desirable to control the pace to which scaling takes effect.\nGeneral Approach and Common Features\nThe key idea behind all the designs proposed here is to change the assumption we've always made with groups and their resources until now: when you go into a JoinGroup, it is expected that you give up control of all resources that you control and get them into a clean state for handoff. Among various components that use rebalancing, here are a few examples of what resources mean:\nKafka Consumer\n: Topic Partitions (clean up and handoff means offset commits)\nKafka Streams\n: Topic Partitions (clean up and handoff means offset commits)\nKafka Connect\n: Connectors and Tasks (clean up and handoff means flush and offset commits)\nFor other known use cases:  leadership (no clean up necessary)\nWe'd want to change the semantics of rebalancing in a way that allows processes to hold onto resources and allow things to be in an imbalanced state for a while. This leads to an enhanced embedded protocol as you need to:\nBe able to express what resources you're holding on to.\nThis is by definition part of the embedded protocol as the broker is not aware of what resources are being managed.\nThis could be done either by having each member include the resource list in their metadata, could default to holding onto everything, or something more customized to the system if it makes sense (since this is also implemented in the embedded protocol).\nBe able to pass information from the leader back to the members about how things are imbalanced or what resources they need to give up in the near future.\nAgain, fundamentally part of embedded protocol since the broker is not aware of what resources are being managed.\nThis can be added to the SyncGroupResponse as another metadata field.\nBe able to trigger another rebalance either immediately or at some time in the future based on feedback from the leader about imbalance/what resources they should give up.\nRequires being able to trigger a rebalance in\nAbstractCoordinator\neither immediately or later.\nFor the consumer and Connect this is straightforward. In fact Connect already uses this process with multiple rounds of rebalancing when it detects that the leader is behind in reading the config topic (which would result in bad assignments).\nWith respect to the implementation aspects of this enhancement, two key characteristics are: a) that ideally we'd want to apply changes to the embedded protocol only, keeping the Kafka brokers and coordinator still agnostic of the changes, and b) that the common components should be shared and usable across Kafka applications and that it will be easy for each application (e.g. Consumer, Streams, Connect) to implement and provide its own policies for rebalancing in a modular way.\nBenefits & Tradeoffs\nGood: Only modifies the embedded protocol. This has significant compatibility, and therefore adoption, implications, especially for shops where upgrading brokers is considered is a big deal compared to upgrading apps.\nGood: Decouples implementation across various Kafka components such as Consumer, Streams and Connect.\nGood: If there are different needs with respect to how imbalance is handled, we have easy flexibility in doing things differently across components. For example, Connect could start considering imbalance in Connector threads differently than imbalance in task threads.\nBad: Need to implement N times, where N == # of\nAbstractCoordinator\nimplementations\nGood: But, by aligning on requirements for the embedded rebalance protocol a good amount of code could be shared and reused.\nBad: If Streams can't just leverage consumer changes, then they need to add an implementation of\nAbstractCoordinator\nto make use of this. Currently it only relies on the\nPartitionAssignor\n.\nGood: Nothing we do using this approach has to affect non-Java clients unless they want to interoperate in the same group as Java consumers.\nConcrete Designs\nHere we'll explore a few concrete designs for the proposed protocol of incremental cooperative rebalancing. We will also evaluate which problems each solves and how various consumer group events are handled (e.g. the case where a new leader is elected).\nThere's one uncommon event that we already know we want to consider for each design:\nLeader exits and new leader is elected in this rebalance\nFurther, we want to characterize the behavior of each in terms of the following scenarios defined above in the Motivation section:\nKubernetes process death\nRolling bounce\nScale up/down\nBelow, the proposed designs will focus on the Kafka Consumer use case. For Connect, although the resources managed differ, the rebalance process is similar enough that what works for consumer groups would work for Connect as well. For Streams we propose to keep leveraging the consumer group implementation as we do today and apply any\nchanges required to adapt to the new partition management model\n. Uses cases that use the rebalance mechanism just for leadership assignment are not affected by the proposed changes and will continue to work in the same way as today.\nDesign I: Simple Cooperative Rebalancing\nThis is the simplest version that provides incremental cooperative rebalancing in multiple rounds. The format of the\nSubscription\nand\nAssignment\nclasses respectively are defined as follows:\nSubscription (Member → Leader):\nSubscription => Version SubscribeTopics PartitionAssignorData AssignedTopicPartitions\nVersion                 => Int16\nSubscribeTopics         => [String]\nPartitionAssignorData   => Bytes\nAssignedTopicPartitions => [Topic Partitions]\nTopic         => String\nPartitions    => [int32]\nAssignment (Leader → Member):\nAssignment => Version AssignedTopicPartitions RevokeTopicPartitions\nVersion\t\t\t\t\t=> int16\nAssignedTopicPartitions \t=> [Topic Partitions]\nTopic         => String\nPartitions    => [int32]\nRevokeTopicPartitions \t=> [Topic Partitions]\nTopic         => String\nPartitions    => [int32]\nMember process:\nInclude subscription topics as usual in\nJoinGroup\n. Also include your previous assignment (null or empty for new members)\nWhen you need to join group, by default hold on to all partitions and continue processing.\nAssignment response includes usual assignment information. Start processing any new partitions. (Since we expect sticky assignment, we could also optimize this and omit the assignment when it is just repeating a previous assignment)\nIf assignment response includes anything in RevokePartitions, stop processing the revoked partitions (continuing processing of still assigned partitions), commit, and then initiate another join group immediately.\nLeader process:\nUse partition assignor on subscription requests as usual for each round of join group. Compute diff from previous data (which we have from\nAssignedTopicPartitions\nin the subscription) and include necessary partitions in\nRevokePartitions\n. The returned\nAssignedTopicPartitions\nshould be the computed assignment minus anything included in\nRevokePartitions\nsince we need to wait for them to give up.\nThe leader must also account for \"lost\" or unaccounted topic partitions when a member is no longer in the group. Such are topic partitions that are included in the metadata for the subscribed topics but are not present in\nAssignedTopicPartitions\n. This case should be handled just by ensuring all topic partitions in the subscriptions are assigned.\nEvents:\nMember joins\n- On the first join group round, the leader will compute partition assignments for the new member but none will be returned in\nAssignedPartitions\nwhile we wait for other members to do revocation. The new member will get its assignment after everyone else revokes and rejoins.\nExample: Member joins\nInitial group and assignment: A(T1,T4), B(T2), C(T3)\nD(T) joins\nLeader computes new assignment as: A(T1), B(T2), C(T3), D(T4)\nbut sends assignment with revocation request: A(assigned:,revoked:T4), B(assigned:,revoked:), C(assigned:,revoked:), D(assigned:,revoked:)\nMembers join with subscriptions: A(T,assigned:T1), B(T,assigned:T2), C(T,assigned:T3), D(T,assigned:)\nLeader computes new assignment as: A(assigned:,revoked:), B(assigned:,revoked:), C(assigned:,revoked:), D(assigned:T4,revoked:)\nMember leaves\n- Rebalance will be triggered by leave group (or\nsession timeout\n). When the leader computes new assignments, the partitions previously assigned to the former member will not be in any\nAssignedTopicPartitions\nof the Subscription messages sent by the currently participating members. Therefore, these partitions are considered revoked already and they can be immediately assigned in this iteration. No further rebalances will be\nrequired\n.\nExample: Member leaves\nInitial group and assignment: A(T1), B(T2), C(T3), D(T4)\nD(T4) leaves\nRebalance is triggered. Remaining member rejoin with subscriptions:\nA(T,assigned:T1), B(T,assigned:T2), C(T,assigned:T3)\nLeader computes new assignment as: A(assigned:T1,T4,revoked:), B(assigned:T2,revoked:), C(assigned:T3,revoked:)\nMember bounces\n- This case is just a\nmember leaves\nevent, followed by a\nmember joins\nevent. There will be one rebalance when the member leaves and partitions are reassigned, then two rebalances: the first one when the member rejoins and revocations are returned and then the second one that will deliver the new assignment to all members.\nLeader exits and new leader is elected in this rebalance\n- Including AssignedTopicPartitions in the member metadata requires re-sending that information, but allows a new leader, even if it was new to the group, to properly maintain stickiness.\nCharacteristics:\nKubernetes process death\n-\nIMPROVED\nif restoring the process takes longer than it takes for other members to rejoin the group, this version doesn't help for this situation. However, it is better than before as only the partitions on the node are affected.\nRolling bounce\n-\nIMPROVED\nrebalances are still triggered immediately so rolling bounces would see multiple rebalances. However, the impact of this is still greatly reduced as only the bounced node's partitions jump around.\nScale up/down\n-\nYES\neveryone is involved in the rebalance, but only affected topic partitions actually stop processing temporarily\nDiscussion:\nIn general this policy results in more rebalances. However, it has the benefit that in all cases only the partitions being moved are affected (assuming join group moved to background thread, see note in decisions below). Further, some of the rebalances are expected to be fast if we get all members to know to rejoin quickly. So having more rebalances isn't really an issue unless it has substantial resource overhead.\nOne implicit change here is that\nConsumerRebalanceListener\nnow may be invoked with partial sets, or we need to introduce another callback interface that handles that case. Possibly this change and a config to switch between protocol versions are the only public API changes that would be needed.\nThe main overhead here is the addition of\nAssignedTopicPartitions\n, which could be substantial in cases like MirrorMaker. However, this is no worse than the assignment data being sent from the leader to members.\nAlso, note that we need to take care in computing assignments. The\nAssignedTopicPartitions\ncould be a subset of the total set of topic partitions; combining this with a new leader, it is important to generate assignments from the\nSubscribeTopics\n, only using\nAssignedTopicPartitions\nto a) keep things sticky and b) figure out what needs to be included in\nRevokeTopicPartitions\n.\nAs with all of these proposals, the assumption is that members are cooperative, so there is no explicit indication that topic partitions were actually given up.\nFinally, one potential concern is that not all members have an indicator that a rebalance is going to be required since they base this on the presence of any\nRevokeTopicPartitions\n. One possible improvement would be to rely on an empty, non-null list (or an extra boolean indicator field) to notify them immediately. This could potentially speed up the second rebalance since they wouldn't have to wait to get the indicator via heartbeat.\nDesign II: Deferred Resolution of Imbalance\nThis variant builds on the previous one and adds control over when we should schedule another rebalance instead of always trying to resolve imbalance immediately.\nSubscription\nand\nAssignment\nclasses are defined as follows:\nSubscription (Member → Leader):\nSubscription => Version SubscribeTopics PartitionAssignorData AssignedTopicPartitions\nVersion                 => Int16\nSubscribeTopics         => [String]\nPartitionAssignorData   => Bytes\nAssignedTopicPartitions => [Topic Partitions]\nTopic         => String\nPartitions    => [int32]\nAssignment (Leader → Member):\nAssignment => Version AssignedTopicPartitions RevokeTopicPartitions ScheduledRebalanceTimeout\nVersion         \t\t\t\t=> int16\nAssignedTopicPartitions \t\t=> [Topic Partitions]\nTopic         => String\nPartitions    => [int32]\nRevokeTopicPartitions \t\t=> [Topic Partitions]\nTopic         => String\nPartitions    => [int32]\nScheduledRebalanceTimeout\t=> int32\nNote that the only difference in the format is\nScheduledRebalanceTimeout\nand we might also choose to make this a fixed constant set via configuration instead of dynamic in the protocol.\nMember process:\n(Same as above)\nInclude subscription topics as usual in join group. Also include your previous assignment (null or empty for new members)\n(Same as above)\nWhen you need to join group, by default hold on to all partitions and continue processing\n(Same as above)\nAssignment response includes usual assignment information. Start processing any new partitions. (Since we expect sticky assignment, we could also optimize this and omit the assignment when it is just repeating a previous assignment)\n(Same as above)\nIf assignment response includes anything in RevokePartitions, stop processing, commit, and then initiate another join group immediately.\nIf\nScheduledRebalanceTimeout\n> 0, plan to rejoin as soon as possible\nafter\nthat timeout. (This should only be set if the\nRevokePartitions\nis empty.)\nLeader process:\n(Same as above)\nUse partition assignor on subscription requests as usual for each round of join group. Compute diff from previous data (which we have from\nAssignedTopicPartitions\nin the subscription) and include necessary partitions in\nRevokePartitions\n. The returned\nAssignedTopicPartitions\nshould be the computed assignment minus anything included in\nRevokePartitions\nsince we need to wait for them to give up.\n\"Lost\" or unaccounted topic partitions that belong to topics subscribed by at least one member but are not listed in any\nAssignedTopicPartitions\nfrom members indicate either:\na) new subscriptions\nThis case can be detected based on the subscriptions and assignments. If a topic is completely missing from previous assignments, assume it is this case.\nThese should be resolved immediately, so just continue with immediate assignment.\nb) a member left the group\nIf we didn't detect a new subscription, assume it is due to this case.\nHowever, if we detect a new member, we want to utilize it immediately to resolve the imbalance. This could happen if the process manages to get restarted within the time it takes to do the rebalance\n. This process rejoins the group but its list of previously\nAssignedTopicPartitions\nis empty due to the restart. For a stable leader, they can just compare the set of member IDs to the last generation. For a new leader we can use a heuristic such as looking for a member that has no previous assignments (assuming there's enough members that they\nshould\nhave had an assignment).\nIf not reassigning immediately, set\nScheduledRebalanceTimeout\nappropriately to defer the actual movement in the hopes the member will reappear. In this case there should be no\nRevokeTopicPartitions\n.\nEvents:\nMember joins\n-\nFirst member: The behavior here is customizable. Either the leader can immediately assign topic partitions or use the deferment process to allow other members to\njoin in that period\n.\nAdditional members: Reassignment happens immediately via two step revoke + assign rebalances. Deferring only applies to \"lost\" partitions.\nMember leaves\n- The policy detects a member left and uses\nScheduledRebalanceTimeout\nto get the group to wait some time before resolving any imbalance.\nExample: Member leaves\nInitial group and assignment: A(T1), B(T2), C(T3), D(T4)\nD(T4) leaves\nRebalance is triggered. Remaining member rejoin with subscriptions:\nA(T,assigned:T1), B(T,assigned:T2), C(T,assigned:T3)\nLeader computes detects \"lost\" partition T4. Sends empty assignments, without revocations and a scheduled rebalance timeout of t1:\nA(assigned:,revoked:,t1), B(assigned:,revoked:,t1), C(assigned:,revoked:,t1)\nAfter t1, remaining members join again:\nA(T,assigned:T1), B(T,assigned:T2), C(T,assigned:T3)\nLeader sends updated assignment:\nA(assigned:T1,T4,revoked:,-), B(assigned:,revoked:,-), C(assigned:,revoked:,-)\nMember bounces\n-\nThis\ncase is same as a\nmember leaves\nevent, followed by a\nmember joins\nevent. The new member is detected as a probably returning member and gets the same assignment back (assuming subscriptions haven't changed and the partition assignor is sticky)\nExample: Member bounces\nInitial group and assignment: A(T1), B(T2), C(T3), D(T4)\nD(T4) bounces. First leaves the group.\nRebalance is triggered. Remaining member rejoin with subscriptions:\nA(T,assigned:T1), B(T,assigned:T2), C(T,assigned:T3)\nLeader computes detects \"lost\" partition T4. Sends empty assignments, without revocations and a scheduled rebalance timeout of t1:\nA(assigned:,revoked:,t1), B(assigned:,revoked:,t1), C(assigned:,revoked:,t1)\nBefore t1, member D joins again as D'\nRebalance is triggered. All members join with subscriptions:\nA(T,assigned:T1), B(T,assigned:T2), C(T,assigned:T3), D'(T,assigned:)\nLeader sends updated assignment:\nA(assigned:,revoked:,-), B(assigned:,revoked:,-), C(assigned:,revoked:,-), D'(assigned:T4,revoked:,-)\nLeader exits and new leader is elected in this rebalance\n- This case is treated as loss of member. Enough info is available to at least heuristically detect whether the leader bounced back and can immediately be reassigned or if we put the missing partitions into purgatory during the timeout.\nWhat if the previous leader was in the middle of waiting for a scheduled timeout?\nIf the new leader was already in the group, they can just use the timeout they should already know about and not override it.\nIf the new leader is new to the group, they should fall back to assuming there wasn't a wait period in effect.\nCharacteristics:\nKubernetes process death\n-\nYES\nOne of the key cases this extension is designed for. This policy will wait for a process to have a chance to reappear before reassigning topic partitions. If the process appears earlier than the\nScheduledRebalanceTimeout\ntimeout, we just assign immediately and get back to work, so you are limited by how fast kubernetes detects the failure and brings back your process.\nRolling bounce\n-\nYES\nEffectively the same as above. We still have multiple rebalances for each process bounce, but they don't have the same impact. As in the case above, we only need to wait as long as it takes for the process to come back up.\nScale up/down\n-\nYES\nScale up is detected as a new member and reassignment happens immediately. Scale down takes the\nScheduledRebalanceTimeout\nto resolve since we need to be confident the member has really left the group.\nDiscussion:\nAs noted, in the cases where a process comes back, the outage is basically only limited to the time it takes the process to come back (plus the heartbeat interval since other members need to find out about the need to rejoin). This means users in environments like that could reasonably set the scheduled rebalance timeout relatively high as long as they don't expect to regularly have scale down events.\nHaving the flexibility to control delaying assignment in the protocol covers the need for delayed rebalancing when a new consumer group is formed, which was the subject of KIP-134 (\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-134%3A+Delay+initial+consumer+group+rebalance\n). Therefore, implementation of this deferred resolution of imbalance policy could allow us to provide a more holistic solution and\ndeprecate the specific configuration (\ngroup.initial.rebalance.delay.ms\n), that is addressing consecutive rebalances only at the initial stage of a consumer group.\nThe loss of state when the leader changes to a member new to the group can result in a liveness situation under which the time before actually performing an assignment keeps getting extended indefinitely. This can only happen if the leader changes to a new member of the group, within the scheduled rebalance timeout indefinitely. Arguably, this is unlikely enough that it is safe to ignore (bouncing members so fast probably means you're doing something wrong and you shouldn't expect stable groups anyway).\nDesign III: Incremental Resolution of Imbalance\nThis variant is just an extension of the behavior of the deferred resolution. The idea is simple so a full description is skipped here. The only change is that the leader could choose to only reassign\nsome\npartitions and handle the rest in the same iterative fashion incrementally.\nIn this case you probably want the\nScheduledRebalanceTimeout\nto be explicit in the message because you might use one interval for detecting members leaving and another for how long to wait between moving partitions.\nThis would be less for the detection of nodes that go missing/reappear since in that case you really just want to assign everything that was \"lost\" to the new node, or evenly distribute. The case where this is useful would be something like Connect where you might scale down # of tasks for a connector such that you would be in an imbalanced state when just those tasks were deleted, or you're scaling up your cluster and add a new node (the key characteristic being that the cluster is imbalanced, but everything is still assigned and humming along). If you move everything at once, all affected tasks have to stop + commit before anyone can proceed. If you have an heavy connector with a lot of buffered data and a light connector with little, the light connector tasks get stuck doing nothing while waiting for the heavy connector tasks to finish. By moving each individually, we decouple them and minimize the outage for each task.\nCharacteristics:\nKubernetes process death\n-\nYES\nRolling bounce\n-\nYES\nScale up/down\n-\nYES\nHere, scale up has the added benefit that you get a smoother, incremental move back to balance and no coupling of flush/commit behavior across partitions.\nDiscussion:\nThis policy aims to be helpful in very large deployments of groups with diverse resources, such as large Connect clusters with a diverse set of connectors and tasks.\nCompatibility\nAs discussed above, the proposed changes here focus on the embedded protocol. In this case, a round of rolling bounces will be sufficient to upgrade all the members of a group to the latest version of the embedded protocol. Another round might be useful if it's desired to disable the old version completely.\nConclusions\nHere are some conclusions that derive from the above:\nWe will probably need to make it possible for the consumer to join group in the background thread. This is necessary because even if we don't give up partitions, the join group process itself blocks processing since the consumer works in single threaded mode. This means that if node A doesn't have any revoked partitions but node B does and takes 1ms to flush + commit + join and node C also has some revoked but takes minutes, when node A joins it will still be blocked from processing until node C finishes up.\nOut of the available options regarding any potential enhancement to the rebalancing mechanisms and the respective protocols, an incremental approach where we keep the changes primarily to the embedded protocol is a good first step. We can evaluate the need for other changes that might involve changes in the Kafka coordinator's protocol after implementing the proposed enhancements here. This will allow for the implementation of various policies in the different Kafka components to move at their own pace.\nAcknowledgements\nThis document was composed based on original contributions by\nEwen Cheslack-Postava\n,\nJason Gustafson\n,\nGuozhang Wang\nand\nKonstantine Karantasis",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/Incremental+Cooperative+Rebalancing%3A+Support+and+Policies",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
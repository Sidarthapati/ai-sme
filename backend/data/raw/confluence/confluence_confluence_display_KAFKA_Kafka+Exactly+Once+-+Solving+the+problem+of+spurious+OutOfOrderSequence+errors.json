{
  "id": "confluence_display_KAFKA_Kafka+Exactly+Once+-+Solving+the+problem+of+spurious+OutOfOrderSequence+errors",
  "title": "Kafka Exactly Once - Solving the problem of spurious OutOfOrderSequence errors - Apache Kafka - Apache Software Foundation",
  "content": "Background\nIn the discussion of\nKIP-185: Make exactly once in order delivery per partition the default producer setting\n, the following point regarding the\nOutOfOrderSequenceException\nwas raised:\nThe\nOutOfOrderSequenceException\nindicates that there has been data loss on the broker.. ie. a previously acknowledged message no longer exists. For most part, this should only occur in rare situations (simultaneous power outages, multiple disk losses, software bugs resulting in data corruption, etc.).\nHowever, there is another perfectly normal scenario where data is removed: in particular, data could be deleted because it is old and crosses the retention threshold.\nHence, if a producer remains inactive for longer than a topic's retention period, we could get an\nOutOfOrderSequence\nwhich is a false positive: the data is removed through valid processes, and this isn't an error.\nIn the current implementation of the code, we currently raise an\nOutOfOrderSequenceException\nwhen we get a duplicate of a batch which is not at the tail of the log. This is also confusing, and a more clear error would be the\nDuplicateSequenceException\nin this case.\nWe would like to eliminate the possibility of getting spurious\nOutOfOrderSequenceExceptions\n– when you get it, it should always mean data loss and should be taken very seriously.\nDesign\nEssentially, we want to distinguish between the case where a producer's state is removed from the broker because the retention time has elapsed, and when the state is lost due to some problem in the system.\nOne solution is described here:\nWhen the producer metadata is removed from the\nProducerStateManager\non the broker due to retention, the next\nProduceRequest\nfrom the client will arrive with the existing producer id and with a non-zero sequence. Currently this results in an\nOutOfOrderSequenceException\nreturned by the broker, since the broker can't find any metadata and gets a non-zero sequence. This isn't strictly correct, and we propose introducing a new\nUnknownProducerException\nand returning this instead.\nThe client can treat the\nUnknownProducerException\nas a non-fatal error and just reinitialize the producer and continue on its merry way\nin most cases\n.\nHowever, the above solution opens a hole: if the first write from the producer is actually lost (maybe due to a simultaneous power outage, multiple disk failures, etc.), we would not detect it. In particular, the first write with sequence = 0 is written, but then the records are lost on the broker. The next write with sequence=N would get an\nUknownProducerException\nand with the protocol above would simply be retried. Hence the fact that a message was lost would never be raised to the application. This applies to the first write because it is only at the front of the log where there could be a confusion between removal due to retention or loss due to an unforeseen circumstance.\nWe can solve the situation in (3), by keeping track of the last ack'd offset on the producer, and also returning the log start offset in each\nProduceResponse\n. With these two pieces of information, we can be sure that an\nUknownProducerException\nis valid if the log start offset returned along with the error code is greater than the last ack'd offset. This means that the front of the log has been truncated, causing the producer to become unknown. In this case, there is no unwanted data loss and the last batch can simply be retried. If we get an\nUnkownProducerException\nbut the log start offset is\nnot\ngreater than the last ack'd offset, then the record has been not been lost due to the retention period elapsing, and this should be treated as a fatal error.\nIf we a trying to append a batch with a sequence less than the sequence of the batch at the tail of the log, we should return a\nDuplicateSequenceException\ninstead of an\nOutOfOrderSequencException\nWith the changes above, an\nOutOfOrderSequenceException\nwould always mean real data loss. An\nUnkownProducerException\nmay mean data loss, and the information of the last ack'd offset and log start offset will enable us to disambiguate.\nLevel of Effort\nClient side changes to track the last ack'd offset and correctly interpret an\nUnknownProducerException\nand either retry it or raise it as an error – 1 day.\nBroker side changes to raise the\nUnkownProducerException\n– 0.25 days.\nUpdates to the protocol to return the\nlogStartOffset\nper partition (with KIP) - 2 days.\nSystem tests + Debugging - 2 days\nTotal : 1.25 weeks.",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Exactly+Once+-+Solving+the+problem+of+spurious+OutOfOrderSequence+errors",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
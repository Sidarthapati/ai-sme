{
  "id": "confluence_display_KAFKA_System+Tools",
  "title": "System Tools - Apache Kafka - Apache Software Foundation",
  "content": "The source code for Apache Kafka System Tools are located\nhttps://github.com/apache/kafka/tree/0.8/core/src/main/scala/kafka/tools\nIf you are looking for the replication related tools then please check out the wiki page on\nReplication tools\nSystem tools can be run from the command line using the run class script (i.e. bin/kafka-run-class.sh package.class --options)\nConsumer Offset Checker\nThis tool has been removed in Kafka 1.0.0.  Use kafka-consumer-groups.sh to get consumer group details.\nDisplays the:  Consumer Group, Topic, Partitions, Offset, logSize, Lag, Owner for the specified set of Topics and Consumer Group\nbin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker\nrequired argument: [group]\nOption Description\n------ -----------\n--broker-info Print broker info\n--group Consumer group.\n--help Print this message.\n--topic Comma-separated list of consumer\ntopics (all topics if absent).\n--zkconnect ZooKeeper connect string. (default:\nlocalhost:2181\n)\nDump Log Segment\nThis can print the messages directly from the log files or just verify the indexes correct for the logs\nbin/kafka-run-class.sh kafka.tools.DumpLogSegments\nrequired argument \"[files]\"\nOption Description\n------ -----------\n--deep-iteration if set, uses deep instead of shallow iteration\n--files <file1, file2, ...> REQUIRED: The comma separated list of data and index log files to be dumped\n--max-message-size <Integer: size> Size of largest message. (default: 5242880)\n--print-data-log if set, printing the messages content when dumping data logs\n--verify-index-only if set, just verify the index log without printing its content\nExport Zookeeper Offsets\nA utility that retrieves the offsets of broker partitions in ZK and prints to an output file in the following format:\n/consumers/group1/offsets/topic1/1-0:286894308\n/consumers/group1/offsets/topic1/2-0:284803985\nbin/kafka-run-class.sh kafka.tools.ExportZkOffsets\nrequired argument: [zkconnect]\nOption Description\n------ -----------\n--group Consumer group.\n--help Print this message.\n--output-file Output file\n--zkconnect ZooKeeper connect string. (default:\nlocalhost:2181\n)\nGet Offset Shell\nget offsets for a topic\nbin/kafka-run-class.sh kafka.tools.GetOffsetShell\nrequired argument [broker-list], [topic]\nOption Description\n------ -----------\n--broker-list <\nhostname:port\n,..., REQUIRED: The list of hostname and\nhostname:port\n> port of the server to connect to.\n--max-wait-ms <Integer: ms> The max amount of time each fetch request waits. (default: 1000)\n--offsets <Integer: count> number of offsets returned (default: 1)\n--partitions <partition ids> comma separated list of partition ids. If not specified, will find offsets for all partitions (default)\n--time <Long: timestamp in milliseconds / -1(latest) / -2 (earliest) timestamp; offsets will come before this timestamp, as in getOffsetsBefore  >\n--topic <topic> REQUIRED: The topic to get offsets from.\nImport Zookeeper Offsets\ncan import offsets for a topic partitions\nfile format is the same as for the export\n/consumers/group1/offsets/topic1/1-0:286894308\n/consumers/group1/offsets/topic1/2-0:284803985\nbin/kafka-run-class.sh kafka.tools.ImportZkOffsets\nrequired argument: [input-file]\nOption Description\n------ -----------\n--help Print this message.\n--input-file Input file\n--zkconnect ZooKeeper connect string. (default:\nlocalhost:2181\n)\nJMX Tool\nprints metrics via JMX\nbin/kafka-run-class.sh kafka.tools.JmxTool\nOption Description\n------ -----------\n--attributes <name> The whitelist of attributes to query. This is a comma-separated list. If no attributes are specified all objects will be queried.\n--date-format <format> The date format to use for formatting the time field. See java.text. SimpleDateFormat for options.\n--help Print usage information.\n--jmx-url <service-url> The url to connect to to poll JMX data. See Oracle javadoc for JMXServiceURL for details. (default:\nservice:jmx:rmi:///jndi/rmi://\n: 9999/jmxrmi)\n--object-name <name> A JMX object name to use as a query. This can contain wild cards, and  this option can be given multiple times to specify more than one query. If no objects are specified all objects will be queried.\n--reporting-interval <Integer: ms> Interval in MS with which to poll jmx stats. (default: 2000)\nKafka Migration Tool\nMigrates a 0.7 broker to 0.8\nbin/kafka-run-class.sh kafka.tools.KafkaMigrationTool\nMissing required argument \"[consumer.config]\"\nOption Description\n------ -----------\n--blacklist <Java regex (String)> Blacklist of topics to migrate from the 0.7 cluster\n--consumer.config <config file> Kafka 0.7 consumer config to consume from the source 0.7 cluster. You man specify multiple of these.\n--help Print this message.\n--kafka.07.jar <kafka 0.7 jar> Kafka 0.7 jar file\n--num.producers <Integer: Number of Number of producer instances (default: producers> 1)\n--num.streams <Integer: Number of Number of consumer streams (default: 1)consumer threads>\n--producer.config <config file> Producer config.\n--queue.size <Integer: Queue size in Number of messages that are buffered terms of number of messages> between the 0.7 consumer and 0.8 producer (default: 10000)\n--whitelist <Java regex (String)> Whitelist of topics to migrate from the 0.7 cluster\n--zkclient.01.jar <zkClient 0.1 jar zkClient 0.1 jar file file required by Kafka 0.7>\nMirror Maker\nProvides mirroring of one Kafka cluster to another, for more info check out the wiki page on\nKafka mirroring (MirrorMaker)\nbin/kafka-run-class.sh kafka.tools.MirrorMaker\nrequired argument [consumer.config]\nOption Description\n------ -----------\n--blacklist <Java regex (String)> Blacklist of topics to mirror.\n--consumer.config <config file> Consumer config to consume from a source cluster. You may specify multiple of these.\n--help Print this message.\n--num.producers <Integer: Number of Number of producer instances (default: producers> 1)\n--num.streams <Integer: Number of Number of consumption streams. threads> (default: 1)\n--producer.config <config file> Embedded producer config.\n--queue.size <Integer: Queue size in Number of messages that are buffered terms of number of messages> between the consumer and producer (default: 10000)\n--whitelist <Java regex (String)> Whitelist of topics to mirror.\nReplay Log Producer\nConsume from one topic and replay those messages and produce to another topic\nbin/kafka-run-class.sh kafka.tools.ReplayLogProducer\nrequired argument [broker-list], [input-topic], [output-topic], [zookeeper]\nOption Description\n------ -----------\n--async If set, messages are sent asynchronously.\n--batch-size <Integer: batch size> Number of messages to send in a single batch. (default: 200)\n--broker-list <\nhostname:port\n> REQUIRED: the broker list must be specified.\n--compression-codec <Integer: If set, messages are sent compressed compression codec > (default: 0)\n--delay-btw-batch-ms <Long: ms> Delay in ms between 2 batch sends. (default: 0)\n--inputtopic <input-topic> REQUIRED: The topic to consume from.\n--messages <Integer: count> The number of messages to send. (default: -1)\n--outputtopic <output-topic> REQUIRED: The topic to produce to\n--reporting-interval <Integer: size> Interval at which to print progress info. (default: 5000)\n--threads <Integer: threads> Number of sending threads. (default: 1)\n--zookeeper <zookeeper url> REQUIRED: The connection string for the zookeeper connection in the form\nhost:port\n. Multiple URLS can be given to allow fail-over. (default: 127.0.0.1:2181)\nSimple Consumer Shell\nDumps out consumed messages to the console using the Simple Consumer\nbin/kafka-run-class.sh kafka.tools.SimpleConsumerShell\nrequired argument [broker-list], [topic]\nOption Description\n------ -----------\n--broker-list <\nhostname:port\n,..., REQUIRED: The list of hostname and\nhostname:port\n> port of the server to connect to.\n--clientId <clientId> The ID of this client. (default: SimpleConsumerShell)\n--fetchsize <Integer: fetchsize> The fetch size of each request. (default: 1048576)\n--formatter <class> The name of a class to use for formatting kafka messages for display. (default: kafka.consumer. DefaultMessageFormatter)\n--max-messages <Integer: max-messages> The number of messages to consume (default: 2147483647)\n--max-wait-ms <Integer: ms> The max amount of time each fetch request waits. (default: 1000)\n--no-wait-at-logend If set, when the simple consumer reaches the end of the Log, it will stop, not waiting for new produced messages\n--offset <Long: consume offset> The offset id to consume from, default to -2 which means from beginning; while value -1 means from end (default: -2)\n--partition <Integer: partition> The partition to consume from. (default: 0)\n--print-offsets Print the offsets returned by the iterator\n--property <prop>\n--replica <Integer: replica id> The replica id to consume from, default -1 means leader broker. (default: -1)\n--skip-message-on-error If there is an error when processing a message, skip it instead of halt.\n--topic <topic> REQUIRED: The topic to consume from.\nState Change Log Merger\nA utility that merges the state change logs (possibly obtained from different brokers and over multiple days).\nbin/kafka-run-class.sh kafka.tools.StateChangeLogMerger\nProvide arguments to exactly one of the two options \"[logs]\" or \"[logs-regex]\"\nOption Description\n------ -----------\n--end-time <end timestamp in the The latest timestamp of state change format java.text. log entries to be merged (default: SimpleDateFormat@f17a63e7> 9999-12-31 23:59:59,999)\n--logs <file1,file2,...> Comma separated list of state change logs or a regex for the log file names\n--logs-regex <for example: /tmp/state- Regex to match the state change log change.log*> files to be merged\n--partitions <0,1,2,...> Comma separated list of partition ids whose state change logs should be merged\n--start-time <start timestamp in the The earliest timestamp of state change format java.text. log entries to be merged (default: SimpleDateFormat@f17a63e7> 0000-00-00 00:00:00,000)\n--topic <topic> The topic whose state change logs should be merged\nUpdate Offsets In Zookeeper\nA utility that updates the offset of every broker partition to the offset of earliest or latest log segment file, in ZK.\nbin/kafka-run-class.sh kafka.tools.UpdateOffsetsInZK\nUSAGE: kafka.tools.UpdateOffsetsInZK$ [earliest | latest] consumer.properties topic\nVerify Consumer Rebalance\nMake sure there is an owner for every partition.\nA successful rebalancing operation would select an owner for each available partition.\nThis means that for each partition registered under /brokers/topics/[topic]/[broker-id], an owner exists\nunder /consumers/[consumer_group]/owners/[topic]/[broker_id-partition_id]\nbin/kafka-run-class.sh kafka.tools.VerifyConsumerRebalance\nrequired argument: [group]\nOption Description\n------ -----------\n--group Consumer group.\n--help Print this message.\n--zookeeper.connect ZooKeeper connect string. (default:\nlocalhost:2181\n)",
  "url": "https://cwiki.apache.org/confluence/display/KAFKA/System+Tools",
  "space": "KAFKA",
  "labels": [],
  "last_modified": null,
  "source_type": "confluence"
}
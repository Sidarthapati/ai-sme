{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_RecordAccumulator.java_911",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "content": "                    // sequence number, since this may introduce duplicates. In particular, the previous attempt\n                    // may actually have been accepted, and if we change the producer id and sequence here, this\n                    // attempt will also be accepted, causing a duplicate.\n                    //\n                    // Additionally, we update the next sequence number bound for the partition, and also have\n                    // the transaction manager track the batch so as to ensure that sequence ordering is maintained\n                    // even if we receive out of order responses.\n                    batch.setProducerState(producerIdAndEpoch, transactionManager.sequenceNumber(batch.topicPartition), isTransactional);\n                    transactionManager.incrementSequenceNumber(batch.topicPartition, batch.recordCount);\n                    log.debug(\"Assigned producerId {} and producerEpoch {} to batch with base sequence \" +\n                            \"{} being sent to partition {}\", producerIdAndEpoch.producerId,\n                        producerIdAndEpoch.epoch, batch.baseSequence(), tp);\n\n                    transactionManager.addInFlightBatch(batch);\n                }\n            }\n\n            // the rest of the work by processing outside the lock\n            // close() is particularly expensive\n            batch.close();\n            size += batch.records().sizeInBytes();\n            ready.add(batch);\n\n            batch.drained(now);\n        } while (start != drainIndex);\n        return ready;\n    }\n\n    private int getDrainIndex(String idString) {\n        return nodesDrainIndex.computeIfAbsent(idString, s -> 0);\n    }\n\n    private void updateDrainIndex(String idString, int drainIndex) {\n        nodesDrainIndex.put(idString, drainIndex);\n    }\n\n    /**\n     * Drain all the data for the given nodes and collate them into a list of batches that will fit\n     * within the specified size on a per-node basis. This method attempts to avoid choosing the same\n     * topic-node over and over.\n     *\n     * @param metadataSnapshot  The current cluster metadata\n     * @param nodes             The list of node to drain\n     * @param maxSize           The maximum number of bytes to drain\n     * @param now               The current unix time in milliseconds\n     * @return A list of {@link ProducerBatch} for each node specified with total size less than the\n     * requested maxSize.\n     */\n    public Map<Integer, List<ProducerBatch>> drain(MetadataSnapshot metadataSnapshot, Set<Node> nodes, int maxSize, long now) {\n        if (nodes.isEmpty())\n            return Collections.emptyMap();\n\n        Map<Integer, List<ProducerBatch>> batches = new HashMap<>();\n        for (Node node : nodes) {\n            List<ProducerBatch> ready = drainBatchesForOneNode(metadataSnapshot, node, maxSize, now);\n            batches.put(node.id(), ready);\n        }\n        return batches;\n    }\n\n    public void updateNodeLatencyStats(Integer nodeId, long nowMs, boolean canDrain) {\n        // Don't bother with updating stats if the feature is turned off.\n        if (partitionAvailabilityTimeoutMs <= 0)\n            return;\n\n        // When the sender gets a node (returned by the ready() function) that has data to send\n        // but the node is not ready (and so we cannot drain the data), we only update the\n        // ready time, then the difference would reflect for how long a node wasn't ready\n        // to send the data.  Then we can temporarily remove partitions that are handled by the\n        // node from the list of available partitions so that the partitioner wouldn't pick\n        // this partition.\n        // NOTE: there is no synchronization for metric updates, so drainTimeMs is updated\n        // first to avoid accidentally marking a partition unavailable if the reader gets\n        // values between updates.\n        NodeLatencyStats nodeLatencyStats = nodeStats.computeIfAbsent(nodeId, id -> new NodeLatencyStats(nowMs));\n        if (canDrain)\n            nodeLatencyStats.drainTimeMs = nowMs;\n        nodeLatencyStats.readyTimeMs = nowMs;\n    }\n\n    /* Visible for testing */\n    public NodeLatencyStats getNodeLatencyStats(Integer nodeId) {\n        return nodeStats.get(nodeId);\n    }\n\n    /* Visible for testing */\n    public BuiltInPartitioner getBuiltInPartitioner(String topic) {\n        return topicInfoMap.get(topic).builtInPartitioner;\n    }\n\n    /**\n     * The earliest absolute time a batch will expire (in milliseconds)\n     */\n    public long nextExpiryTimeMs() {\n        return this.nextBatchExpiryTimeMs;\n    }\n\n      /* Visible for testing */\n    public Deque<ProducerBatch> getDeque(TopicPartition tp) {\n        TopicInfo topicInfo = topicInfoMap.get(tp.topic());\n        if (topicInfo == null)\n            return null;\n        return topicInfo.batches.get(tp.partition());\n    }\n\n    /**\n     * Get the deque for the given topic-partition, creating it if necessary.\n     */\n    private Deque<ProducerBatch> getOrCreateDeque(TopicPartition tp) {\n        TopicInfo topicInfo = topicInfoMap.computeIfAbsent(tp.topic(),\n                k -> new TopicInfo(createBuiltInPartitioner(logContext, k, batchSize)));\n        return topicInfo.batches.computeIfAbsent(tp.partition(), k -> new ArrayDeque<>());\n    }\n\n    BuiltInPartitioner createBuiltInPartitioner(LogContext logContext, String topic, int stickyBatchSize) {\n        return new BuiltInPartitioner(logContext, topic, stickyBatchSize);\n    }\n\n    /**\n     * Complete and deallocate the record batch\n     */\n    public void completeAndDeallocateBatch(ProducerBatch batch) {\n        completeBatch(batch);\n        deallocate(batch);\n    }\n\n    /**\n     * Only perform deallocation (and not removal from the incomplete set)\n     */\n    public void deallocate(ProducerBatch batch) {\n        // Only deallocate the batch if it is not a split batch because split batch are allocated outside the\n        // buffer pool.\n        if (!batch.isSplitBatch()) {\n            if (batch.isBufferDeallocated()) {\n                log.warn(\"Skipping deallocating a batch that has already been deallocated. Batch is {}, created time is {}\", batch, batch.createdMs);\n            } else {\n                batch.markBufferDeallocated();\n                if (batch.isInflight()) {\n                    // Create a fresh ByteBuffer to give to BufferPool to reuse since we can't safely call deallocate with the ProduceBatch's buffer\n                    free.deallocate(ByteBuffer.allocate(batch.initialCapacity()));\n                    throw new IllegalStateException(\"Attempting to deallocate a batch that is inflight. Batch is \" + batch);\n                }\n                free.deallocate(batch.buffer(), batch.initialCapacity());\n            }\n        }\n    }\n\n    /**\n     * Remove from the incomplete list but do not free memory yet\n     */",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java#L911-L1060",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 911,
  "end_line": 1060,
  "last_modified": "2026-02-06T01:16:27.610051",
  "source_type": "github"
}
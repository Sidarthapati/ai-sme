{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_Metadata.java_521",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/Metadata.java",
  "content": "                    updateLatestMetadata(partitionMetadata, metadataResponse.hasReliableLeaderEpochs(), topicId, oldTopicId)\n                        .ifPresent(partitions::add);\n\n                    if (partitionMetadata.error.exception() instanceof InvalidMetadataException) {\n                        log.debug(\"Requesting metadata update for partition {} due to error {}\",\n                                partitionMetadata.topicPartition, partitionMetadata.error);\n                        requestUpdate(false);\n                    }\n                }\n            } else {\n                if (metadata.error().exception() instanceof InvalidMetadataException) {\n                    log.debug(\"Requesting metadata update for topic {} due to error {}\", topicName, metadata.error());\n                    requestUpdate(false);\n                }\n\n                if (metadata.error() == Errors.INVALID_TOPIC_EXCEPTION)\n                    invalidTopics.add(topicName);\n                else if (metadata.error() == Errors.TOPIC_AUTHORIZATION_FAILED)\n                    unauthorizedTopics.add(topicName);\n            }\n        }\n\n        Map<Integer, Node> nodes = metadataResponse.brokersById();\n        if (isPartialUpdate)\n            return this.metadataSnapshot.mergeWith(metadataResponse.clusterId(), nodes, partitions,\n                unauthorizedTopics, invalidTopics, internalTopics, metadataResponse.controller(), topicIds,\n                (topic, isInternal) -> !topics.contains(topic) && retainTopic(topic, isInternal, nowMs));\n        else\n            return new MetadataSnapshot(metadataResponse.clusterId(), nodes, partitions,\n                unauthorizedTopics, invalidTopics, internalTopics, metadataResponse.controller(), topicIds);\n    }\n\n    /**\n     * Compute the latest partition metadata to cache given ordering by leader epochs (if both\n     * available and reliable) and whether the topic ID changed.\n     */\n    private Optional<MetadataResponse.PartitionMetadata> updateLatestMetadata(\n            MetadataResponse.PartitionMetadata partitionMetadata,\n            boolean hasReliableLeaderEpoch,\n            Uuid topicId,\n            Uuid oldTopicId) {\n        TopicPartition tp = partitionMetadata.topicPartition;\n        if (hasReliableLeaderEpoch && partitionMetadata.leaderEpoch.isPresent()) {\n            int newEpoch = partitionMetadata.leaderEpoch.get();\n            Integer currentEpoch = lastSeenLeaderEpochs.get(tp);\n            if (currentEpoch == null) {\n                // We have no previous info, so we can just insert the new epoch info\n                log.debug(\"Setting the last seen epoch of partition {} to {} since the last known epoch was undefined.\",\n                        tp, newEpoch);\n                lastSeenLeaderEpochs.put(tp, newEpoch);\n                this.equivalentResponseCount = 0;\n                return Optional.of(partitionMetadata);\n            } else if (topicId != null && !topicId.equals(oldTopicId)) {\n                // If the new topic ID is valid and different from the last seen topic ID, update the metadata.\n                // Between the time that a topic is deleted and re-created, the client may lose track of the\n                // corresponding topicId (i.e. `oldTopicId` will be null). In this case, when we discover the new\n                // topicId, we allow the corresponding leader epoch to override the last seen value.\n                log.info(\"Resetting the last seen epoch of partition {} to {} since the associated topicId changed from {} to {}\",\n                        tp, newEpoch, oldTopicId, topicId);\n                lastSeenLeaderEpochs.put(tp, newEpoch);\n                this.equivalentResponseCount = 0;\n                return Optional.of(partitionMetadata);\n            } else if (newEpoch >= currentEpoch) {\n                // If the received leader epoch is at least the same as the previous one, update the metadata\n                log.debug(\"Updating last seen epoch for partition {} from {} to epoch {} from new metadata\", tp, currentEpoch, newEpoch);\n                lastSeenLeaderEpochs.put(tp, newEpoch);\n                if (newEpoch > currentEpoch) {\n                    this.equivalentResponseCount = 0;\n                }\n                return Optional.of(partitionMetadata);\n            } else {\n                // Otherwise ignore the new metadata and use the previously cached info\n                log.debug(\"Got metadata for an older epoch {} (current is {}) for partition {}, not updating\", newEpoch, currentEpoch, tp);\n                return metadataSnapshot.partitionMetadata(tp);\n            }\n        } else {\n            // Handle old cluster formats as well as error responses where leader and epoch are missing\n            lastSeenLeaderEpochs.remove(tp);\n            this.equivalentResponseCount = 0;\n            return Optional.of(partitionMetadata.withoutLeaderEpoch());\n        }\n    }\n\n    /**\n     * If any non-retriable exceptions were encountered during metadata update, clear and throw the exception.\n     * This is used by the consumer to propagate any fatal exceptions or topic exceptions for any of the topics\n     * in the consumer's Metadata.\n     */\n    public synchronized void maybeThrowAnyException() {\n        clearErrorsAndMaybeThrowException(this::recoverableException);\n    }\n\n    /**\n     * If any fatal exceptions were encountered during metadata update, throw the exception. This is used by\n     * the producer to abort waiting for metadata if there were fatal exceptions (e.g. authentication failures)\n     * in the last metadata update.\n     */\n    protected synchronized void maybeThrowFatalException() {\n        KafkaException metadataException = this.fatalException;\n        if (metadataException != null) {\n            fatalException = null;\n            throw metadataException;\n        }\n    }\n\n    /**\n     * If any non-retriable exceptions were encountered during metadata update, throw exception if the exception\n     * is fatal or related to the specified topic. All exceptions from the last metadata update are cleared.\n     * This is used by the producer to propagate topic metadata errors for send requests.\n     */\n    public synchronized void maybeThrowExceptionForTopic(String topic) {\n        clearErrorsAndMaybeThrowException(() -> recoverableExceptionForTopic(topic));\n    }\n\n    private void clearErrorsAndMaybeThrowException(Supplier<KafkaException> recoverableExceptionSupplier) {\n        KafkaException metadataException = Optional.ofNullable(fatalException).orElseGet(recoverableExceptionSupplier);\n        fatalException = null;\n        clearRecoverableErrors();\n        if (metadataException != null)\n            throw metadataException;\n    }\n\n    // We may be able to recover from this exception if metadata for this topic is no longer needed\n    private KafkaException recoverableException() {\n        if (!unauthorizedTopics.isEmpty())\n            return new TopicAuthorizationException(unauthorizedTopics);\n        else if (!invalidTopics.isEmpty())\n            return new InvalidTopicException(invalidTopics);\n        else\n            return null;\n    }\n\n    private KafkaException recoverableExceptionForTopic(String topic) {\n        if (unauthorizedTopics.contains(topic))\n            return new TopicAuthorizationException(Collections.singleton(topic));\n        else if (invalidTopics.contains(topic))\n            return new InvalidTopicException(Collections.singleton(topic));\n        else\n            return null;\n    }\n\n    private void clearRecoverableErrors() {\n        invalidTopics = Collections.emptySet();\n        unauthorizedTopics = Collections.emptySet();\n    }\n\n    /**\n     * Record an attempt to update the metadata that failed. We need to keep track of this\n     * to avoid retrying immediately.\n     */",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/Metadata.java#L521-L670",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/Metadata.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 521,
  "end_line": 670,
  "last_modified": "2026-02-06T01:16:27.578514",
  "source_type": "github"
}
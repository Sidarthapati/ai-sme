{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_BuiltInPartitioner.java_131",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/BuiltInPartitioner.java",
  "content": "     *\n     * 1. peekCurrentPartitionInfo is called to know which partition to lock.\n     * 2. Lock partition's batch queue.\n     * 3. isPartitionChanged under lock to make sure that nobody raced us.\n     * 4. Append data to buffer.\n     * 5. updatePartitionInfo to update produced bytes and maybe switch partition.\n     *\n     *  It's important that steps 3-5 are under partition's batch queue lock.\n     *\n     * @param cluster The cluster information (needed if there is no current partition)\n     * @return sticky partition info object\n     */\n    StickyPartitionInfo peekCurrentPartitionInfo(Cluster cluster) {\n        StickyPartitionInfo partitionInfo = stickyPartitionInfo.get();\n        if (partitionInfo != null)\n            return partitionInfo;\n\n        // We're the first to create it.\n        partitionInfo = new StickyPartitionInfo(nextPartition(cluster));\n        if (stickyPartitionInfo.compareAndSet(null, partitionInfo))\n            return partitionInfo;\n\n        // Someone has raced us.\n        return stickyPartitionInfo.get();\n    }\n\n    /**\n     * Check if partition is changed by a concurrent thread.  NOTE this function needs to be called under\n     * the partition's batch queue lock.\n     *\n     * @param partitionInfo The sticky partition info object returned by peekCurrentPartitionInfo\n     * @return true if sticky partition object is changed (race condition)\n     */\n    boolean isPartitionChanged(StickyPartitionInfo partitionInfo) {\n        // partitionInfo may be null if the caller didn't use built-in partitioner.\n        return partitionInfo != null && stickyPartitionInfo.get() != partitionInfo;\n    }\n\n    /**\n     * Update partition info with the number of bytes appended and maybe switch partition.\n     * NOTE this function needs to be called under the partition's batch queue lock.\n     *\n     * @param partitionInfo The sticky partition info object returned by peekCurrentPartitionInfo\n     * @param appendedBytes The number of bytes appended to this partition\n     * @param cluster The cluster information\n     */\n    void updatePartitionInfo(StickyPartitionInfo partitionInfo, int appendedBytes, Cluster cluster) {\n        updatePartitionInfo(partitionInfo, appendedBytes, cluster, true);\n    }\n\n    /**\n     * Update partition info with the number of bytes appended and maybe switch partition.\n     * NOTE this function needs to be called under the partition's batch queue lock.\n     *\n     * @param partitionInfo The sticky partition info object returned by peekCurrentPartitionInfo\n     * @param appendedBytes The number of bytes appended to this partition\n     * @param cluster The cluster information\n     * @param enableSwitch If true, switch partition once produced enough bytes\n     */\n    void updatePartitionInfo(StickyPartitionInfo partitionInfo, int appendedBytes, Cluster cluster, boolean enableSwitch) {\n        // partitionInfo may be null if the caller didn't use built-in partitioner.\n        if (partitionInfo == null)\n            return;\n\n        assert partitionInfo == stickyPartitionInfo.get();\n        int producedBytes = partitionInfo.producedBytes.addAndGet(appendedBytes);\n\n        // We're trying to switch partition once we produce stickyBatchSize bytes to a partition\n        // but doing so may hinder batching because partition switch may happen while batch isn't\n        // ready to send.  This situation is especially likely with high linger.ms setting.\n        // Consider the following example:\n        //   linger.ms=500, producer produces 12KB in 500ms, batch.size=16KB\n        //     - first batch collects 12KB in 500ms, gets sent\n        //     - second batch collects 4KB, then we switch partition, so 4KB gets eventually sent\n        //     - ... and so on - we'd get 12KB and 4KB batches\n        // To get more optimal batching and avoid 4KB fractional batches, the caller may disallow\n        // partition switch if batch is not ready to send, so with the example above we'd avoid\n        // fractional 4KB batches: in that case the scenario would look like this:\n        //     - first batch collects 12KB in 500ms, gets sent\n        //     - second batch collects 4KB, but partition switch doesn't happen because batch in not ready\n        //     - second batch collects 12KB in 500ms, gets sent and now we switch partition.\n        //     - ... and so on - we'd just send 12KB batches\n        // We cap the produced bytes to not exceed 2x of the batch size to avoid pathological cases\n        // (e.g. if we have a mix of keyed and unkeyed messages, key messages may create an\n        // unready batch after the batch that disabled partition switch becomes ready).\n        // As a result, with high latency.ms setting we end up switching partitions after producing\n        // between stickyBatchSize and stickyBatchSize * 2 bytes, to better align with batch boundary.\n        if (producedBytes >= stickyBatchSize * 2) {\n            log.trace(\"Produced {} bytes, exceeding twice the batch size of {} bytes, with switching set to {}\",\n                producedBytes, stickyBatchSize, enableSwitch);\n        }\n\n        if (producedBytes >= stickyBatchSize && enableSwitch || producedBytes >= stickyBatchSize * 2) {\n            // We've produced enough to this partition, switch to next.\n            StickyPartitionInfo newPartitionInfo = new StickyPartitionInfo(nextPartition(cluster));\n            stickyPartitionInfo.set(newPartitionInfo);\n        }\n    }\n\n    /**\n     * Update partition load stats from the queue sizes of each partition\n     * NOTE: queueSizes are modified in place to avoid allocations\n     *\n     * @param queueSizes The queue sizes, partitions without leaders are excluded\n     * @param partitionIds The partition ids for the queues, partitions without leaders are excluded\n     * @param length The logical length of the arrays (could be less): we may eliminate some partitions\n     *               based on latency, but to avoid reallocation of the arrays, we just decrement\n     *               logical length\n     * Visible for testing\n     */\n    public void updatePartitionLoadStats(int[] queueSizes, int[] partitionIds, int length) {\n        if (queueSizes == null) {\n            log.trace(\"No load stats for topic {}, not using adaptive\", topic);\n            partitionLoadStats = null;\n            return;\n        }\n        assert queueSizes.length == partitionIds.length;\n        assert length <= queueSizes.length;\n\n        // The queueSizes.length represents the number of all partitions in the topic and if we have\n        // less than 2 partitions, there is no need to do adaptive logic.\n        // If partitioner.availability.timeout.ms != 0, then partitions that experience high latencies\n        // (greater than partitioner.availability.timeout.ms) may be excluded, the length represents\n        // partitions that are not excluded.  If some partitions were excluded, we'd still want to\n        // go through adaptive logic, even if we have one partition.\n        // See also RecordAccumulator#partitionReady where the queueSizes are built.\n        if (length < 1 || queueSizes.length < 2) {\n            log.trace(\"The number of partitions is too small: available={}, all={}, not using adaptive for topic {}\",\n                    length, queueSizes.length, topic);\n            partitionLoadStats = null;\n            return;\n        }\n\n        // We build cumulative frequency table from the queue sizes in place.  At the beginning\n        // each entry contains queue size, then we invert it (so it represents the frequency)\n        // and convert to a running sum.  Then a uniformly distributed random variable\n        // in the range [0..last) would map to a partition with weighted probability.\n        // Example: suppose we have 3 partitions with the corresponding queue sizes:\n        //  0 3 1\n        // Then we can invert them by subtracting the queue size from the max queue size + 1 = 4:\n        //  4 1 3\n        // Then we can convert it into a running sum (next value adds previous value):\n        //  4 5 8\n        // Now if we get a random number in the range [0..8) and find the first value that\n        // is strictly greater than the number (e.g. for 4 it would be 5), then the index of\n        // the value is the index of the partition we're looking for.  In this example\n        // random numbers 0, 1, 2, 3 would map to partition[0], 4 would map to partition[1]\n        // and 5, 6, 7 would map to partition[2].\n\n        // Calculate max queue size + 1 and check if all sizes are the same.",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/BuiltInPartitioner.java#L131-L280",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/BuiltInPartitioner.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 131,
  "end_line": 280,
  "last_modified": "2026-02-06T01:16:27.609205",
  "source_type": "github"
}
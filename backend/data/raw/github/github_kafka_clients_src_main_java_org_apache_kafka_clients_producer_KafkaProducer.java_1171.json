{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_KafkaProducer.java_1171",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "content": "                TopicPartition tp = appendCallbacks.topicPartition();\n                RecordMetadata nullMetadata = new RecordMetadata(tp, -1, -1, RecordBatch.NO_TIMESTAMP, -1, -1);\n                callback.onCompletion(nullMetadata, e);\n            }\n            this.errors.record();\n            this.interceptors.onSendError(record, appendCallbacks.topicPartition(), e);\n            if (transactionManager != null) {\n                transactionManager.maybeTransitionToErrorState(e);\n            }\n            return new FutureFailure(e);\n        } catch (InterruptedException e) {\n            this.errors.record();\n            this.interceptors.onSendError(record, appendCallbacks.topicPartition(), e);\n            throw new InterruptException(e);\n        } catch (KafkaException e) {\n            this.errors.record();\n            this.interceptors.onSendError(record, appendCallbacks.topicPartition(), e);\n            throw e;\n        } catch (Exception e) {\n            // we notify interceptor about all exceptions, since onSend is called before anything else in this method\n            this.interceptors.onSendError(record, appendCallbacks.topicPartition(), e);\n            throw e;\n        }\n    }\n\n    private void setReadOnly(Headers headers) {\n        if (headers instanceof RecordHeaders) {\n            ((RecordHeaders) headers).setReadOnly();\n        }\n    }\n\n    /**\n     * Wait for cluster metadata including partitions for the given topic to be available.\n     * @param topic The topic we want metadata for\n     * @param partition A specific partition expected to exist in metadata, or null if there's no preference\n     * @param nowMs The current time in ms\n     * @param maxWaitMs The maximum time in ms for waiting on the metadata\n     * @return The cluster containing topic metadata and the amount of time we waited in ms\n     * @throws TimeoutException if metadata could not be refreshed within {@code max.block.ms}\n     * @throws KafkaException for all Kafka-related exceptions, including the case where this method is called after producer close\n     */\n    private ClusterAndWaitTime waitOnMetadata(String topic, Integer partition, long nowMs, long maxWaitMs) throws InterruptedException {\n        Cluster cluster = metadata.fetch();\n\n        if (cluster.invalidTopics().contains(topic))\n            throw new InvalidTopicException(topic);\n\n        // add topic to metadata topic list if it is not there already and reset expiry\n        metadata.add(topic, nowMs);\n\n        Integer partitionsCount = cluster.partitionCountForTopic(topic);\n        // Return cached metadata if we have it, and if the record's partition is either undefined\n        // or within the known partition range\n        if (partitionsCount != null && (partition == null || partition < partitionsCount))\n            return new ClusterAndWaitTime(cluster, 0);\n\n        long remainingWaitMs = maxWaitMs;\n        long elapsed = 0;\n        // Issue metadata requests until we have metadata for the topic and the requested partition,\n        // or until maxWaitTimeMs is exceeded. This is necessary in case the metadata\n        // is stale and the number of partitions for this topic has increased in the meantime.\n        long nowNanos = time.nanoseconds();\n        do {\n            if (partition != null) {\n                log.trace(\"Requesting metadata update for partition {} of topic {}.\", partition, topic);\n            } else {\n                log.trace(\"Requesting metadata update for topic {}.\", topic);\n            }\n            metadata.add(topic, nowMs + elapsed);\n            int version = metadata.requestUpdateForTopic(topic);\n            sender.wakeup();\n            try {\n                metadata.awaitUpdate(version, remainingWaitMs);\n            } catch (TimeoutException ex) {\n                // Rethrow with original maxWaitMs to prevent logging exception with remainingWaitMs\n                final String errorMessage = getErrorMessage(partitionsCount, topic, partition, maxWaitMs);\n                if (metadata.getError(topic) != null) {\n                    throw new TimeoutException(errorMessage, metadata.getError(topic).exception());\n                }\n                throw new TimeoutException(errorMessage);\n            }\n            cluster = metadata.fetch();\n            elapsed = time.milliseconds() - nowMs;\n            if (elapsed >= maxWaitMs) {\n                final String errorMessage = getErrorMessage(partitionsCount, topic, partition, maxWaitMs);\n                if (metadata.getError(topic) != null && metadata.getError(topic).exception() instanceof RetriableException) {\n                    throw new TimeoutException(errorMessage, metadata.getError(topic).exception());\n                }\n                throw new TimeoutException(errorMessage);\n            }\n            metadata.maybeThrowExceptionForTopic(topic);\n            remainingWaitMs = maxWaitMs - elapsed;\n            partitionsCount = cluster.partitionCountForTopic(topic);\n        } while (partitionsCount == null || (partition != null && partition >= partitionsCount));\n\n        producerMetrics.recordMetadataWait(time.nanoseconds() - nowNanos);\n\n        return new ClusterAndWaitTime(cluster, elapsed);\n    }\n\n    private String getErrorMessage(Integer partitionsCount, String topic, Integer partition, long maxWaitMs) {\n        return partitionsCount == null ?\n            String.format(\"Topic %s not present in metadata after %d ms.\",\n                topic, maxWaitMs) :\n            String.format(\"Partition %d of topic %s with partition count %d is not present in metadata after %d ms.\",\n                partition, topic, partitionsCount, maxWaitMs);\n    }\n    /**\n     * Validate that the record size isn't too large\n     */\n    private void ensureValidRecordSize(int size) {\n        if (size > maxRequestSize)\n            throw new RecordTooLargeException(\"The message is \" + size +\n                    \" bytes when serialized which is larger than \" + maxRequestSize + \", which is the value of the \" +\n                    ProducerConfig.MAX_REQUEST_SIZE_CONFIG + \" configuration.\");\n        if (size > totalMemorySize)\n            throw new RecordTooLargeException(\"The message is \" + size +\n                    \" bytes when serialized which is larger than the total memory buffer you have configured with the \" +\n                    ProducerConfig.BUFFER_MEMORY_CONFIG +\n                    \" configuration.\");\n    }\n\n    /**\n     * Invoking this method makes all buffered records immediately available to send (even if <code>linger.ms</code> is\n     * greater than 0) and blocks on the completion of the requests associated with these records. The post-condition\n     * of <code>flush()</code> is that any previously sent record will have completed (e.g. <code>Future.isDone() == true</code>\n     * and callbacks passed to {@link #send(ProducerRecord,Callback)} have been called).\n     * A request is considered completed when it is successfully acknowledged\n     * according to the <code>acks</code> configuration you have specified or else it results in an error.\n     * <p>\n     * Other threads can continue sending records while one thread is blocked waiting for a flush call to complete,\n     * however no guarantee is made about the completion of records sent after the flush call begins.\n     * <p>\n     * This method can be useful when consuming from some input system and producing into Kafka. The <code>flush()</code> call\n     * gives a convenient way to ensure all previously sent messages have actually completed.\n     * <p>\n     * This example shows how to consume from one Kafka topic and produce to another Kafka topic:\n     * <pre>\n     * {@code\n     * for(ConsumerRecord<String, String> record: consumer.poll(100))\n     *     producer.send(new ProducerRecord(\"my-topic\", record.key(), record.value());\n     * producer.flush();\n     * consumer.commitSync();\n     * }\n     * </pre>\n     *\n     * Note that the above example may drop records if the produce request fails. If we want to ensure that this does not occur\n     * we need to set <code>retries=&lt;large_number&gt;</code> in our config.\n     * </p>\n     * <p>",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L1171-L1320",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 1171,
  "end_line": 1320,
  "last_modified": "2026-02-06T01:16:27.608270",
  "source_type": "github"
}
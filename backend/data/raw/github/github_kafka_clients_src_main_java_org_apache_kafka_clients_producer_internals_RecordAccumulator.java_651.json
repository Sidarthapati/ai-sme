{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_RecordAccumulator.java_651",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "content": "        ConcurrentMap<Integer, Deque<ProducerBatch>> batches = topicInfo.batches;\n        // Collect the queue sizes for available partitions to be used in adaptive partitioning.\n        int[] queueSizes = null;\n        int[] partitionIds = null;\n        if (enableAdaptivePartitioning && batches.size() >= metadataSnapshot.cluster().partitionsForTopic(topic).size()) {\n            // We don't do adaptive partitioning until we scheduled at least a batch for all\n            // partitions (i.e. we have the corresponding entries in the batches map), we just\n            // do uniform.  The reason is that we build queue sizes from the batches map,\n            // and if an entry is missing in the batches map, then adaptive partitioning logic\n            // won't know about it and won't switch to it.\n            queueSizes = new int[batches.size()];\n            partitionIds = new int[queueSizes.length];\n        }\n\n        int queueSizesIndex = -1;\n        boolean exhausted = this.free.queued() > 0;\n        for (Map.Entry<Integer, Deque<ProducerBatch>> entry : batches.entrySet()) {\n            TopicPartition part = new TopicPartition(topic, entry.getKey());\n            // Advance queueSizesIndex so that we properly index available\n            // partitions.  Do it here so that it's done for all code paths.\n\n            Node leader = metadataSnapshot.cluster().leaderFor(part);\n            if (leader != null && queueSizes != null) {\n                ++queueSizesIndex;\n                assert queueSizesIndex < queueSizes.length;\n                partitionIds[queueSizesIndex] = part.partition();\n            }\n\n            Deque<ProducerBatch> deque = entry.getValue();\n\n            final long waitedTimeMs;\n            final boolean backingOff;\n            final int backoffAttempts;\n            final int dequeSize;\n            final boolean full;\n\n            OptionalInt leaderEpoch = metadataSnapshot.leaderEpochFor(part);\n\n            // This loop is especially hot with large partition counts. So -\n\n            // 1. We should avoid code that increases synchronization between application thread calling\n            // send(), and background thread running runOnce(), see https://issues.apache.org/jira/browse/KAFKA-16226\n\n            // 2. We are careful to only perform the minimum required inside the\n            // synchronized block, as this lock is also used to synchronize producer threads\n            // attempting to append() to a partition/batch.\n\n            synchronized (deque) {\n                // Deques are often empty in this path, esp with large partition counts,\n                // so we exit early if we can.\n                ProducerBatch batch = deque.peekFirst();\n                if (batch == null) {\n                    continue;\n                }\n\n                waitedTimeMs = batch.waitedTimeMs(nowMs);\n                batch.maybeUpdateLeaderEpoch(leaderEpoch);\n                backingOff = shouldBackoff(batch.hasLeaderChangedForTheOngoingRetry(), batch, waitedTimeMs);\n                backoffAttempts = batch.attempts();\n                dequeSize = deque.size();\n                full = dequeSize > 1 || batch.isFull();\n            }\n\n            if (leader == null) {\n                // This is a partition for which leader is not known, but messages are available to send.\n                // Note that entries are currently not removed from batches when deque is empty.\n                unknownLeaderTopics.add(part.topic());\n            } else {\n                if (queueSizes != null)\n                    queueSizes[queueSizesIndex] = dequeSize;\n                if (partitionAvailabilityTimeoutMs > 0) {\n                    // Check if we want to exclude the partition from the list of available partitions\n                    // if the broker hasn't responded for some time.\n                    NodeLatencyStats nodeLatencyStats = nodeStats.get(leader.id());\n                    if (nodeLatencyStats != null) {\n                        // NOTE: there is no synchronization between reading metrics,\n                        // so we read ready time first to avoid accidentally marking partition\n                        // unavailable if we read while the metrics are being updated.\n                        long readyTimeMs = nodeLatencyStats.readyTimeMs;\n                        if (readyTimeMs - nodeLatencyStats.drainTimeMs > partitionAvailabilityTimeoutMs)\n                            --queueSizesIndex;\n                    }\n                }\n\n                nextReadyCheckDelayMs = batchReady(exhausted, part, leader, waitedTimeMs, backingOff,\n                    backoffAttempts, full, nextReadyCheckDelayMs, readyNodes);\n            }\n        }\n\n        // We've collected the queue sizes for partitions of this topic, now we can calculate\n        // load stats.  NOTE: the stats are calculated in place, modifying the\n        // queueSizes array.\n        topicInfo.builtInPartitioner.updatePartitionLoadStats(queueSizes, partitionIds, queueSizesIndex + 1);\n        return nextReadyCheckDelayMs;\n    }\n\n    /**\n     * Get a list of nodes whose partitions are ready to be sent, and the earliest time at which any non-sendable\n     * partition will be ready; Also return the flag for whether there are any unknown leaders for the accumulated\n     * partition batches.\n     * <p>\n     * A destination node is ready to send data if:\n     * <ol>\n     * <li>There is at least one partition that is not backing off its send\n     * <li><b>and</b> those partitions are not muted (to prevent reordering if\n     *   {@value org.apache.kafka.clients.producer.ProducerConfig#MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION}\n     *   is set to one)</li>\n     * <li><b>and <i>any</i></b> of the following are true</li>\n     * <ul>\n     *     <li>The record set is full</li>\n     *     <li>The record set has sat in the accumulator for at least lingerMs milliseconds</li>\n     *     <li>The accumulator is out of memory and threads are blocking waiting for data (in this case all partitions\n     *     are immediately considered ready).</li>\n     *     <li>The accumulator has been closed</li>\n     * </ul>\n     * </ol>\n     */\n    public ReadyCheckResult ready(MetadataSnapshot metadataSnapshot, long nowMs) {\n        Set<Node> readyNodes = new HashSet<>();\n        long nextReadyCheckDelayMs = Long.MAX_VALUE;\n        Set<String> unknownLeaderTopics = new HashSet<>();\n        // Go topic by topic so that we can get queue sizes for partitions in a topic and calculate\n        // cumulative frequency table (used in partitioner).\n        for (Map.Entry<String, TopicInfo> topicInfoEntry : this.topicInfoMap.entrySet()) {\n            final String topic = topicInfoEntry.getKey();\n            nextReadyCheckDelayMs = partitionReady(metadataSnapshot, nowMs, topic, topicInfoEntry.getValue(), nextReadyCheckDelayMs, readyNodes, unknownLeaderTopics);\n        }\n        return new ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);\n    }\n\n    /**\n     * Check whether there are any batches which haven't been drained\n     */\n    public boolean hasUndrained() {\n        for (TopicInfo topicInfo : topicInfoMap.values()) {\n            for (Deque<ProducerBatch> deque : topicInfo.batches.values()) {\n                synchronized (deque) {\n                    if (!deque.isEmpty())\n                        return true;\n                }\n            }\n        }\n        return false;\n    }\n\n    private boolean shouldBackoff(boolean hasLeaderChanged, final ProducerBatch batch, final long waitedTimeMs) {\n        boolean shouldWaitMore = batch.attempts() > 0 && waitedTimeMs < retryBackoff.backoff(batch.attempts() - 1);\n        boolean shouldBackoff = !hasLeaderChanged && shouldWaitMore;\n        if (log.isTraceEnabled()) {\n            if (shouldBackoff) {",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java#L651-L800",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 651,
  "end_line": 800,
  "last_modified": "2026-02-06T01:16:27.610051",
  "source_type": "github"
}
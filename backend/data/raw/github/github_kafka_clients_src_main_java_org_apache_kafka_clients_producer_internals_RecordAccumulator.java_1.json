{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_RecordAccumulator.java_1",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.clients.producer.internals;\n\nimport org.apache.kafka.clients.CommonClientConfigs;\nimport org.apache.kafka.clients.MetadataSnapshot;\nimport org.apache.kafka.clients.producer.Callback;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport org.apache.kafka.common.Cluster;\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.Node;\nimport org.apache.kafka.common.PartitionInfo;\nimport org.apache.kafka.common.TopicPartition;\nimport org.apache.kafka.common.compress.Compression;\nimport org.apache.kafka.common.header.Header;\nimport org.apache.kafka.common.metrics.Metrics;\nimport org.apache.kafka.common.record.AbstractRecords;\nimport org.apache.kafka.common.record.CompressionRatioEstimator;\nimport org.apache.kafka.common.record.MemoryRecords;\nimport org.apache.kafka.common.record.MemoryRecordsBuilder;\nimport org.apache.kafka.common.record.Record;\nimport org.apache.kafka.common.record.RecordBatch;\nimport org.apache.kafka.common.record.TimestampType;\nimport org.apache.kafka.common.utils.CopyOnWriteMap;\nimport org.apache.kafka.common.utils.ExponentialBackoff;\nimport org.apache.kafka.common.utils.LogContext;\nimport org.apache.kafka.common.utils.ProducerIdAndEpoch;\nimport org.apache.kafka.common.utils.Time;\n\nimport org.slf4j.Logger;\n\nimport java.nio.ByteBuffer;\nimport java.util.ArrayDeque;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Deque;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.OptionalInt;\nimport java.util.Set;\nimport java.util.concurrent.ConcurrentMap;\nimport java.util.concurrent.atomic.AtomicInteger;\n\n/**\n * This class acts as a queue that accumulates records into {@link MemoryRecords}\n * instances to be sent to the server.\n * <p>\n * The accumulator uses a bounded amount of memory and append calls will block when that memory is exhausted, unless\n * this behavior is explicitly disabled.\n */\npublic class RecordAccumulator {\n\n    private final LogContext logContext;\n    private final Logger log;\n    private volatile boolean closed;\n    private final AtomicInteger flushesInProgress;\n    private final AtomicInteger appendsInProgress;\n    private final int batchSize;\n    private final Compression compression;\n    private final int lingerMs;\n    private final ExponentialBackoff retryBackoff;\n    private final int deliveryTimeoutMs;\n    private final long partitionAvailabilityTimeoutMs;  // latency threshold for marking partition temporary unavailable\n    private final boolean enableAdaptivePartitioning;\n    private final BufferPool free;\n    private final Time time;\n    private final ConcurrentMap<String /*topic*/, TopicInfo> topicInfoMap = new CopyOnWriteMap<>();\n    private final ConcurrentMap<Integer /*nodeId*/, NodeLatencyStats> nodeStats = new CopyOnWriteMap<>();\n    private final IncompleteBatches incomplete;\n    // The following variables are only accessed by the sender thread, so we don't need to protect them.\n    private final Set<TopicPartition> muted;\n    private final Map<String, Integer> nodesDrainIndex;\n    private final TransactionManager transactionManager;\n    private long nextBatchExpiryTimeMs = Long.MAX_VALUE; // the earliest time (absolute) a batch will expire.\n\n    /**\n     * Create a new record accumulator\n     *\n     * @param logContext The log context used for logging\n     * @param batchSize The size to use when allocating {@link MemoryRecords} instances\n     * @param compression The compression codec for the records\n     * @param lingerMs An artificial delay time to add before declaring a records instance that isn't full ready for\n     *        sending. This allows time for more records to arrive. Setting a non-zero lingerMs will trade off some\n     *        latency for potentially better throughput due to more batching (and hence fewer, larger requests).\n     * @param retryBackoffMs An artificial delay time to retry the produce request upon receiving an error. This avoids\n     *        exhausting all retries in a short period of time.\n     * @param retryBackoffMaxMs The upper bound of the retry backoff time.\n     * @param deliveryTimeoutMs An upper bound on the time to report success or failure on record delivery\n     * @param partitionerConfig Partitioner config\n     * @param metrics The metrics\n     * @param metricGrpName The metric group name\n     * @param time The time instance to use\n     * @param transactionManager The shared transaction state object which tracks producer IDs, epochs, and sequence\n     *                           numbers per partition.\n     * @param bufferPool The buffer pool\n     */\n    public RecordAccumulator(LogContext logContext,\n                             int batchSize,\n                             Compression compression,\n                             int lingerMs,\n                             long retryBackoffMs,\n                             long retryBackoffMaxMs,\n                             int deliveryTimeoutMs,\n                             PartitionerConfig partitionerConfig,\n                             Metrics metrics,\n                             String metricGrpName,\n                             Time time,\n                             TransactionManager transactionManager,\n                             BufferPool bufferPool) {\n        this.logContext = logContext;\n        this.log = logContext.logger(RecordAccumulator.class);\n        this.closed = false;\n        this.flushesInProgress = new AtomicInteger(0);\n        this.appendsInProgress = new AtomicInteger(0);\n        this.batchSize = batchSize;\n        this.compression = compression;\n        this.lingerMs = lingerMs;\n        this.retryBackoff = new ExponentialBackoff(retryBackoffMs,\n                CommonClientConfigs.RETRY_BACKOFF_EXP_BASE,\n                retryBackoffMaxMs,\n                CommonClientConfigs.RETRY_BACKOFF_JITTER);\n        this.deliveryTimeoutMs = deliveryTimeoutMs;\n        this.enableAdaptivePartitioning = partitionerConfig.enableAdaptivePartitioning;\n        this.partitionAvailabilityTimeoutMs = partitionerConfig.partitionAvailabilityTimeoutMs;\n        this.free = bufferPool;\n        this.incomplete = new IncompleteBatches();\n        this.muted = new HashSet<>();\n        this.time = time;\n        nodesDrainIndex = new HashMap<>();\n        this.transactionManager = transactionManager;\n        registerMetrics(metrics, metricGrpName);\n    }\n",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java#L1-L150",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 1,
  "end_line": 150,
  "last_modified": "2026-02-06T01:16:27.610051",
  "source_type": "github"
}
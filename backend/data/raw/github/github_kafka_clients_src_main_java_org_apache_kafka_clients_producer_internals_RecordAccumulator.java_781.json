{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_RecordAccumulator.java_781",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "content": "    /**\n     * Check whether there are any batches which haven't been drained\n     */\n    public boolean hasUndrained() {\n        for (TopicInfo topicInfo : topicInfoMap.values()) {\n            for (Deque<ProducerBatch> deque : topicInfo.batches.values()) {\n                synchronized (deque) {\n                    if (!deque.isEmpty())\n                        return true;\n                }\n            }\n        }\n        return false;\n    }\n\n    private boolean shouldBackoff(boolean hasLeaderChanged, final ProducerBatch batch, final long waitedTimeMs) {\n        boolean shouldWaitMore = batch.attempts() > 0 && waitedTimeMs < retryBackoff.backoff(batch.attempts() - 1);\n        boolean shouldBackoff = !hasLeaderChanged && shouldWaitMore;\n        if (log.isTraceEnabled()) {\n            if (shouldBackoff) {\n                log.trace(\n                    \"For {}, will backoff\", batch);\n            } else {\n                log.trace(\n                    \"For {}, will not backoff, shouldWaitMore {}, hasLeaderChanged {}\", batch,\n                    shouldWaitMore, hasLeaderChanged);\n            }\n        } else if (log.isDebugEnabled() && hasLeaderChanged) {\n            // Add less-verbose log at DEBUG.\n            log.debug(\"For {}, leader has changed, hence skipping backoff.\", batch);\n        }\n        return shouldBackoff;\n    }\n\n    private boolean shouldStopDrainBatchesForPartition(ProducerBatch first, TopicPartition tp) {\n        ProducerIdAndEpoch producerIdAndEpoch;\n        if (transactionManager != null) {\n            if (!transactionManager.isSendToPartitionAllowed(tp))\n                return true;\n\n            producerIdAndEpoch = transactionManager.producerIdAndEpoch();\n            if (!producerIdAndEpoch.isValid())\n                // we cannot send the batch until we have refreshed the producer id\n                return true;\n\n            if (!first.hasSequence()) {\n                if (transactionManager.hasInflightBatches(tp) && transactionManager.hasStaleProducerIdAndEpoch(tp)) {\n                    // Don't drain any new batches while the partition has in-flight batches with a different epoch\n                    // and/or producer ID. Otherwise, a batch with a new epoch and sequence number\n                    // 0 could be written before earlier batches complete, which would cause out of sequence errors\n                    return true;\n                }\n\n                if (transactionManager.hasUnresolvedSequence(first.topicPartition))\n                    // Don't drain any new batches while the state of previous sequence numbers\n                    // is unknown. The previous batches would be unknown if they were aborted\n                    // on the client after being sent to the broker at least once.\n                    return true;\n            }\n\n            int firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);\n            // If the queued batch already has an assigned sequence, then it is being retried.\n            // In this case, we wait until the next immediate batch is ready and drain that.\n            // We only move on when the next in line batch is complete (either successfully or due to\n            // a fatal broker error). This effectively reduces our in flight request count to 1.\n            return firstInFlightSequence != RecordBatch.NO_SEQUENCE && first.hasSequence()\n                    && first.baseSequence() != firstInFlightSequence;\n        }\n        return false;\n    }\n\n    private List<ProducerBatch> drainBatchesForOneNode(MetadataSnapshot metadataSnapshot, Node node, int maxSize, long now) {\n        int size = 0;\n        List<PartitionInfo> parts = metadataSnapshot.cluster().partitionsForNode(node.id());\n        List<ProducerBatch> ready = new ArrayList<>();\n        if (parts.isEmpty())\n            return ready;\n        /* to make starvation less likely each node has it's own drainIndex */\n        int drainIndex = getDrainIndex(node.idString());\n        int start = drainIndex = drainIndex % parts.size();\n        do {\n            PartitionInfo part = parts.get(drainIndex);\n\n            TopicPartition tp = new TopicPartition(part.topic(), part.partition());\n            updateDrainIndex(node.idString(), drainIndex);\n            drainIndex = (drainIndex + 1) % parts.size();\n            // Only proceed if the partition has no in-flight batches.\n            if (isMuted(tp))\n                continue;\n            Deque<ProducerBatch> deque = getDeque(tp);\n            if (deque == null)\n                continue;\n\n            OptionalInt leaderEpoch = metadataSnapshot.leaderEpochFor(tp);\n\n            final ProducerBatch batch;\n            synchronized (deque) {\n                // invariant: !isMuted(tp,now) && deque != null\n                ProducerBatch first = deque.peekFirst();\n                if (first == null)\n                    continue;\n\n                // first != null\n                // Only drain the batch if it is not during backoff period.\n                first.maybeUpdateLeaderEpoch(leaderEpoch);\n                if (shouldBackoff(first.hasLeaderChangedForTheOngoingRetry(), first, first.waitedTimeMs(now)))\n                    continue;\n\n                if (size + first.estimatedSizeInBytes() > maxSize && !ready.isEmpty()) {\n                    // there is a rare case that a single batch size is larger than the request size due to\n                    // compression; in this case we will still eventually send this batch in a single request\n                    break;\n                } else {\n                    if (shouldStopDrainBatchesForPartition(first, tp))\n                        break;\n                }\n\n                batch = deque.pollFirst();\n\n                boolean isTransactional = transactionManager != null && transactionManager.isTransactional();\n                ProducerIdAndEpoch producerIdAndEpoch =\n                    transactionManager != null ? transactionManager.producerIdAndEpoch() : null;\n                if (producerIdAndEpoch != null && !batch.hasSequence()) {\n                    // If the producer id/epoch of the partition do not match the latest one\n                    // of the producer, we update it and reset the sequence. This should be\n                    // only done when all its in-flight batches have completed. This is guarantee\n                    // in `shouldStopDrainBatchesForPartition`.\n                    transactionManager.maybeUpdateProducerIdAndEpoch(batch.topicPartition);\n\n                    // If the batch already has an assigned sequence, then we should not change the producer id and\n                    // sequence number, since this may introduce duplicates. In particular, the previous attempt\n                    // may actually have been accepted, and if we change the producer id and sequence here, this\n                    // attempt will also be accepted, causing a duplicate.\n                    //\n                    // Additionally, we update the next sequence number bound for the partition, and also have\n                    // the transaction manager track the batch so as to ensure that sequence ordering is maintained\n                    // even if we receive out of order responses.\n                    batch.setProducerState(producerIdAndEpoch, transactionManager.sequenceNumber(batch.topicPartition), isTransactional);\n                    transactionManager.incrementSequenceNumber(batch.topicPartition, batch.recordCount);\n                    log.debug(\"Assigned producerId {} and producerEpoch {} to batch with base sequence \" +\n                            \"{} being sent to partition {}\", producerIdAndEpoch.producerId,\n                        producerIdAndEpoch.epoch, batch.baseSequence(), tp);\n\n                    transactionManager.addInFlightBatch(batch);\n                }\n            }\n\n            // the rest of the work by processing outside the lock\n            // close() is particularly expensive\n            batch.close();",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java#L781-L930",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 781,
  "end_line": 930,
  "last_modified": "2026-02-06T01:16:27.610051",
  "source_type": "github"
}
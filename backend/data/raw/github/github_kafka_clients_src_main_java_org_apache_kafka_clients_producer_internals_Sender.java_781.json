{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_Sender.java_781",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java",
  "content": "\n        if (response.recordErrors == null || response.recordErrors.isEmpty()) {\n            failBatch(batch, topLevelException, adjustSequenceNumbers, deallocateBatch);\n        } else {\n            Map<Integer, RuntimeException> recordErrorMap = new HashMap<>(response.recordErrors.size());\n            for (ProduceResponse.RecordError recordError : response.recordErrors) {\n                // The API leaves us with some awkwardness interpreting the errors in the response.\n                // We cannot differentiate between different error cases (such as INVALID_TIMESTAMP)\n                // from the single error code at the partition level, so instead we use INVALID_RECORD\n                // for all failed records and rely on the message to distinguish the cases.\n                final String errorMessage;\n                if (recordError.message != null) {\n                    errorMessage = recordError.message;\n                } else if (response.errorMessage != null) {\n                    errorMessage = response.errorMessage;\n                } else {\n                    errorMessage = response.error.message();\n                }\n\n                // If the batch contained only a single record error, then we can unambiguously\n                // use the exception type corresponding to the partition-level error code.\n                if (response.recordErrors.size() == 1) {\n                    recordErrorMap.put(recordError.batchIndex, response.error.exception(errorMessage));\n                } else {\n                    recordErrorMap.put(recordError.batchIndex, new InvalidRecordException(errorMessage));\n                }\n            }\n\n            Function<Integer, RuntimeException> recordExceptions = batchIndex -> {\n                RuntimeException exception = recordErrorMap.get(batchIndex);\n                if (exception != null) {\n                    return exception;\n                } else {\n                    // If the response contains record errors, then the records which failed validation\n                    // will be present in the response. To avoid confusion for the remaining records, we\n                    // return a generic exception.\n                    return new KafkaException(\"Failed to append record because it was part of a batch \" +\n                        \"which had one more more invalid records\");\n                }\n            };\n\n            failBatch(batch, topLevelException, recordExceptions, adjustSequenceNumbers, deallocateBatch);\n        }\n    }\n\n    private void failBatch(\n        ProducerBatch batch,\n        RuntimeException topLevelException,\n        boolean adjustSequenceNumbers,\n        boolean deallocateBatch\n    ) {\n        failBatch(batch, topLevelException, batchIndex -> topLevelException, adjustSequenceNumbers, deallocateBatch);\n    }\n\n    private void failBatch(\n        ProducerBatch batch,\n        RuntimeException topLevelException,\n        Function<Integer, RuntimeException> recordExceptions,\n        boolean adjustSequenceNumbers,\n        boolean deallocateBatch\n    ) {\n        this.sensors.recordErrors(batch.topicPartition.topic(), batch.recordCount);\n\n        if (batch.completeExceptionally(topLevelException, recordExceptions)) {\n            if (transactionManager != null) {\n                try {\n                    // This call can throw an exception in the rare case that there's an invalid state transition\n                    // attempted. Catch these so as not to interfere with the rest of the logic.\n                    transactionManager.handleFailedBatch(batch, topLevelException, adjustSequenceNumbers);\n                } catch (Exception e) {\n                    log.debug(\"Encountered error when transaction manager was handling a failed batch\", e);\n                }\n            }\n            if (deallocateBatch) {\n                maybeRemoveAndDeallocateBatch(batch);\n            } else {\n                // Fix for KAFKA-19012\n                // The pooled ByteBuffer associated with this batch might still be in use by the network client so we\n                // cannot allow it to be reused yet. We skip deallocating it now. When the request in the network client \n                // completes with a response, either completeBatch() or failBatch() will be called with deallocateBatch=true.\n                // The buffer associated with the batch will be deallocated then.\n                maybeRemoveAndDeallocateBatchLater(batch);\n            }\n        } else {\n            if (deallocateBatch) {\n                this.accumulator.deallocate(batch);\n            }\n        }\n    }\n\n    /**\n     * We can retry a send if the error is transient and the number of attempts taken is fewer than the maximum allowed.\n     * We can also retry OutOfOrderSequence exceptions for future batches, since if the first batch has failed, the\n     * future batches are certain to fail with an OutOfOrderSequence exception.\n     */\n    private boolean canRetry(ProducerBatch batch, ProduceResponse.PartitionResponse response, long now) {\n        return !batch.hasReachedDeliveryTimeout(accumulator.getDeliveryTimeoutMs(), now) &&\n            batch.attempts() < this.retries &&\n            !batch.isDone() &&\n            (transactionManager == null ?\n                    response.error.exception() instanceof RetriableException :\n                    transactionManager.canRetry(response, batch));\n    }\n\n    /**\n     * Transfer the record batches into a list of produce requests on a per-node basis\n     */\n    private void sendProduceRequests(Map<Integer, List<ProducerBatch>> collated, long now) {\n        for (Map.Entry<Integer, List<ProducerBatch>> entry : collated.entrySet())\n            sendProduceRequest(now, entry.getKey(), acks, requestTimeoutMs, entry.getValue());\n    }\n\n    /**\n     * Create a produce request from the given record batches\n     */\n    private void sendProduceRequest(long now, int destination, short acks, int timeout, List<ProducerBatch> batches) {\n        if (batches.isEmpty())\n            return;\n\n        final Map<TopicPartition, ProducerBatch> recordsByPartition = new HashMap<>(batches.size());\n        Map<String, Uuid> topicIds = topicIdsForBatches(batches);\n\n        ProduceRequestData.TopicProduceDataCollection tpd = new ProduceRequestData.TopicProduceDataCollection();\n        for (ProducerBatch batch : batches) {\n            TopicPartition tp = batch.topicPartition;\n            MemoryRecords records = batch.records();\n            Uuid topicId = topicIds.get(tp.topic());\n            ProduceRequestData.TopicProduceData tpData = tpd.find(tp.topic(), topicId);\n\n            if (tpData == null) {\n                tpData = new ProduceRequestData.TopicProduceData()\n                        .setTopicId(topicId).setName(tp.topic());\n                tpd.add(tpData);\n            }\n\n            tpData.partitionData().add(new ProduceRequestData.PartitionProduceData()\n                    .setIndex(tp.partition())\n                    .setRecords(records));\n            recordsByPartition.put(tp, batch);\n            batch.setInflight(true);\n        }\n\n        String transactionalId = null;\n        boolean useTransactionV1Version = false;\n        if (transactionManager != null && transactionManager.isTransactional()) {\n            transactionalId = transactionManager.transactionalId();\n            useTransactionV1Version = !transactionManager.isTransactionV2Enabled();\n        }\n\n        ProduceRequest.Builder requestBuilder = ProduceRequest.builder(",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java#L781-L930",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 781,
  "end_line": 930,
  "last_modified": "2026-02-06T01:16:27.610196",
  "source_type": "github"
}
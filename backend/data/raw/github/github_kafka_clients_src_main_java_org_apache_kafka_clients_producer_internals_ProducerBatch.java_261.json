{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_ProducerBatch.java_261",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java",
  "content": "     */\n    private boolean done(\n        long baseOffset,\n        long logAppendTime,\n        RuntimeException topLevelException,\n        Function<Integer, RuntimeException> recordExceptions\n    ) {\n        final FinalState tryFinalState = (topLevelException == null) ? FinalState.SUCCEEDED : FinalState.FAILED;\n        if (tryFinalState == FinalState.SUCCEEDED) {\n            log.trace(\"Successfully produced messages to {} with base offset {}.\", topicPartition, baseOffset);\n        } else {\n            log.trace(\"Failed to produce messages to {} with base offset {}.\", topicPartition, baseOffset, topLevelException);\n        }\n\n        if (this.finalState.compareAndSet(null, tryFinalState)) {\n            completeFutureAndFireCallbacks(baseOffset, logAppendTime, recordExceptions);\n            return true;\n        }\n\n        if (this.finalState.get() != FinalState.SUCCEEDED) {\n            if (tryFinalState == FinalState.SUCCEEDED) {\n                // Log if a previously unsuccessful batch succeeded later on.\n                log.debug(\"ProduceResponse returned {} for {} after batch with base offset {} had already been {}.\",\n                    tryFinalState, topicPartition, baseOffset, this.finalState.get());\n            } else {\n                // FAILED --> FAILED and ABORTED --> FAILED transitions are ignored.\n                log.debug(\"Ignored state transition {} -> {} for {} batch with base offset {}\",\n                    this.finalState.get(), tryFinalState, topicPartition, baseOffset);\n            }\n        } else {\n            // A SUCCESSFUL batch must not attempt another state change.\n            throw new IllegalStateException(\"A \" + this.finalState.get() + \" batch must not attempt another state change to \" + tryFinalState);\n        }\n        return false;\n    }\n\n    private void completeFutureAndFireCallbacks(\n        long baseOffset,\n        long logAppendTime,\n        Function<Integer, RuntimeException> recordExceptions\n    ) {\n        // Set the future before invoking the callbacks as we rely on its state for the `onCompletion` call\n        produceFuture.set(baseOffset, logAppendTime, recordExceptions);\n\n        // execute callbacks\n        for (int i = 0; i < thunks.size(); i++) {\n            try {\n                Thunk thunk = thunks.get(i);\n                if (thunk.callback != null) {\n                    if (recordExceptions == null) {\n                        RecordMetadata metadata = thunk.future.value();\n                        thunk.callback.onCompletion(metadata, null);\n                    } else {\n                        RuntimeException exception = recordExceptions.apply(i);\n                        thunk.callback.onCompletion(null, exception);\n                    }\n                }\n            } catch (Exception e) {\n                log.error(\"Error executing user-provided callback on message for topic-partition '{}'\", topicPartition, e);\n            }\n        }\n\n        produceFuture.done();\n    }\n\n    public Deque<ProducerBatch> split(int splitBatchSize) {\n        RecordBatch recordBatch = validateAndGetRecordBatch();\n        Deque<ProducerBatch> batches = splitRecordsIntoBatches(recordBatch, splitBatchSize);\n        finalizeSplitBatches(batches);\n        return batches;\n    }\n\n    private RecordBatch validateAndGetRecordBatch() {\n        MemoryRecords memoryRecords = recordsBuilder.build();\n        Iterator<MutableRecordBatch> recordBatchIter = memoryRecords.batches().iterator();\n\n        if (!recordBatchIter.hasNext())\n            throw new IllegalStateException(\"Cannot split an empty producer batch.\");\n\n        RecordBatch recordBatch = recordBatchIter.next();\n        if (recordBatch.magic() < MAGIC_VALUE_V2 && !recordBatch.isCompressed())\n            throw new IllegalArgumentException(\"Batch splitting cannot be used with non-compressed messages \" +\n                    \"with version v0 and v1\");\n\n        if (recordBatchIter.hasNext())\n            throw new IllegalArgumentException(\"A producer batch should only have one record batch.\");\n\n        return recordBatch;\n    }\n\n    private Deque<ProducerBatch> splitRecordsIntoBatches(RecordBatch recordBatch, int splitBatchSize) {\n        Deque<ProducerBatch> batches = new ArrayDeque<>();\n        Iterator<Thunk> thunkIter = thunks.iterator();\n        // We always allocate batch size because we are already splitting a big batch.\n        // And we also Retain the create time of the original batch.\n        ProducerBatch batch = null;\n\n        for (Record record : recordBatch) {\n            assert thunkIter.hasNext();\n            Thunk thunk = thunkIter.next();\n            if (batch == null)\n                batch = createBatchOffAccumulatorForRecord(record, splitBatchSize);\n\n            // A newly created batch can always host the first message.\n            if (!batch.tryAppendForSplit(record.timestamp(), record.key(), record.value(), record.headers(), thunk)) {\n                batches.add(batch);\n                batch.closeForRecordAppends();\n                batch = createBatchOffAccumulatorForRecord(record, splitBatchSize);\n                batch.tryAppendForSplit(record.timestamp(), record.key(), record.value(), record.headers(), thunk);\n            }\n        }\n\n        // Close the last batch and add it to the batch list after split.\n        if (batch != null) {\n            batches.add(batch);\n            batch.closeForRecordAppends();\n        }\n\n        return batches;\n    }\n\n    private void finalizeSplitBatches(Deque<ProducerBatch> batches) {\n        // Chain all split batch ProduceRequestResults to the original batch's produceFuture\n        // Ensures the original batch's future doesn't complete until all split batches complete\n        for (ProducerBatch splitBatch : batches) {\n            produceFuture.addDependent(splitBatch.produceFuture);\n        }\n\n        produceFuture.set(ProduceResponse.INVALID_OFFSET, NO_TIMESTAMP, index -> new RecordBatchTooLargeException());\n        produceFuture.done();\n\n        assignProducerStateToBatches(batches);\n    }\n\n    private void assignProducerStateToBatches(Deque<ProducerBatch> batches) {\n        if (hasSequence()) {\n            int sequence = baseSequence();\n            ProducerIdAndEpoch producerIdAndEpoch = new ProducerIdAndEpoch(producerId(), producerEpoch());\n            for (ProducerBatch newBatch : batches) {\n                newBatch.setProducerState(producerIdAndEpoch, sequence, isTransactional());\n                sequence += newBatch.recordCount;\n            }\n        }\n    }\n\n    private ProducerBatch createBatchOffAccumulatorForRecord(Record record, int batchSize) {\n        int initialSize = Math.max(AbstractRecords.estimateSizeInBytesUpperBound(magic(),\n                recordsBuilder.compression().type(), record.key(), record.value(), record.headers()), batchSize);\n        ByteBuffer buffer = ByteBuffer.allocate(initialSize);\n",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java#L261-L410",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 261,
  "end_line": 410,
  "last_modified": "2026-02-06T01:16:27.609686",
  "source_type": "github"
}
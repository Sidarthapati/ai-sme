{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_RecordAccumulator.java_521",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "content": "        }\n        Deque<ProducerBatch> dq = bigBatch.split(targetSplitBatchSize);\n        int numSplitBatches = dq.size();\n        Deque<ProducerBatch> partitionDequeue = getOrCreateDeque(bigBatch.topicPartition);\n        while (!dq.isEmpty()) {\n            ProducerBatch batch = dq.pollLast();\n            incomplete.add(batch);\n            // We treat the newly split batches as if they are not even tried.\n            synchronized (partitionDequeue) {\n                if (transactionManager != null) {\n                    // We should track the newly created batches since they already have assigned sequences.\n                    transactionManager.addInFlightBatch(batch);\n                    insertInSequenceOrder(partitionDequeue, batch);\n                } else {\n                    partitionDequeue.addFirst(batch);\n                }\n            }\n        }\n        return numSplitBatches;\n    }\n\n    // We will have to do extra work to ensure the queue is in order when requests are being retried and there are\n    // multiple requests in flight to that partition. If the first in flight request fails to append, then all the\n    // subsequent in flight requests will also fail because the sequence numbers will not be accepted.\n    //\n    // Further, once batches are being retried, we are reduced to a single in flight request for that partition. So when\n    // the subsequent batches come back in sequence order, they will have to be placed further back in the queue.\n    //\n    // Note that this assumes that all the batches in the queue which have an assigned sequence also have the current\n    // producer id. We will not attempt to reorder messages if the producer id has changed, we will throw an\n    // IllegalStateException instead.\n    private void insertInSequenceOrder(Deque<ProducerBatch> deque, ProducerBatch batch) {\n        // When we are re-enqueueing and have enabled idempotence, the re-enqueued batch must always have a sequence.\n        if (batch.baseSequence() == RecordBatch.NO_SEQUENCE)\n            throw new IllegalStateException(\"Trying to re-enqueue a batch which doesn't have a sequence even \" +\n                \"though idempotency is enabled.\");\n\n        if (!transactionManager.hasInflightBatches(batch.topicPartition))\n            throw new IllegalStateException(\"We are re-enqueueing a batch which is not tracked as part of the in flight \" +\n                \"requests. batch.topicPartition: \" + batch.topicPartition + \"; batch.baseSequence: \" + batch.baseSequence());\n\n        ProducerBatch firstBatchInQueue = deque.peekFirst();\n        if (firstBatchInQueue != null && firstBatchInQueue.hasSequence() && firstBatchInQueue.baseSequence() < batch.baseSequence()) {\n            // The incoming batch can't be inserted at the front of the queue without violating the sequence ordering.\n            // This means that the incoming batch should be placed somewhere further back.\n            // We need to find the right place for the incoming batch and insert it there.\n            // We will only enter this branch if we have multiple inflights sent to different brokers and we need to retry\n            // the inflight batches.\n            //\n            // Since we reenqueue exactly one batch a time and ensure that the queue is ordered by sequence always, it\n            // is a simple linear scan of a subset of the in flight batches to find the right place in the queue each time.\n            List<ProducerBatch> orderedBatches = new ArrayList<>();\n            while (deque.peekFirst() != null && deque.peekFirst().hasSequence() && deque.peekFirst().baseSequence() < batch.baseSequence())\n                orderedBatches.add(deque.pollFirst());\n\n            log.debug(\"Reordered incoming batch with sequence {} for partition {}. It was placed in the queue at \" +\n                \"position {}\", batch.baseSequence(), batch.topicPartition, orderedBatches.size());\n            // Either we have reached a point where there are batches without a sequence (ie. never been drained\n            // and are hence in order by default), or the batch at the front of the queue has a sequence greater\n            // than the incoming batch. This is the right place to add the incoming batch.\n            deque.addFirst(batch);\n\n            // Now we have to re insert the previously queued batches in the right order.\n            for (int i = orderedBatches.size() - 1; i >= 0; --i) {\n                deque.addFirst(orderedBatches.get(i));\n            }\n\n            // At this point, the incoming batch has been queued in the correct place according to its sequence.\n        } else {\n            deque.addFirst(batch);\n        }\n    }\n\n    /**\n     * Add the leader to the ready nodes if the batch is ready\n     *\n     * @param exhausted 'true' is the buffer pool is exhausted\n     * @param part The partition\n     * @param leader The leader for the partition\n     * @param waitedTimeMs How long batch waited\n     * @param backingOff Is backing off\n     * @param backoffAttempts Number of attempts for calculating backoff delay\n     * @param full Is batch full\n     * @param nextReadyCheckDelayMs The delay for next check\n     * @param readyNodes The set of ready nodes (to be filled in)\n     * @return The delay for next check\n     */\n    private long batchReady(boolean exhausted, TopicPartition part, Node leader,\n                            long waitedTimeMs, boolean backingOff, int backoffAttempts,\n                            boolean full, long nextReadyCheckDelayMs, Set<Node> readyNodes) {\n        if (!readyNodes.contains(leader) && !isMuted(part)) {\n            long timeToWaitMs = backingOff ? retryBackoff.backoff(backoffAttempts > 0 ? backoffAttempts - 1 : 0) : lingerMs;\n            boolean expired = waitedTimeMs >= timeToWaitMs;\n            boolean transactionCompleting = transactionManager != null && transactionManager.isCompleting();\n            boolean sendable = full\n                    || expired\n                    || exhausted\n                    || closed\n                    || flushInProgress()\n                    || transactionCompleting;\n            if (sendable && !backingOff) {\n                readyNodes.add(leader);\n            } else {\n                long timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, 0);\n                // Note that this results in a conservative estimate since an un-sendable partition may have\n                // a leader that will later be found to have sendable data. However, this is good enough\n                // since we'll just wake up and then sleep again for the remaining time.\n                nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);\n            }\n        }\n        return nextReadyCheckDelayMs;\n    }\n\n    /**\n     * Iterate over partitions to see which one have batches ready and collect leaders of those\n     * partitions into the set of ready nodes.  If partition has no leader, add the topic to the set\n     * of topics with no leader.  This function also calculates stats for adaptive partitioning.\n     *\n     * @param metadataSnapshot      The cluster metadata\n     * @param nowMs                 The current time\n     * @param topic                 The topic\n     * @param topicInfo             The topic info\n     * @param nextReadyCheckDelayMs The delay for next check\n     * @param readyNodes            The set of ready nodes (to be filled in)\n     * @param unknownLeaderTopics   The set of topics with no leader (to be filled in)\n     * @return The delay for next check\n     */\n    private long partitionReady(MetadataSnapshot metadataSnapshot, long nowMs, String topic,\n                                TopicInfo topicInfo,\n                                long nextReadyCheckDelayMs, Set<Node> readyNodes, Set<String> unknownLeaderTopics) {\n        ConcurrentMap<Integer, Deque<ProducerBatch>> batches = topicInfo.batches;\n        // Collect the queue sizes for available partitions to be used in adaptive partitioning.\n        int[] queueSizes = null;\n        int[] partitionIds = null;\n        if (enableAdaptivePartitioning && batches.size() >= metadataSnapshot.cluster().partitionsForTopic(topic).size()) {\n            // We don't do adaptive partitioning until we scheduled at least a batch for all\n            // partitions (i.e. we have the corresponding entries in the batches map), we just\n            // do uniform.  The reason is that we build queue sizes from the batches map,\n            // and if an entry is missing in the batches map, then adaptive partitioning logic\n            // won't know about it and won't switch to it.\n            queueSizes = new int[batches.size()];\n            partitionIds = new int[queueSizes.length];\n        }\n\n        int queueSizesIndex = -1;\n        boolean exhausted = this.free.queued() > 0;\n        for (Map.Entry<Integer, Deque<ProducerBatch>> entry : batches.entrySet()) {\n            TopicPartition part = new TopicPartition(topic, entry.getKey());\n            // Advance queueSizesIndex so that we properly index available\n            // partitions.  Do it here so that it's done for all code paths.",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java#L521-L670",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 521,
  "end_line": 670,
  "last_modified": "2026-02-06T01:16:27.610051",
  "source_type": "github"
}
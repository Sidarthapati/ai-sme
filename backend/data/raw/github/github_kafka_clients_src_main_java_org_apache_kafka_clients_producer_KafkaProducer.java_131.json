{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_KafkaProducer.java_131",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "content": " * and immediately returns. This allows the producer to batch together individual records for efficiency.\n * <p>\n * The <code>acks</code> config controls the criteria under which requests are considered complete. The default setting \"all\"\n * will result in blocking on the full commit of the record, the slowest but most durable setting.\n * <p>\n * If the request fails, the producer can automatically retry. The <code>retries</code> setting defaults to <code>Integer.MAX_VALUE</code>, and\n * it's recommended to use <code>delivery.timeout.ms</code> to control retry behavior, instead of <code>retries</code>.\n * <p>\n * The producer maintains buffers of unsent records for each partition. These buffers are of a size specified by\n * the <code>batch.size</code> config. Making this larger can result in more batching, but requires more memory (since we will\n * generally have one of these buffers for each active partition).\n * <p>\n * By default a buffer is available to send immediately even if there is additional unused space in the buffer. However if you\n * want to reduce the number of requests you can set <code>linger.ms</code> to something greater than 0. This will\n * instruct the producer to wait up to that number of milliseconds before sending a request in hope that more records will\n * arrive to fill up the same batch. This is analogous to Nagle's algorithm in TCP. For example, in the code snippet above,\n * likely all 100 records would be sent in a single request since we set our linger time to 1 millisecond. However this setting\n * would add 1 millisecond of latency to our request waiting for more records to arrive if we didn't fill up the buffer. Note that\n * records that arrive close together in time will generally batch together even with <code>linger.ms=0</code>. So, under heavy load,\n * batching will occur regardless of the linger configuration; however setting this to something larger than 0 can lead to fewer, more\n * efficient requests when not under maximal load at the cost of a small amount of latency.\n * <p>\n * The <code>buffer.memory</code> controls the total amount of memory available to the producer for buffering. If records\n * are sent faster than they can be transmitted to the server then this buffer space will be exhausted. When the buffer space is\n * exhausted additional send calls will block. The threshold for time to block is determined by <code>max.block.ms</code> after which it returns\n * a failed future with BufferExhaustedException.\n * <p>\n * The <code>key.serializer</code> and <code>value.serializer</code> instruct how to turn the key and value objects the user provides with\n * their <code>ProducerRecord</code> into bytes. You can use the included {@link org.apache.kafka.common.serialization.ByteArraySerializer} or\n * {@link org.apache.kafka.common.serialization.StringSerializer} for simple byte or string types.\n * <p>\n * From Kafka 0.11, the KafkaProducer supports two additional modes: the idempotent producer and the transactional producer.\n * The idempotent producer strengthens Kafka's delivery semantics from at least once to exactly once delivery. In particular\n * producer retries will no longer introduce duplicates. The transactional producer allows an application to send messages\n * to multiple partitions (and topics!) atomically.\n * </p>\n * <p>\n * From Kafka 3.0, the <code>enable.idempotence</code> configuration defaults to true. When enabling idempotence,\n * <code>retries</code> config will default to <code>Integer.MAX_VALUE</code> and the <code>acks</code> config will\n * default to <code>all</code>. There are no API changes for the idempotent producer, so existing applications will\n * not need to be modified to take advantage of this feature.\n * </p>\n * <p>\n * To take advantage of the idempotent producer, it is imperative to avoid application level re-sends since these cannot\n * be de-duplicated. As such, if an application enables idempotence, it is recommended to leave the <code>retries</code>\n * config unset, as it will be defaulted to <code>Integer.MAX_VALUE</code>. Additionally, if a {@link #send(ProducerRecord)}\n * returns an error even with infinite retries (for instance if the message expires in the buffer before being sent),\n * then it is recommended to shut down the producer and check the contents of the last produced message to ensure that\n * it is not duplicated. Finally, the producer can only guarantee idempotence for messages sent within a single session.\n * </p>\n * <p>To use the transactional producer and the attendant APIs, you must set the <code>transactional.id</code>\n * configuration property. If the <code>transactional.id</code> is set, idempotence is automatically enabled along with\n * the producer configs which idempotence depends on. Further, topics which are included in transactions should be configured\n * for durability. In particular, the <code>replication.factor</code> should be at least <code>3</code>, and the\n * <code>min.insync.replicas</code> for these topics should be set to 2. Finally, in order for transactional guarantees\n * to be realized from end-to-end, the consumers must be configured to read only committed messages as well.\n * </p>\n * <p>\n * The purpose of the <code>transactional.id</code> is to enable transaction recovery across multiple sessions of a\n * single producer instance. It would typically be derived from the shard identifier in a partitioned, stateful, application.\n * As such, it should be unique to each producer instance running within a partitioned application.\n * </p>\n * <p>All the new transactional APIs are blocking and will throw exceptions on failure. The example\n * below illustrates how the new APIs are meant to be used. It is similar to the example above, except that all\n * 100 messages are part of a single transaction.\n * </p>\n * <p>\n * <pre>\n * {@code\n * Properties props = new Properties();\n * props.put(\"bootstrap.servers\", \"localhost:9092\");\n * props.put(\"transactional.id\", \"my-transactional-id\");\n * Producer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());\n *\n * producer.initTransactions();\n *\n * try {\n *     producer.beginTransaction();\n *     for (int i = 0; i < 100; i++)\n *         producer.send(new ProducerRecord<>(\"my-topic\", Integer.toString(i), Integer.toString(i)));\n *     producer.commitTransaction();\n * } catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) {\n *     // We can't recover from these exceptions, so our only option is to close the producer and exit.\n *     producer.close();\n * } catch (KafkaException e) {\n *     // For all other exceptions, just abort the transaction and try again.\n *     producer.abortTransaction();\n * }\n * producer.close();\n * } </pre>\n * </p>\n * <p>\n * As is hinted at in the example, there can be only one open transaction per producer. All messages sent between the\n * {@link #beginTransaction()} and {@link #commitTransaction()} calls will be part of a single transaction. When the\n * <code>transactional.id</code> is specified, all messages sent by the producer must be part of a transaction.\n * </p>\n * <p>\n * The transactional producer uses exceptions to communicate error states. In particular, it is not required\n * to specify callbacks for <code>producer.send()</code> or to call <code>.get()</code> on the returned Future: a\n * <code>KafkaException</code> would be thrown if any of the\n * <code>producer.send()</code> or transactional calls hit an irrecoverable error during a transaction. See the {@link #send(ProducerRecord)}\n * documentation for more details about detecting errors from a transactional send.\n * </p>\n * </p>By calling\n * <code>producer.abortTransaction()</code> upon receiving a <code>KafkaException</code> we can ensure that any\n * successful writes are marked as aborted, hence keeping the transactional guarantees.\n * </p>\n * <p>\n * This client can communicate with brokers that are version 0.10.0 or newer. Older or newer brokers may not support\n * certain client features.  For instance, the transactional APIs need broker versions 0.11.0 or later. You will receive an\n * <code>UnsupportedVersionException</code> when invoking an API that is not available in the running broker version.\n * </p>\n */\npublic class KafkaProducer<K, V> implements Producer<K, V> {\n\n    private final Logger log;\n    private static final String JMX_PREFIX = \"kafka.producer\";\n    public static final String NETWORK_THREAD_PREFIX = \"kafka-producer-network-thread\";\n    public static final String PRODUCER_METRIC_GROUP_NAME = \"producer-metrics\";\n\n    private static final String INIT_TXN_TIMEOUT_MSG = \"InitTransactions timed out - \" +\n            \"did not complete coordinator discovery or \" +\n            \"receive the InitProducerId response within max.block.ms.\";\n\n    private static final String SEND_OFFSETS_TIMEOUT_MSG =\n            \"SendOffsetsToTransaction timed out - did not reach the coordinator or \" +\n                    \"receive the TxnOffsetCommit/AddOffsetsToTxn response within max.block.ms\";\n    private static final String COMMIT_TXN_TIMEOUT_MSG =\n            \"CommitTransaction timed out - did not complete EndTxn with the transaction coordinator within max.block.ms\";\n    private static final String ABORT_TXN_TIMEOUT_MSG =\n            \"AbortTransaction timed out - did not complete EndTxn(abort) with the transaction coordinator within max.block.ms\";\n    \n    private final String clientId;\n    // Visible for testing\n    final Metrics metrics;\n    private final KafkaProducerMetrics producerMetrics;\n    private final Plugin<Partitioner> partitionerPlugin;\n    private final int maxRequestSize;\n    private final long totalMemorySize;\n    private final ProducerMetadata metadata;\n    private final RecordAccumulator accumulator;\n    private final Sender sender;\n    private final Sender.SenderThread ioThread;\n    private final Compression compression;\n    private final Sensor errors;\n    private final Time time;\n    private final Plugin<Serializer<K>> keySerializerPlugin;\n    private final Plugin<Serializer<V>> valueSerializerPlugin;\n    private final ProducerConfig producerConfig;\n    private final long maxBlockTimeMs;",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L131-L280",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 131,
  "end_line": 280,
  "last_modified": "2026-02-06T01:16:27.608270",
  "source_type": "github"
}
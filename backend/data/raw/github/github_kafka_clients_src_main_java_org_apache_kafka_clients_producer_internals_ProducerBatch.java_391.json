{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_ProducerBatch.java_391",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java",
  "content": "\n        assignProducerStateToBatches(batches);\n    }\n\n    private void assignProducerStateToBatches(Deque<ProducerBatch> batches) {\n        if (hasSequence()) {\n            int sequence = baseSequence();\n            ProducerIdAndEpoch producerIdAndEpoch = new ProducerIdAndEpoch(producerId(), producerEpoch());\n            for (ProducerBatch newBatch : batches) {\n                newBatch.setProducerState(producerIdAndEpoch, sequence, isTransactional());\n                sequence += newBatch.recordCount;\n            }\n        }\n    }\n\n    private ProducerBatch createBatchOffAccumulatorForRecord(Record record, int batchSize) {\n        int initialSize = Math.max(AbstractRecords.estimateSizeInBytesUpperBound(magic(),\n                recordsBuilder.compression().type(), record.key(), record.value(), record.headers()), batchSize);\n        ByteBuffer buffer = ByteBuffer.allocate(initialSize);\n\n        // Note that we intentionally do not set producer state (producerId, epoch, sequence, and isTransactional)\n        // for the newly created batch. This will be set when the batch is dequeued for sending (which is consistent\n        // with how normal batches are handled).\n        MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic(), recordsBuilder.compression(),\n                TimestampType.CREATE_TIME, 0L);\n        return new ProducerBatch(topicPartition, builder, this.createdMs, true);\n    }\n\n    public boolean isCompressed() {\n        return recordsBuilder.compression().type() != CompressionType.NONE;\n    }\n\n    /**\n     * A callback and the associated FutureRecordMetadata argument to pass to it.\n     */\n    private static final class Thunk {\n        final Callback callback;\n        final FutureRecordMetadata future;\n\n        Thunk(Callback callback, FutureRecordMetadata future) {\n            this.callback = callback;\n            this.future = future;\n        }\n    }\n\n    @Override\n    public String toString() {\n        return \"ProducerBatch(topicPartition=\" + topicPartition + \", recordCount=\" + recordCount + \")\";\n    }\n\n    boolean hasReachedDeliveryTimeout(long deliveryTimeoutMs, long now) {\n        return deliveryTimeoutMs <= now - this.createdMs;\n    }\n\n    public FinalState finalState() {\n        return this.finalState.get();\n    }\n\n    int attempts() {\n        return attempts.get();\n    }\n\n    void reenqueued(long now) {\n        attempts.getAndIncrement();\n        lastAttemptMs = Math.max(lastAppendTime, now);\n        lastAppendTime = Math.max(lastAppendTime, now);\n        retry = true;\n    }\n\n    long queueTimeMs() {\n        return drainedMs - createdMs;\n    }\n\n    long waitedTimeMs(long nowMs) {\n        return Math.max(0, nowMs - lastAttemptMs);\n    }\n\n    void drained(long nowMs) {\n        this.drainedMs = Math.max(drainedMs, nowMs);\n    }\n\n    boolean isSplitBatch() {\n        return isSplitBatch;\n    }\n\n    /**\n     * Returns if the batch is been retried for sending to kafka\n     */\n    public boolean inRetry() {\n        return this.retry;\n    }\n\n    public MemoryRecords records() {\n        return recordsBuilder.build();\n    }\n\n    public int estimatedSizeInBytes() {\n        return recordsBuilder.estimatedSizeInBytes();\n    }\n\n    public double compressionRatio() {\n        return recordsBuilder.compressionRatio();\n    }\n\n    public boolean isFull() {\n        return recordsBuilder.isFull();\n    }\n\n    public void setProducerState(ProducerIdAndEpoch producerIdAndEpoch, int baseSequence, boolean isTransactional) {\n        recordsBuilder.setProducerState(producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, baseSequence, isTransactional);\n    }\n\n    public void resetProducerState(ProducerIdAndEpoch producerIdAndEpoch, int baseSequence) {\n        log.info(\"Resetting sequence number of batch with current sequence {} for partition {} to {}\",\n                this.baseSequence(), this.topicPartition, baseSequence);\n        reopened = true;\n        recordsBuilder.reopenAndRewriteProducerState(producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, baseSequence, isTransactional());\n    }\n\n    /**\n     * Release resources required for record appends (e.g. compression buffers). Once this method is called, it's only\n     * possible to update the RecordBatch header.\n     */\n    public void closeForRecordAppends() {\n        recordsBuilder.closeForRecordAppends();\n    }\n\n    public void close() {\n        recordsBuilder.close();\n        if (!recordsBuilder.isControlBatch()) {\n            CompressionRatioEstimator.updateEstimation(topicPartition.topic(),\n                                                       recordsBuilder.compression().type(),\n                                                       (float) recordsBuilder.compressionRatio());\n        }\n        reopened = false;\n    }\n\n    /**\n     * Abort the record builder and reset the state of the underlying buffer. This is used prior to aborting\n     * the batch with {@link #abort(RuntimeException)} and ensures that no record previously appended can be\n     * read. This is used in scenarios where we want to ensure a batch ultimately gets aborted, but in which\n     * it is not safe to invoke the completion callbacks (e.g. because we are holding a lock, such as\n     * when aborting batches in {@link RecordAccumulator}).\n     */\n    public void abortRecordAppends() {\n        recordsBuilder.abort();\n    }\n\n    public boolean isClosed() {\n        return recordsBuilder.isClosed();",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java#L391-L540",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 391,
  "end_line": 540,
  "last_modified": "2026-02-06T01:16:27.609686",
  "source_type": "github"
}
{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_RecordAccumulator.java_1041",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "content": "        // Only deallocate the batch if it is not a split batch because split batch are allocated outside the\n        // buffer pool.\n        if (!batch.isSplitBatch()) {\n            if (batch.isBufferDeallocated()) {\n                log.warn(\"Skipping deallocating a batch that has already been deallocated. Batch is {}, created time is {}\", batch, batch.createdMs);\n            } else {\n                batch.markBufferDeallocated();\n                if (batch.isInflight()) {\n                    // Create a fresh ByteBuffer to give to BufferPool to reuse since we can't safely call deallocate with the ProduceBatch's buffer\n                    free.deallocate(ByteBuffer.allocate(batch.initialCapacity()));\n                    throw new IllegalStateException(\"Attempting to deallocate a batch that is inflight. Batch is \" + batch);\n                }\n                free.deallocate(batch.buffer(), batch.initialCapacity());\n            }\n        }\n    }\n\n    /**\n     * Remove from the incomplete list but do not free memory yet\n     */\n    public void completeBatch(ProducerBatch batch) {\n        incomplete.remove(batch);\n    }\n\n    /**\n     * Package private for unit test. Get the buffer pool remaining size in bytes.\n     */\n    long bufferPoolAvailableMemory() {\n        return free.availableMemory();\n    }\n\n    /**\n     * Are there any threads currently waiting on a flush?\n     *\n     * package private for test\n     */\n    boolean flushInProgress() {\n        return flushesInProgress.get() > 0;\n    }\n\n    /**\n     * Initiate the flushing of data from the accumulator...this makes all requests immediately ready\n     */\n    public void beginFlush() {\n        this.flushesInProgress.getAndIncrement();\n    }\n\n    /**\n     * Are there any threads currently appending messages?\n     */\n    private boolean appendsInProgress() {\n        return appendsInProgress.get() > 0;\n    }\n\n    /**\n     * Mark all partitions as ready to send and block until the send is complete\n     */\n    public void awaitFlushCompletion() throws InterruptedException {\n        try {\n            // Obtain a copy of all of the incomplete ProduceRequestResult(s) at the time of the flush.\n            // We must be careful not to hold a reference to the ProduceBatch(s) so that garbage\n            // collection can occur on the contents.\n            // The sender will remove ProducerBatch(s) from the original incomplete collection.\n            //\n            // We use awaitAllDependents() here instead of await() to ensure that if any batch\n            // was split into multiple batches, we wait for all the split batches to complete.\n            // This is required to guarantee that all records sent before flush()\n            // must be fully complete, including records in split batches.\n            for (ProduceRequestResult result : this.incomplete.requestResults())\n                result.awaitAllDependents();\n        } finally {\n            this.flushesInProgress.decrementAndGet();\n        }\n    }\n\n    /**\n     * Check whether there are any pending batches (whether sent or unsent).\n     */\n    public boolean hasIncomplete() {\n        return !this.incomplete.isEmpty();\n    }\n\n    /**\n     * This function is only called when sender is closed forcefully. It will fail all the\n     * incomplete batches and return.\n     */\n    public void abortIncompleteBatches() {\n        // We need to keep aborting the incomplete batch until no thread is trying to append to\n        // 1. Avoid losing batches.\n        // 2. Free up memory in case appending threads are blocked on buffer full.\n        // This is a tight loop but should be able to get through very quickly.\n        do {\n            abortBatches();\n        } while (appendsInProgress());\n        // After this point, no thread will append any messages because they will see the close\n        // flag set. We need to do the last abort after no thread was appending in case there was a new\n        // batch appended by the last appending thread.\n        abortBatches();\n        this.topicInfoMap.clear();\n    }\n\n    /**\n     * Go through incomplete batches and abort them.\n     */\n    private void abortBatches() {\n        abortBatches(new KafkaException(\"Producer is closed forcefully.\"));\n    }\n\n    /**\n     * Abort all incomplete batches (whether they have been sent or not)\n     */\n    void abortBatches(final RuntimeException reason) {\n        for (ProducerBatch batch : incomplete.copyAll()) {\n            Deque<ProducerBatch> dq = getDeque(batch.topicPartition);\n            synchronized (dq) {\n                batch.abortRecordAppends();\n                dq.remove(batch);\n            }\n            batch.abort(reason);\n            if (batch.isInflight()) {\n                // KAFKA-19012: if the batch has been sent it might still be in use by the network client so we cannot allow it to be reused yet.\n                // We skip deallocating it now. When the request in network client completes with a response, either Sender.completeBatch() or\n                // Sender.failBatch() will be called with deallocateBatch=true. The buffer associated with the batch will be deallocated then.\n                completeBatch(batch);\n            } else {\n                completeAndDeallocateBatch(batch);\n            }\n        }\n    }\n\n    /**\n     * Abort any batches which have not been drained\n     */\n    void abortUndrainedBatches(RuntimeException reason) {\n        for (ProducerBatch batch : incomplete.copyAll()) {\n            Deque<ProducerBatch> dq = getDeque(batch.topicPartition);\n            boolean aborted = false;\n            synchronized (dq) {\n                if ((transactionManager != null && !batch.hasSequence()) || (transactionManager == null && !batch.isClosed())) {\n                    aborted = true;\n                    batch.abortRecordAppends();\n                    dq.remove(batch);\n                }\n            }\n            if (aborted) {\n                batch.abort(reason);\n                completeAndDeallocateBatch(batch);\n            }\n        }\n    }",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java#L1041-L1190",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 1041,
  "end_line": 1190,
  "last_modified": "2026-02-06T01:16:27.610051",
  "source_type": "github"
}
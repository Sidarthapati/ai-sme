{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_KafkaProducer.java_1041",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "content": "     * </p>\n     * <p>\n     * Note that callbacks will generally execute in the I/O thread of the producer and so should be reasonably fast or\n     * they will delay the sending of messages from other threads. If you want to execute blocking or computationally\n     * expensive callbacks it is recommended to use your own {@link java.util.concurrent.Executor} in the callback body\n     * to parallelize processing.\n     *\n     * @param record   The record to send. If the topic or the partition specified in it cannot be found\n     *                 in metadata within {@code max.block.ms}, the returned future will time out when retrieved.\n     * @param callback A user-supplied callback to execute when the record has been acknowledged by the server (null\n     *                 indicates no callback)\n     * @throws IllegalStateException  if a transactional.id has been configured and no transaction has been started, or\n     *                                when send is invoked after producer has been closed.\n     * @throws InterruptException     If the thread is interrupted while blocked\n     * @throws SerializationException If the key or value are not valid objects given the configured serializers\n     * @throws KafkaException         If a Kafka related error occurs that does not belong to the public API exceptions.\n     * @see #partitionsFor(String)\n     */\n    @Override\n    public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {\n        // intercept the record, which can be potentially modified; this method does not throw exceptions\n        ProducerRecord<K, V> interceptedRecord = this.interceptors.onSend(record);\n        return doSend(interceptedRecord, callback);\n    }\n\n    // Verify that this producer instance has not been closed. This method throws IllegalStateException if the producer\n    // has already been closed.\n    private void throwIfProducerClosed() {\n        if (sender == null || !sender.isRunning())\n            throw new IllegalStateException(\"Cannot perform operation after producer has been closed\");\n    }\n\n    /**\n     * Throws an exception if the transaction is in a prepared state.\n     * In a two-phase commit (2PC) flow, once a transaction enters the prepared state,\n     * only commit, abort, or complete operations are allowed.\n     *\n     * @throws IllegalStateException if any other operation is attempted in the prepared state.\n     */\n    private void throwIfInPreparedState() {\n        if (transactionManager != null &&\n            transactionManager.isTransactional() &&\n            transactionManager.isPrepared()\n        ) {\n            throw new IllegalStateException(\"Cannot perform operation while the transaction is in a prepared state. \" +\n                \"Only commitTransaction(), abortTransaction(), or completeTransaction() are permitted.\");\n        }\n    }\n\n    /**\n     * Implementation of asynchronously send a record to a topic.\n     */\n    private Future<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback) {\n        // Append callback takes care of the following:\n        //  - call interceptors and user callback on completion\n        //  - remember partition that is calculated in RecordAccumulator.append\n        AppendCallbacks appendCallbacks = new AppendCallbacks(callback, this.interceptors, record);\n\n        try {\n            throwIfProducerClosed();\n            throwIfInPreparedState();\n\n            // first make sure the metadata for the topic is available\n            long nowMs = time.milliseconds();\n            ClusterAndWaitTime clusterAndWaitTime;\n            try {\n                clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);\n            } catch (KafkaException e) {\n                if (metadata.isClosed())\n                    throw new KafkaException(\"Producer closed while send in progress\", e);\n                throw e;\n            }\n            nowMs += clusterAndWaitTime.waitedOnMetadataMs;\n            long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);\n            Cluster cluster = clusterAndWaitTime.cluster;\n            byte[] serializedKey;\n            try {\n                serializedKey = keySerializerPlugin.get().serialize(record.topic(), record.headers(), record.key());\n            } catch (ClassCastException cce) {\n                throw new SerializationException(\"Can't convert key of class \" + record.key().getClass().getName() +\n                        \" to class \" + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +\n                        \" specified in key.serializer\", cce);\n            }\n            byte[] serializedValue;\n            try {\n                serializedValue = valueSerializerPlugin.get().serialize(record.topic(), record.headers(), record.value());\n            } catch (ClassCastException cce) {\n                throw new SerializationException(\"Can't convert value of class \" + record.value().getClass().getName() +\n                        \" to class \" + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +\n                        \" specified in value.serializer\", cce);\n            }\n\n            // Try to calculate partition, but note that after this call it can be RecordMetadata.UNKNOWN_PARTITION,\n            // which means that the RecordAccumulator would pick a partition using built-in logic (which may\n            // take into account broker load, the amount of data produced to each partition, etc.).\n            int partition = partition(record, serializedKey, serializedValue, cluster);\n\n            setReadOnly(record.headers());\n            Header[] headers = record.headers().toArray();\n\n            int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(RecordBatch.CURRENT_MAGIC_VALUE,\n                    compression.type(), serializedKey, serializedValue, headers);\n            ensureValidRecordSize(serializedSize);\n            long timestamp = record.timestamp() == null ? nowMs : record.timestamp();\n\n            // Append the record to the accumulator.  Note, that the actual partition may be\n            // calculated there and can be accessed via appendCallbacks.topicPartition.\n            RecordAccumulator.RecordAppendResult result = accumulator.append(record.topic(), partition, timestamp, serializedKey,\n                    serializedValue, headers, appendCallbacks, remainingWaitMs, nowMs, cluster);\n            assert appendCallbacks.getPartition() != RecordMetadata.UNKNOWN_PARTITION;\n\n            // Add the partition to the transaction (if in progress) after it has been successfully\n            // appended to the accumulator. We cannot do it before because the partition may be\n            // unknown. Note that the `Sender` will refuse to dequeue\n            // batches from the accumulator until they have been added to the transaction.\n            if (transactionManager != null) {\n                transactionManager.maybeAddPartition(appendCallbacks.topicPartition());\n            }\n\n            if (result.batchIsFull || result.newBatchCreated) {\n                log.trace(\"Waking up the sender since topic {} partition {} is either full or getting a new batch\", record.topic(), appendCallbacks.getPartition());\n                this.sender.wakeup();\n            }\n            return result.future;\n            // handling exceptions and record the errors;\n            // for API exceptions return them in the future,\n            // for other exceptions throw directly\n        } catch (ApiException e) {\n            log.debug(\"Exception occurred during message send:\", e);\n            if (callback != null) {\n                TopicPartition tp = appendCallbacks.topicPartition();\n                RecordMetadata nullMetadata = new RecordMetadata(tp, -1, -1, RecordBatch.NO_TIMESTAMP, -1, -1);\n                callback.onCompletion(nullMetadata, e);\n            }\n            this.errors.record();\n            this.interceptors.onSendError(record, appendCallbacks.topicPartition(), e);\n            if (transactionManager != null) {\n                transactionManager.maybeTransitionToErrorState(e);\n            }\n            return new FutureFailure(e);\n        } catch (InterruptedException e) {\n            this.errors.record();\n            this.interceptors.onSendError(record, appendCallbacks.topicPartition(), e);\n            throw new InterruptException(e);\n        } catch (KafkaException e) {\n            this.errors.record();\n            this.interceptors.onSendError(record, appendCallbacks.topicPartition(), e);\n            throw e;\n        } catch (Exception e) {\n            // we notify interceptor about all exceptions, since onSend is called before anything else in this method",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L1041-L1190",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 1041,
  "end_line": 1190,
  "last_modified": "2026-02-06T01:16:27.608270",
  "source_type": "github"
}
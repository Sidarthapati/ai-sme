{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_RecordAccumulator.java_391",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "content": "        }\n\n        MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer);\n        ProducerBatch batch = new ProducerBatch(new TopicPartition(topic, partition), recordsBuilder, nowMs);\n        FutureRecordMetadata future = Objects.requireNonNull(batch.tryAppend(timestamp, key, value, headers,\n                callbacks, nowMs));\n\n        dq.addLast(batch);\n        incomplete.add(batch);\n\n        return new RecordAppendResult(future, dq.size() > 1 || batch.isFull(), true, batch.estimatedSizeInBytes());\n    }\n\n    private MemoryRecordsBuilder recordsBuilder(ByteBuffer buffer) {\n        return MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compression, TimestampType.CREATE_TIME, 0L);\n    }\n\n    /**\n     * Check if all batches in the queue are full.\n     */\n    private boolean allBatchesFull(Deque<ProducerBatch> deque) {\n        // Only the last batch may be incomplete, so we just check that.\n        ProducerBatch last = deque.peekLast();\n        return last == null || last.isFull();\n    }\n\n     /**\n     *  Try to append to a ProducerBatch.\n     *\n     *  If it is full, we return null and a new batch is created. We also close the batch for record appends to free up\n     *  resources like compression buffers. The batch will be fully closed (ie. the record batch headers will be written\n     *  and memory records built) in one of the following cases (whichever comes first): right before send,\n     *  if it is expired, or when the producer is closed.\n     */\n    private RecordAppendResult tryAppend(long timestamp, byte[] key, byte[] value, Header[] headers,\n                                         Callback callback, Deque<ProducerBatch> deque, long nowMs) {\n        if (closed)\n            throw new KafkaException(\"Producer closed while send in progress\");\n        ProducerBatch last = deque.peekLast();\n        if (last != null) {\n            int initialBytes = last.estimatedSizeInBytes();\n            FutureRecordMetadata future = last.tryAppend(timestamp, key, value, headers, callback, nowMs);\n            if (future == null) {\n                last.closeForRecordAppends();\n            } else {\n                int appendedBytes = last.estimatedSizeInBytes() - initialBytes;\n                return new RecordAppendResult(future, deque.size() > 1 || last.isFull(), false, appendedBytes);\n            }\n        }\n        return null;\n    }\n\n    private boolean isMuted(TopicPartition tp) {\n        return muted.contains(tp);\n    }\n\n    public void resetNextBatchExpiryTime() {\n        nextBatchExpiryTimeMs = Long.MAX_VALUE;\n    }\n\n    public void maybeUpdateNextBatchExpiryTime(ProducerBatch batch) {\n        if (batch.createdMs + deliveryTimeoutMs  > 0) {\n            // the non-negative check is to guard us against potential overflow due to setting\n            // a large value for deliveryTimeoutMs\n            nextBatchExpiryTimeMs = Math.min(nextBatchExpiryTimeMs, batch.createdMs + deliveryTimeoutMs);\n        } else {\n            log.warn(\"Skipping next batch expiry time update due to addition overflow: \"\n                + \"batch.createMs={}, deliveryTimeoutMs={}\", batch.createdMs, deliveryTimeoutMs);\n        }\n    }\n\n    /**\n     * Get a list of batches which have been sitting in the accumulator too long and need to be expired.\n     */\n    public List<ProducerBatch> expiredBatches(long now) {\n        List<ProducerBatch> expiredBatches = new ArrayList<>();\n        for (TopicInfo topicInfo : topicInfoMap.values()) {\n            for (Deque<ProducerBatch> deque : topicInfo.batches.values()) {\n                // expire the batches in the order of sending\n                synchronized (deque) {\n                    while (!deque.isEmpty()) {\n                        ProducerBatch batch = deque.getFirst();\n                        if (batch.hasReachedDeliveryTimeout(deliveryTimeoutMs, now)) {\n                            deque.poll();\n                            batch.abortRecordAppends();\n                            expiredBatches.add(batch);\n                        } else {\n                            maybeUpdateNextBatchExpiryTime(batch);\n                            break;\n                        }\n                    }\n                }\n            }\n        }\n        return expiredBatches;\n    }\n\n    public long getDeliveryTimeoutMs() {\n        return deliveryTimeoutMs;\n    }\n\n    /**\n     * Re-enqueue the given record batch in the accumulator. In Sender.completeBatch method, we check\n     * whether the batch has reached deliveryTimeoutMs or not. Hence we do not do the delivery timeout check here.\n     */\n    public void reenqueue(ProducerBatch batch, long now) {\n        batch.reenqueued(now);\n        Deque<ProducerBatch> deque = getOrCreateDeque(batch.topicPartition);\n        synchronized (deque) {\n            if (transactionManager != null)\n                insertInSequenceOrder(deque, batch);\n            else\n                deque.addFirst(batch);\n        }\n    }\n\n    /**\n     * Split the big batch that has been rejected and reenqueue the split batches in to the accumulator.\n     * @return the number of split batches.\n     */\n    public int splitAndReenqueue(ProducerBatch bigBatch) {\n        // Reset the estimated compression ratio to the initial value or the big batch compression ratio, whichever\n        // is bigger. There are several different ways to do the reset. We chose the most conservative one to ensure\n        // the split doesn't happen too often.\n        CompressionRatioEstimator.setEstimation(bigBatch.topicPartition.topic(), compression.type(),\n                                                Math.max(1.0f, (float) bigBatch.compressionRatio()));\n        int targetSplitBatchSize = this.batchSize;\n\n        if (bigBatch.isSplitBatch()) {\n            targetSplitBatchSize = Math.max(bigBatch.maxRecordSize, bigBatch.estimatedSizeInBytes() / 2);\n        }\n        Deque<ProducerBatch> dq = bigBatch.split(targetSplitBatchSize);\n        int numSplitBatches = dq.size();\n        Deque<ProducerBatch> partitionDequeue = getOrCreateDeque(bigBatch.topicPartition);\n        while (!dq.isEmpty()) {\n            ProducerBatch batch = dq.pollLast();\n            incomplete.add(batch);\n            // We treat the newly split batches as if they are not even tried.\n            synchronized (partitionDequeue) {\n                if (transactionManager != null) {\n                    // We should track the newly created batches since they already have assigned sequences.\n                    transactionManager.addInFlightBatch(batch);\n                    insertInSequenceOrder(partitionDequeue, batch);\n                } else {\n                    partitionDequeue.addFirst(batch);\n                }\n            }\n        }\n        return numSplitBatches;\n    }",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java#L391-L540",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 391,
  "end_line": 540,
  "last_modified": "2026-02-06T01:16:27.610051",
  "source_type": "github"
}
{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_internals_RecordAccumulator.java_261",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "content": "     * <p>\n     *\n     * @param topic The topic to which this record is being sent\n     * @param partition The partition to which this record is being sent or RecordMetadata.UNKNOWN_PARTITION\n     *                  if any partition could be used\n     * @param timestamp The timestamp of the record\n     * @param key The key for the record\n     * @param value The value for the record\n     * @param headers the Headers for the record\n     * @param callbacks The callbacks to execute\n     * @param maxTimeToBlock The maximum time in milliseconds to block for buffer memory to be available\n     * @param nowMs The current time, in milliseconds\n     * @param cluster The cluster metadata\n     */\n    public RecordAppendResult append(String topic,\n                                     int partition,\n                                     long timestamp,\n                                     byte[] key,\n                                     byte[] value,\n                                     Header[] headers,\n                                     AppendCallbacks callbacks,\n                                     long maxTimeToBlock,\n                                     long nowMs,\n                                     Cluster cluster) throws InterruptedException {\n        TopicInfo topicInfo = topicInfoMap.computeIfAbsent(topic, k -> new TopicInfo(createBuiltInPartitioner(logContext, k, batchSize)));\n\n        // We keep track of the number of appending thread to make sure we do not miss batches in\n        // abortIncompleteBatches().\n        appendsInProgress.incrementAndGet();\n        ByteBuffer buffer = null;\n        if (headers == null) headers = Record.EMPTY_HEADERS;\n        try {\n            // Loop to retry in case we encounter partitioner's race conditions.\n            while (true) {\n                // If the message doesn't have any partition affinity, so we pick a partition based on the broker\n                // availability and performance.  Note, that here we peek current partition before we hold the\n                // deque lock, so we'll need to make sure that it's not changed while we were waiting for the\n                // deque lock.\n                final BuiltInPartitioner.StickyPartitionInfo partitionInfo;\n                final int effectivePartition;\n                if (partition == RecordMetadata.UNKNOWN_PARTITION) {\n                    partitionInfo = topicInfo.builtInPartitioner.peekCurrentPartitionInfo(cluster);\n                    effectivePartition = partitionInfo.partition();\n                } else {\n                    partitionInfo = null;\n                    effectivePartition = partition;\n                }\n\n                // Now that we know the effective partition, let the caller know.\n                setPartition(callbacks, effectivePartition);\n\n                // check if we have an in-progress batch\n                Deque<ProducerBatch> dq = topicInfo.batches.computeIfAbsent(effectivePartition, k -> new ArrayDeque<>());\n                synchronized (dq) {\n                    // After taking the lock, validate that the partition hasn't changed and retry.\n                    if (partitionChanged(topic, topicInfo, partitionInfo, dq, nowMs, cluster))\n                        continue;\n\n                    RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callbacks, dq, nowMs);\n                    if (appendResult != null) {\n                        // If queue has incomplete batches we disable switch (see comments in updatePartitionInfo).\n                        boolean enableSwitch = allBatchesFull(dq);\n                        topicInfo.builtInPartitioner.updatePartitionInfo(partitionInfo, appendResult.appendedBytes, cluster, enableSwitch);\n                        return appendResult;\n                    }\n                }\n\n                if (buffer == null) {\n                    int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(\n                            RecordBatch.CURRENT_MAGIC_VALUE, compression.type(), key, value, headers));\n                    log.trace(\"Allocating a new {} byte message buffer for topic {} partition {} with remaining timeout {}ms\", size, topic, effectivePartition, maxTimeToBlock);\n                    // This call may block if we exhausted buffer space.\n                    buffer = free.allocate(size, maxTimeToBlock);\n                    // Update the current time in case the buffer allocation blocked above.\n                    // NOTE: getting time may be expensive, so calling it under a lock\n                    // should be avoided.\n                    nowMs = time.milliseconds();\n                }\n\n                synchronized (dq) {\n                    // After taking the lock, validate that the partition hasn't changed and retry.\n                    if (partitionChanged(topic, topicInfo, partitionInfo, dq, nowMs, cluster))\n                        continue;\n\n                    RecordAppendResult appendResult = appendNewBatch(topic, effectivePartition, dq, timestamp, key, value, headers, callbacks, buffer, nowMs);\n                    // Set buffer to null, so that deallocate doesn't return it back to free pool, since it's used in the batch.\n                    if (appendResult.newBatchCreated)\n                        buffer = null;\n                    // If queue has incomplete batches we disable switch (see comments in updatePartitionInfo).\n                    boolean enableSwitch = allBatchesFull(dq);\n                    topicInfo.builtInPartitioner.updatePartitionInfo(partitionInfo, appendResult.appendedBytes, cluster, enableSwitch);\n                    return appendResult;\n                }\n            }\n        } finally {\n            free.deallocate(buffer);\n            appendsInProgress.decrementAndGet();\n        }\n    }\n\n    /**\n     * Append a new batch to the queue\n     *\n     * @param topic The topic\n     * @param partition The partition (cannot be RecordMetadata.UNKNOWN_PARTITION)\n     * @param dq The queue\n     * @param timestamp The timestamp of the record\n     * @param key The key for the record\n     * @param value The value for the record\n     * @param headers the Headers for the record\n     * @param callbacks The callbacks to execute\n     * @param buffer The buffer for the new batch\n     * @param nowMs The current time, in milliseconds\n     */\n    private RecordAppendResult appendNewBatch(String topic,\n                                              int partition,\n                                              Deque<ProducerBatch> dq,\n                                              long timestamp,\n                                              byte[] key,\n                                              byte[] value,\n                                              Header[] headers,\n                                              AppendCallbacks callbacks,\n                                              ByteBuffer buffer,\n                                              long nowMs) {\n        assert partition != RecordMetadata.UNKNOWN_PARTITION;\n\n        RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callbacks, dq, nowMs);\n        if (appendResult != null) {\n            // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often...\n            return appendResult;\n        }\n\n        MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer);\n        ProducerBatch batch = new ProducerBatch(new TopicPartition(topic, partition), recordsBuilder, nowMs);\n        FutureRecordMetadata future = Objects.requireNonNull(batch.tryAppend(timestamp, key, value, headers,\n                callbacks, nowMs));\n\n        dq.addLast(batch);\n        incomplete.add(batch);\n\n        return new RecordAppendResult(future, dq.size() > 1 || batch.isFull(), true, batch.estimatedSizeInBytes());\n    }\n\n    private MemoryRecordsBuilder recordsBuilder(ByteBuffer buffer) {\n        return MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compression, TimestampType.CREATE_TIME, 0L);\n    }\n\n    /**\n     * Check if all batches in the queue are full.\n     */",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java#L261-L410",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 261,
  "end_line": 410,
  "last_modified": "2026-02-06T01:16:27.610051",
  "source_type": "github"
}
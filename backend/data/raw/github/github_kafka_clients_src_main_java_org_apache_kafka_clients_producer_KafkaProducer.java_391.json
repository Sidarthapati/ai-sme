{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_KafkaProducer.java_391",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "content": "            this.partitionerIgnoreKeys = config.getBoolean(ProducerConfig.PARTITIONER_IGNORE_KEYS_CONFIG);\n            long retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);\n            long retryBackoffMaxMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MAX_MS_CONFIG);\n            if (keySerializer == null) {\n                keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, Serializer.class);\n                keySerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), true);\n            } else {\n                config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);\n            }\n            this.keySerializerPlugin = Plugin.wrapInstance(keySerializer, metrics, ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);\n\n            if (valueSerializer == null) {\n                valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, Serializer.class);\n                valueSerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), false);\n            } else {\n                config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);\n            }\n            this.valueSerializerPlugin = Plugin.wrapInstance(valueSerializer, metrics, ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);\n\n\n            List<ProducerInterceptor<K, V>> interceptorList = (List<ProducerInterceptor<K, V>>) ClientUtils.configuredInterceptors(config,\n                    ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,\n                    ProducerInterceptor.class);\n            if (interceptors != null)\n                this.interceptors = interceptors;\n            else\n                this.interceptors = new ProducerInterceptors<>(interceptorList, metrics);\n            ClusterResourceListeners clusterResourceListeners = ClientUtils.configureClusterResourceListeners(\n                    interceptorList,\n                    reporters,\n                    Arrays.asList(this.keySerializerPlugin.get(), this.valueSerializerPlugin.get()));\n            this.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);\n            this.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);\n            this.compression = configureCompression(config);\n\n            this.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);\n            int deliveryTimeoutMs = configureDeliveryTimeout(config, log);\n\n            this.apiVersions = apiVersions;\n            this.transactionManager = configureTransactionState(config, logContext);\n            // There is no need to do work required for adaptive partitioning, if we use a custom partitioner.\n            boolean enableAdaptivePartitioning = partitionerPlugin.get() == null &&\n                config.getBoolean(ProducerConfig.PARTITIONER_ADAPTIVE_PARTITIONING_ENABLE_CONFIG);\n            RecordAccumulator.PartitionerConfig partitionerConfig = new RecordAccumulator.PartitionerConfig(\n                enableAdaptivePartitioning,\n                config.getLong(ProducerConfig.PARTITIONER_AVAILABILITY_TIMEOUT_MS_CONFIG)\n            );\n            // As per Kafka producer configuration documentation batch.size may be set to 0 to explicitly disable\n            // batching which in practice actually means using a batch size of 1.\n            int batchSize = Math.max(1, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG));\n            this.accumulator = new RecordAccumulator(logContext,\n                    batchSize,\n                    compression,\n                    lingerMs(config),\n                    retryBackoffMs,\n                    retryBackoffMaxMs,\n                    deliveryTimeoutMs,\n                    partitionerConfig,\n                    metrics,\n                    PRODUCER_METRIC_GROUP_NAME,\n                    time,\n                    transactionManager,\n                    new BufferPool(this.totalMemorySize, batchSize, metrics, time, PRODUCER_METRIC_GROUP_NAME));\n\n            List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(config);\n            if (metadata != null) {\n                this.metadata = metadata;\n            } else {\n                this.metadata = new ProducerMetadata(retryBackoffMs,\n                        retryBackoffMaxMs,\n                        config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),\n                        config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG),\n                        logContext,\n                        clusterResourceListeners,\n                        Time.SYSTEM);\n                this.metadata.bootstrap(addresses);\n            }\n            this.errors = this.metrics.sensor(\"errors\");\n            this.sender = newSender(logContext, kafkaClient, this.metadata);\n            String ioThreadName = NETWORK_THREAD_PREFIX + \" | \" + clientId;\n            this.ioThread = new Sender.SenderThread(ioThreadName, this.sender, true);\n            this.ioThread.start();\n            config.logUnused();\n            AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());\n            log.debug(\"Kafka producer started\");\n        } catch (Throwable t) {\n            // call close methods if internal objects are already constructed this is to prevent resource leak. see KAFKA-2121\n            close(Duration.ofMillis(0), true);\n            // now propagate the exception\n            throw new KafkaException(\"Failed to construct kafka producer\", t);\n        }\n    }\n\n    // visible for testing\n    KafkaProducer(ProducerConfig config,\n                  LogContext logContext,\n                  Metrics metrics,\n                  Serializer<K> keySerializer,\n                  Serializer<V> valueSerializer,\n                  ProducerMetadata metadata,\n                  RecordAccumulator accumulator,\n                  TransactionManager transactionManager,\n                  Sender sender,\n                  ProducerInterceptors<K, V> interceptors,\n                  Partitioner partitioner,\n                  Time time,\n                  Sender.SenderThread ioThread,\n                  Optional<ClientTelemetryReporter> clientTelemetryReporter) {\n        this.producerConfig = config;\n        this.time = time;\n        this.clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);\n        this.log = logContext.logger(KafkaProducer.class);\n        this.metrics = metrics;\n        this.producerMetrics = new KafkaProducerMetrics(metrics);\n        this.partitionerPlugin = Plugin.wrapInstance(partitioner, metrics, ProducerConfig.PARTITIONER_CLASS_CONFIG);\n        this.keySerializerPlugin = Plugin.wrapInstance(keySerializer, metrics, ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);\n        this.valueSerializerPlugin = Plugin.wrapInstance(valueSerializer, metrics, ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);\n        this.interceptors = interceptors;\n        this.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);\n        this.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);\n        this.compression = configureCompression(config);\n        this.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);\n        this.partitionerIgnoreKeys = config.getBoolean(ProducerConfig.PARTITIONER_IGNORE_KEYS_CONFIG);\n        this.apiVersions = new ApiVersions();\n        this.transactionManager = transactionManager;\n        this.accumulator = accumulator;\n        this.errors = this.metrics.sensor(\"errors\");\n        this.metadata = metadata;\n        this.sender = sender;\n        this.ioThread = ioThread;\n        this.clientTelemetryReporter = clientTelemetryReporter;\n    }\n\n    // visible for testing\n    Sender newSender(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata) {\n        int maxInflightRequests = producerConfig.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION);\n        int requestTimeoutMs = producerConfig.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);\n        ProducerMetrics metricsRegistry = new ProducerMetrics(this.metrics);\n        Sensor throttleTimeSensor = Sender.throttleTimeSensor(metricsRegistry.senderMetrics);\n        KafkaClient client = kafkaClient != null ? kafkaClient : ClientUtils.createNetworkClient(producerConfig,\n                this.metrics,\n                \"producer\",\n                logContext,\n                apiVersions,\n                time,\n                maxInflightRequests,\n                metadata,\n                throttleTimeSensor,\n                clientTelemetryReporter.map(ClientTelemetryReporter::telemetrySender).orElse(null));\n",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L391-L540",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 391,
  "end_line": 540,
  "last_modified": "2026-02-06T01:16:27.608270",
  "source_type": "github"
}
{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_KafkaProducer.java_1301",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "content": "     * Other threads can continue sending records while one thread is blocked waiting for a flush call to complete,\n     * however no guarantee is made about the completion of records sent after the flush call begins.\n     * <p>\n     * This method can be useful when consuming from some input system and producing into Kafka. The <code>flush()</code> call\n     * gives a convenient way to ensure all previously sent messages have actually completed.\n     * <p>\n     * This example shows how to consume from one Kafka topic and produce to another Kafka topic:\n     * <pre>\n     * {@code\n     * for(ConsumerRecord<String, String> record: consumer.poll(100))\n     *     producer.send(new ProducerRecord(\"my-topic\", record.key(), record.value());\n     * producer.flush();\n     * consumer.commitSync();\n     * }\n     * </pre>\n     *\n     * Note that the above example may drop records if the produce request fails. If we want to ensure that this does not occur\n     * we need to set <code>retries=&lt;large_number&gt;</code> in our config.\n     * </p>\n     * <p>\n     * Applications don't need to call this method for transactional producers, since the {@link #commitTransaction()} will\n     * flush all buffered records before performing the commit. This ensures that all the {@link #send(ProducerRecord)}\n     * calls made since the previous {@link #beginTransaction()} are completed before the commit.\n     * </p>\n     * <p>\n     * <b>Important:</b> This method must not be called from within the callback provided to\n     * {@link #send(ProducerRecord, Callback)}. Invoking <code>flush()</code> in this context will result in a\n     * {@link KafkaException} being thrown, as it will cause a deadlock.\n     * </p>\n     *\n     * @throws InterruptException If the thread is interrupted while blocked\n     * @throws KafkaException If the method is invoked inside a {@link #send(ProducerRecord, Callback)} callback\n     */\n    @Override\n    public void flush() {\n        if (Thread.currentThread() == this.ioThread) {\n            log.error(\"KafkaProducer.flush() invocation inside a callback is not permitted because it may lead to deadlock.\");\n            throw new KafkaException(\"KafkaProducer.flush() invocation inside a callback is not permitted because it may lead to deadlock.\");\n        }\n\n        log.trace(\"Flushing accumulated records in producer.\");\n\n        long start = time.nanoseconds();\n        this.accumulator.beginFlush();\n        this.sender.wakeup();\n        try {\n            this.accumulator.awaitFlushCompletion();\n        } catch (InterruptedException e) {\n            throw new InterruptException(\"Flush interrupted.\", e);\n        } finally {\n            producerMetrics.recordFlush(time.nanoseconds() - start);\n        }\n    }\n\n    /**\n     * Get the partition metadata for the given topic. This can be used for custom partitioning.\n     * <p/>\n     * This will attempt to refresh metadata until it finds the topic in it, or the configured {@link ProducerConfig#MAX_BLOCK_MS_CONFIG} expires.\n     *\n     * @throws AuthenticationException if authentication fails. See the exception for more details\n     * @throws AuthorizationException  if not authorized to the specified topic. See the exception for more details\n     * @throws InterruptException      if the thread is interrupted while blocked\n     * @throws TimeoutException        if the topic cannot be found in metadata within {@code max.block.ms}\n     * @throws KafkaException          for all Kafka-related exceptions, including the case where this method is called after producer close\n     */\n    @Override\n    public List<PartitionInfo> partitionsFor(String topic) {\n        Objects.requireNonNull(topic, \"topic cannot be null\");\n        try {\n            return waitOnMetadata(topic, null, time.milliseconds(), maxBlockTimeMs).cluster.partitionsForTopic(topic);\n        } catch (InterruptedException e) {\n            throw new InterruptException(e);\n        }\n    }\n\n    /**\n     * Get the full set of internal metrics maintained by the producer.\n     */\n    @Override\n    public Map<MetricName, ? extends Metric> metrics() {\n        return Collections.unmodifiableMap(this.metrics.metrics());\n    }\n\n\n    /**\n     * Add the provided application metric for subscription.\n     * This metric will be added to this client's metrics\n     * that are available for subscription and sent as\n     * telemetry data to the broker.\n     * The provided metric must map to an OTLP metric data point\n     * type in the OpenTelemetry v1 metrics protobuf message types.\n     * Specifically, the metric should be one of the following:\n     * <ul>\n     *  <li>\n     *     `Sum`: Monotonic total count meter (Counter). Suitable for metrics like total number of X, e.g., total bytes sent.\n     *  </li>\n     *  <li>\n     *     `Gauge`: Non-monotonic current value meter (UpDownCounter). Suitable for metrics like current value of Y, e.g., current queue count.\n     *  </li>\n     * </ul>\n     * Metrics not matching these types are silently ignored.\n     * Executing this method for a previously registered metric is a benign operation and results in updating that metrics entry.\n     *\n     * @param metric The application metric to register\n     */\n    @Override\n    public void registerMetricForSubscription(KafkaMetric metric) {\n        if (!metrics().containsKey(metric.metricName())) {\n            clientTelemetryReporter.ifPresent(reporter -> reporter.metricChange(metric));\n        }  else {\n            log.debug(\"Skipping registration for metric {}. Existing producer metrics cannot be overwritten.\", metric.metricName());\n        }\n    }\n\n    /**\n     * Remove the provided application metric for subscription.\n     * This metric is removed from this client's metrics\n     * and will not be available for subscription any longer.\n     * Executing this method with a metric that has not been registered is a\n     * benign operation and does not result in any action taken (no-op).\n     *\n     * @param metric The application metric to remove\n     */\n    @Override\n    public void unregisterMetricFromSubscription(KafkaMetric metric) {\n        if (!metrics().containsKey(metric.metricName())) {\n            clientTelemetryReporter.ifPresent(reporter -> reporter.metricRemoval(metric));\n        } else {\n            log.debug(\"Skipping unregistration for metric {}. Existing producer metrics cannot be removed.\", metric.metricName());\n        }\n    }\n\n    /**\n     * Determines the client's unique client instance ID used for telemetry. This ID is unique to\n     * this specific client instance and will not change after it is initially generated.\n     * The ID is useful for correlating client operations with telemetry sent to the broker and\n     * to its eventual monitoring destinations.\n     * <p>\n     * If telemetry is enabled, this will first require a connection to the cluster to generate\n     * the unique client instance ID. This method waits up to {@code timeout} for the producer\n     * client to complete the request.\n     * <p>\n     * Client telemetry is controlled by the {@link ProducerConfig#ENABLE_METRICS_PUSH_CONFIG}\n     * configuration option.\n     *\n     * @param timeout The maximum time to wait for producer client to determine its client instance ID.\n     *                The value must be non-negative. Specifying a timeout of zero means do not\n     *                wait for the initial request to complete if it hasn't already.\n     * @throws InterruptException If the thread is interrupted while blocked.\n     * @throws KafkaException If an unexpected error occurs while trying to determine the client",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L1301-L1450",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 1301,
  "end_line": 1450,
  "last_modified": "2026-02-06T01:16:27.608270",
  "source_type": "github"
}
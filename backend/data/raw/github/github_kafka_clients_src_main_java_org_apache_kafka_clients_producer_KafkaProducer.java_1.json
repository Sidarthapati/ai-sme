{
  "id": "kafka_clients_src_main_java_org_apache_kafka_clients_producer_KafkaProducer.java_1",
  "title": "kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.clients.producer;\n\nimport org.apache.kafka.clients.ApiVersions;\nimport org.apache.kafka.clients.ClientUtils;\nimport org.apache.kafka.clients.CommonClientConfigs;\nimport org.apache.kafka.clients.KafkaClient;\nimport org.apache.kafka.clients.consumer.ConsumerGroupMetadata;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.clients.consumer.OffsetAndMetadata;\nimport org.apache.kafka.clients.consumer.OffsetCommitCallback;\nimport org.apache.kafka.clients.producer.internals.BufferPool;\nimport org.apache.kafka.clients.producer.internals.BuiltInPartitioner;\nimport org.apache.kafka.clients.producer.internals.KafkaProducerMetrics;\nimport org.apache.kafka.clients.producer.internals.ProducerInterceptors;\nimport org.apache.kafka.clients.producer.internals.ProducerMetadata;\nimport org.apache.kafka.clients.producer.internals.ProducerMetrics;\nimport org.apache.kafka.clients.producer.internals.RecordAccumulator;\nimport org.apache.kafka.clients.producer.internals.Sender;\nimport org.apache.kafka.clients.producer.internals.TransactionManager;\nimport org.apache.kafka.clients.producer.internals.TransactionalRequestResult;\nimport org.apache.kafka.common.Cluster;\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.Metric;\nimport org.apache.kafka.common.MetricName;\nimport org.apache.kafka.common.PartitionInfo;\nimport org.apache.kafka.common.TopicPartition;\nimport org.apache.kafka.common.Uuid;\nimport org.apache.kafka.common.compress.Compression;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.common.errors.ApiException;\nimport org.apache.kafka.common.errors.AuthenticationException;\nimport org.apache.kafka.common.errors.AuthorizationException;\nimport org.apache.kafka.common.errors.InterruptException;\nimport org.apache.kafka.common.errors.InvalidTopicException;\nimport org.apache.kafka.common.errors.InvalidTxnStateException;\nimport org.apache.kafka.common.errors.ProducerFencedException;\nimport org.apache.kafka.common.errors.RecordTooLargeException;\nimport org.apache.kafka.common.errors.RetriableException;\nimport org.apache.kafka.common.errors.SerializationException;\nimport org.apache.kafka.common.errors.TimeoutException;\nimport org.apache.kafka.common.errors.UnsupportedVersionException;\nimport org.apache.kafka.common.header.Header;\nimport org.apache.kafka.common.header.Headers;\nimport org.apache.kafka.common.header.internals.RecordHeaders;\nimport org.apache.kafka.common.internals.ClusterResourceListeners;\nimport org.apache.kafka.common.internals.Plugin;\nimport org.apache.kafka.common.metrics.KafkaMetric;\nimport org.apache.kafka.common.metrics.KafkaMetricsContext;\nimport org.apache.kafka.common.metrics.MetricConfig;\nimport org.apache.kafka.common.metrics.Metrics;\nimport org.apache.kafka.common.metrics.MetricsContext;\nimport org.apache.kafka.common.metrics.MetricsReporter;\nimport org.apache.kafka.common.metrics.Sensor;\nimport org.apache.kafka.common.record.AbstractRecords;\nimport org.apache.kafka.common.record.CompressionType;\nimport org.apache.kafka.common.record.RecordBatch;\nimport org.apache.kafka.common.requests.JoinGroupRequest;\nimport org.apache.kafka.common.serialization.Serializer;\nimport org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter;\nimport org.apache.kafka.common.telemetry.internals.ClientTelemetryUtils;\nimport org.apache.kafka.common.utils.AppInfoParser;\nimport org.apache.kafka.common.utils.LogContext;\nimport org.apache.kafka.common.utils.ProducerIdAndEpoch;\nimport org.apache.kafka.common.utils.Time;\nimport org.apache.kafka.common.utils.Timer;\nimport org.apache.kafka.common.utils.Utils;\n\nimport org.slf4j.Logger;\n\nimport java.net.InetSocketAddress;\nimport java.time.Duration;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Properties;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicReference;\n\n\n/**\n * A Kafka client that publishes records to the Kafka cluster.\n * <P>\n * The producer is <i>thread safe</i> and sharing a single producer instance across threads will generally be faster than\n * having multiple instances.\n * <p>\n * Here is a simple example of using the producer to send records with strings containing sequential numbers as the key/value\n * pairs.\n * <pre>\n * {@code\n * Properties props = new Properties();\n * props.put(\"bootstrap.servers\", \"localhost:9092\");\n * props.put(\"linger.ms\", 1);\n * props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n * props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n *\n * Producer<String, String> producer = new KafkaProducer<>(props);\n * for (int i = 0; i < 100; i++)\n *     producer.send(new ProducerRecord<String, String>(\"my-topic\", Integer.toString(i), Integer.toString(i)));\n *\n * producer.close();\n * }</pre>\n * <p>\n * The producer consists of a pool of buffer space that holds records that haven't yet been transmitted to the server\n * as well as a background I/O thread that is responsible for turning these records into requests and transmitting them\n * to the cluster. Failure to close the producer after use will leak these resources.\n * <p>\n * The {@link #send(ProducerRecord) send()} method is asynchronous. When called, it adds the record to a buffer of pending record sends\n * and immediately returns. This allows the producer to batch together individual records for efficiency.\n * <p>\n * The <code>acks</code> config controls the criteria under which requests are considered complete. The default setting \"all\"\n * will result in blocking on the full commit of the record, the slowest but most durable setting.\n * <p>\n * If the request fails, the producer can automatically retry. The <code>retries</code> setting defaults to <code>Integer.MAX_VALUE</code>, and\n * it's recommended to use <code>delivery.timeout.ms</code> to control retry behavior, instead of <code>retries</code>.\n * <p>\n * The producer maintains buffers of unsent records for each partition. These buffers are of a size specified by\n * the <code>batch.size</code> config. Making this larger can result in more batching, but requires more memory (since we will\n * generally have one of these buffers for each active partition).\n * <p>\n * By default a buffer is available to send immediately even if there is additional unused space in the buffer. However if you\n * want to reduce the number of requests you can set <code>linger.ms</code> to something greater than 0. This will\n * instruct the producer to wait up to that number of milliseconds before sending a request in hope that more records will\n * arrive to fill up the same batch. This is analogous to Nagle's algorithm in TCP. For example, in the code snippet above,\n * likely all 100 records would be sent in a single request since we set our linger time to 1 millisecond. However this setting\n * would add 1 millisecond of latency to our request waiting for more records to arrive if we didn't fill up the buffer. Note that\n * records that arrive close together in time will generally batch together even with <code>linger.ms=0</code>. So, under heavy load,\n * batching will occur regardless of the linger configuration; however setting this to something larger than 0 can lead to fewer, more",
  "url": "https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L1-L150",
  "file_path": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
  "repo_name": "kafka",
  "language": "java",
  "start_line": 1,
  "end_line": 150,
  "last_modified": "2026-02-06T01:16:27.608270",
  "source_type": "github"
}